{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10353768,"sourceType":"datasetVersion","datasetId":6411678},{"sourceId":104623,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72254,"modelId":76277}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma Template\n\nThis library was developed for the Kaggle challenge:\n[**Google - Unlocking Global Communication with Gemma**](https://www.kaggle.com/competitions/gemma-language-tuning), sponsored by Google.\n\n## Credit Requirement\n\n**Important:** If you are a participant in the competition and wish to use this source code in your submission,\nyou must clearly credit the original author before the competition's end date, **January 14, 2025**.\n\nPlease include the following information in your submission:\n\n```text\nAuthor: Tu Pham\nKaggle Username: [bigfishdev](https://www.kaggle.com/bigfishdev)\nGitHub: [https://github.com/thewebscraping/gemma-template/](https://github.com/thewebscraping/gemma-template)\nLinkedIn: [https://www.linkedin.com/in/thetwofarm](https://www.linkedin.com/in/thetwofarm)\n```\n\n# Overview\n\nGemma Template is a lightweight and efficient Python library for generating templates to fine-tune models and craft prompts.\nDesigned for flexibility, it seamlessly supports Gemma, LLaMA, and other language frameworks, offering fast, user-friendly customization.\nWith multilingual capabilities and advanced configuration options, it ensures precise, professional, and dynamic template creation.\n\n### Learning Process and Acknowledgements\nAs a newbie, I created Gemma Template based on what I read and learned from the following sources:\n\n- Google Gemma Cookbook: [Advanced Prompting Techniques](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Advanced_Prompting_Techniques.ipynb)\n- Google Gemma Cookbook: [Finetune with LLaMA Factory](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetune_with_LLaMA_Factory.ipynb)\n- Google Gemma Cookbook: [Fine tuning Gemma for Function Calling](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetuning_Gemma_for_Function_Calling.ipynb)\n- Alpaca: [Alpaca Lora Documentation](https://github.com/tloen/alpaca-lora)\n- Unsloth: [Finetune Llama 3.2, Mistral, Phi-3.5, Qwen 2.5 & Gemma 2-5x faster with 80% less memory!](https://github.com/unslothai/unsloth)\n\n\nGemma Template supports exporting dataset files in three formats: `Text`, `Alpaca`, and `OpenAI`.\n\n# Multilingual Content Writing Assistant\n\nThis writing assistant is a multilingual professional writer specializing in crafting structured, engaging, and SEO-optimized content.\nIt enhances text readability, aligns with linguistic nuances, and preserves original context across various languages.\n\n---\n\n## Key Features:\n#### 1. **Creative and Engaging Rewrites**\n- Transforms input text into captivating and reader-friendly content.\n- Utilizes vivid imagery and descriptive language to enhance engagement.\n\n#### 2. **Advanced Text Analysis**\n- Processes text with unigrams, bigrams, and trigrams to understand linguistic patterns.\n- Ensures language-specific nuances and cultural integrity are preserved.\n\n#### 3. **SEO-Optimized Responses**\n- Incorporates keywords naturally to improve search engine visibility.\n- Aligns rewritten content with SEO best practices for discoverability.\n\n#### 4. **Professional and Multilingual Expertise**\n- Full support for creating templates in local languages.\n- Supports multiple languages with advanced prompting techniques.\n- Vocabulary and grammar enhancement with unigrams, bigrams, and trigrams instruction template.\n- Supports hidden mask input text. Adapts tone and style to maintain professionalism and clarity.\n- Full documentation with easy configuration prompts and examples.\n\n#### 5. **Customize Advanced Response Structure and Dataset Format**\n- Supports advanced response structure format customization.\n- Compatible with other models such as LLaMa.\n- Enhances dynamic prompts using Round-Robin loops.\n- Outputs multiple formats such as Text, Alpaca and OpenAI.\n\n**Installation**\n----------------\n\nTo install the library, you can choose between two methods:\n\n#### **1\\. Install via PyPI:**\n\n```shell\npip install gemma-template\n```\n\n#### **2\\. Install via GitHub Repository:**\n\n```shell\npip install git+https://github.com/thewebscraping/gemma-template.git\n```\n\n**Quick Start**\n----------------\nStart using Gemma Template with just a few lines of code:\n\n## Load Dataset\nReturns: A Hugging Face Dataset or DatasetDict object containing the processed prompts.\n\n**Load Dataset from data dict**\n```python\nprompt_instance = Template()\ndata_dict = [\n    {\n        \"id\": \"JnZJolR76_u2\",\n        \"title\": \"Sample title\",\n        \"description\": \"Sample description\",\n        \"document\": \"Sample document\",\n        \"categories\": [\"Topic 1\", \"Topic 2\"],\n        \"tags\": [\"Tag 1\", \"Tag 2\"],\n        \"output\": \"Sample output\",\n        \"main_points\": [\"Main point 1\", \"Main point 2\"],\n    }\n]\ndataset = prompt_instance.load_dataset(data_dict, output_format='text')   # enum: `text`, `alpaca` and `openai`.\nprint(dataset['text'][0])\n```\n\n**Load Dataset from HuggingFace**\n```python\ndataset = gemma_template.load_dataset(\n    \"YOUR_JSON_FILE_PATH_OR_HUGGINGFACE_DATASET\",\n    # enum: `text`, `alpaca` and `openai`.\n    output_format='text',\n    # Percentage of documents that need to be word masked.\n    # Min: 0, Max: 1. Default: 0.\n    max_hidden_ratio=.1,\n    # Replace 10% of words in the input document with '_____'.\n    # Use int to extract the correct number of words. The `max_hidden_ratio` parameter must be greater than 0.\n    max_hidden_words=.1,\n    # Minimum character of a word, used to create unigrams, bigrams, and trigrams. Default is 2.\n    min_chars_length=2,\n    # Maximum character of a word, used to create unigrams, bigrams and trigrams. Default is 0.\n    max_chars_length=8,\n)\n```\n\n## Fully Customized Template\n\n```python\nfrom gemma_template import Template, FieldPosition, INPUT_TEMPLATE, OUTPUT_TEMPLATE, INSTRUCTION_TEMPLATE, PROMPT_TEMPLATE\n\ntemplate_instance = Template(\n    instruction_template=[INSTRUCTION_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    prompt_template=[PROMPT_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    input_template=[INPUT_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    output_template=[OUTPUT_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    position=FieldPosition(\n            title=[\"Custom Title\"],\n            description=[\"Custom Description\"],\n            document=[\"Custom Article\"],\n            main_points=[\"Custom Main Points\"],\n            categories=[\"Custom Categories\"],\n            tags=[\"Custom Tags\"],\n    ),  # Optional: dynamic Round-Robin loops\n)\n\nresponse = template_instance.apply_template(\n    title=\"Gemma open models\",\n    description=\"Gemma: Introducing new state-of-the-art open models.\",\n    main_points=[\"Main point 1\", \"Main point 2\"],\n    categories=[\"Artificial Intelligence\", \"Gemma\"],\n    tags=[\"AI\", \"LLM\", \"Google\"],\n    document=\"Gemma open models are built from the same research and technology as Gemini models. Gemma 2 comes in 2B, 9B and 27B and Gemma 1 comes in 2B and 7B sizes.\",\n    output=\"A new family of open language models demonstrating strong performance across academic benchmarks for language understanding, reasoning, and safety.\",\n    max_hidden_words=.1,  # set 0 if you don't want to hide words.\n    min_chars_length=2,  # Minimum character of a word, used to create unigrams, bigrams, and trigrams. Default is 2.\n    max_chars_length=0,  # Maximum character of a word, used to create unigrams, bigrams and trigrams. Default is 0.\n)  # remove kwargs if not used.\n\nprint(response)\n```\n\n### Output:\n\n```text\n<start_of_turn>user\nYou are a multilingual professional writer.\n\n# Role:\nYou are a highly skilled professional content writer, linguistic analyst, and multilingual expert specializing in structured writing and advanced text processing.\n\n# Task:\nYour primary objectives are:\n1. Simplification: Rewrite the input text or document to ensure it is accessible and easy to understand for a general audience while preserving the original meaning and essential details.\n2. Lexical and Grammatical Analysis: Analyze and refine vocabulary and grammar using unigrams (single words), bigrams (two words), and trigrams (three words) to enhance readability and depth.\n3. Structure and Organization: Ensure your response adheres strictly to the prescribed structure format.\n4. Language Consistency: Respond in the same language as the input text unless explicitly directed otherwise.\n\n# Additional Guidelines:\n1. Provide a rewritten, enhanced version of the input text, ensuring professionalism, clarity, and improved structure.\n2. Focus on multilingual proficiency, using complex vocabulary, grammar to improve your responses.\n3. Preserve the context and cultural nuances of the original text when rewriting.\n\n# Text Analysis:\nExample 1: Unigrams (single words)\nand => English\nbuilt => English\nfrom => English\nthe => English\nresearch => English\nText Analysis 3: These are common English words, indicating the text is in English.\n\nExample 2: Bigrams (two words)\ntechnology as => English\nText Analysis 2: Frequent bigrams in English confirm the language context.\n\nExample 3: Trigrams (three words)\ntechnology as Gemini => English\nText Analysis 3: Trigrams further validate the linguistic analysis and the necessity to respond in English.\n\n# Conclusion of Text Analysis:\nThe linguistic analysis confirms the text is predominantly in English. Consequently, the response should be structured and written in English to align with the original text and context.\n\n# Input Text:\nRewrite the input text or document to highlight its unique value proposition while ensuring it ranks well for targeted keywords.\n\n# Response Structure Format:\nYou must follow the response structure:\n\n**Custom Title (Title):** Rewrite the title to maximize clarity, appeal, and relevance to the content.\n**Custom Description (Description):** Create a description focusing on how the article addresses a common problem or challenge readers face.\n**Custom Article (Article):** Rewrite the input text or document with an authoritative tone, incorporating credible sources, data, and references to boost trustworthiness and SEO ranking.\n**Custom Main Points (Main Points):** Ensure all key points flow logically from one to the next.\n**Custom Categories (Categories):** Use categories that align with similar articles on the topic and improve SEO and discoverability.\n**Custom Tags (Tags):** Rewrite tags to make them more specific and targeted.\n\nBy adhering to this format, the response will maintain linguistic integrity while enhancing professionalism, structure and alignment with user expectations.\n\n# Text:\nGemma open models are built _____ the same _____ and technology as Gemini models. Gemma 2 comes in 2B, 9B _____ 27B and Gemma 1 comes in 2B and 7B sizes.<end_of_turn>\n<start_of_turn>model\n## **Custom Title:**\n### Gemma open models\n\n## **Custom Description:**\nGemma: Introducing new state-of-the-art open models.\n\n## **Custom Article:**\nA new family of open language models demonstrating strong performance across academic benchmarks for language understanding, reasoning, and safety.\n\n## **Custom Main Points:**\n* Main point 1\n* Main point 2\n\n## **Custom Categories:**\n* Artificial Intelligence\n* Gemma\n\n## **Custom Tags:**\n* AI\n* LLM\n* Google<end_of_turn>\n\n```\n","metadata":{}},{"cell_type":"code","source":"import time\nfrom datetime import datetime, timedelta\nstopped_at = datetime.now() + timedelta(hours=11, minutes=30)\nstart_at = time.perf_counter()\nprint(\"Start time:\", str(stopped_at))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:32:21.427856Z","iopub.execute_input":"2025-01-01T12:32:21.428213Z","iopub.status.idle":"2025-01-01T12:32:21.442386Z","shell.execute_reply.started":"2025-01-01T12:32:21.428180Z","shell.execute_reply":"2025-01-01T12:32:21.441282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Install Gemma Template dependencies to generate template","metadata":{}},{"cell_type":"code","source":"!pip install -q -U gemma-template\n!pip install -q evaluate rouge_score sacrebleu nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:32:21.443495Z","iopub.execute_input":"2025-01-01T12:32:21.443842Z","iopub.status.idle":"2025-01-01T12:32:29.956542Z","shell.execute_reply.started":"2025-01-01T12:32:21.443804Z","shell.execute_reply":"2025-01-01T12:32:29.955256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Install Unsloth dependencies for Fine Tuning","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:32:29.957696Z","iopub.execute_input":"2025-01-01T12:32:29.958009Z","iopub.status.idle":"2025-01-01T12:33:34.379834Z","shell.execute_reply.started":"2025-01-01T12:32:29.957982Z","shell.execute_reply":"2025-01-01T12:33:34.376157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install -q unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:33:34.382500Z","iopub.execute_input":"2025-01-01T12:33:34.383058Z","iopub.status.idle":"2025-01-01T12:36:35.830496Z","shell.execute_reply.started":"2025-01-01T12:33:34.382997Z","shell.execute_reply":"2025-01-01T12:36:35.829039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport json\nimport random\nfrom pathlib import Path\n\nmodel_name = \"/kaggle/input/gemma-2/transformers/gemma-2-2b-it/2/\"\nproject_id = \"gemma-template-gemma-2-2b-it-test-v2\"\n\nseed = 3407\n\nif 'google.colab' in sys.modules:\n    # Running on Colab\n    from google.colab import userdata\n    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_TOKEN')\nelif os.path.exists('/kaggle/working'):\n    # Running on Kaggle\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    os.environ['HF_TOKEN'] = user_secrets.get_secret(\"HF_TOKEN\")\n    os.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_TOKEN\")\nelse:\n    # Not running on Colab or Kaggle\n    raise EnvironmentError('This notebook is designed to run on Google Colab or Kaggle.')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:35.832034Z","iopub.execute_input":"2025-01-01T12:36:35.832440Z","iopub.status.idle":"2025-01-01T12:36:36.061679Z","shell.execute_reply.started":"2025-01-01T12:36:35.832393Z","shell.execute_reply":"2025-01-01T12:36:36.060714Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Configure Unsloth 4bit-quantized\n\nRead document here: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)","metadata":{}},{"cell_type":"code","source":"try:\n    from unsloth import FastLanguageModel\n    import torch\n    max_seq_length = 3072 # Choose any! We auto support RoPE Scaling internally!\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = model_name,\n        max_seq_length = max_seq_length,\n        dtype = None,\n        load_in_4bit = True,\n    )\n    \n    model = FastLanguageModel.get_peft_model(\n        model,\n        r = 16,\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n        lora_alpha = 16,\n        lora_dropout = 0,\n        bias = \"none\",\n        use_gradient_checkpointing = \"unsloth\",\n        random_state = 3407,\n        use_rslora = False,\n        loftq_config = None,\n    )\n    \n    model.config.use_cache = False\n    model.print_trainable_parameters()\nexcept (Exception, RuntimeError):\n    # Test template, tokenizer required.\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:36.065343Z","iopub.execute_input":"2025-01-01T12:36:36.065653Z","iopub.status.idle":"2025-01-01T12:36:43.272205Z","shell.execute_reply.started":"2025-01-01T12:36:36.065627Z","shell.execute_reply":"2025-01-01T12:36:43.271253Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Configure Tokenizer Padding Side\n\nFix overflow issue with bf16/fp16 training. See also: [Google Gemma Cookbook: Finetuning Gemma for Function Calling](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetuning_Gemma_for_Function_Calling.ipynb)","metadata":{}},{"cell_type":"code","source":"tokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:43.274143Z","iopub.execute_input":"2025-01-01T12:36:43.274737Z","iopub.status.idle":"2025-01-01T12:36:43.278717Z","shell.execute_reply.started":"2025-01-01T12:36:43.274696Z","shell.execute_reply":"2025-01-01T12:36:43.277784Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Preparation\n\nThe train dataset consists of 10,000 rows processed to fine-tune models for content writing. The eval dataset consists of 10,000 rows.\n\n**Example:**\n\n| ID             | Title         | Description        | Document         | Categories        | Tags         | Output          | Main Points            |\n|----------------|---------------|--------------------|------------------|-------------------|--------------|-----------------|------------------------|\n| JnZJolR76_u2   | Sample title  | Sample description | Sample document  | [\"Topic 1\", \"Topic 2\"]  | [\"Tag 1\", \"Tag 2\"] | Sample output   | [\"MP 1\", \"MP 2\"]       |\n\n\n## Data Source and Timeline\n\nThe data was collected between September 2024 and December 2024 from 50 news pages. The output response data is generate by LLaMa 3, Gemma 7B, and Gemma 9B.\n\n## Language Distribution\n\nThe dataset maintains a balanced language ratio:\n- **50% English**\n- **50% Vietnamese**\n\n## Dataset Composition\n\nThe data distribution is as follows:\n\n- **10%**: Combined responses including title and document.\n- **10%**: Combined responses including title, document, and description.\n- **10%**: Combined responses including title, document, and main points.\n- **10%**: Combined responses including title, document, categories, and tags.\n- **10%**: Responses without instructional templates, structured format only.\n- **10%**: Converted into GPT-style conversations with default output as a response document and random additional fields (e.g., title, description, main points, categories, tags).\n- **40%**: Responses using fully instructions and a prompt structure format.\n- `warmup_ratio`: 5%.\n\n## Additional Features\n\n- **15%** of the dataset incorporates hidden masked words, applied at a ratio of **5% of words** within each document.","metadata":{}},{"cell_type":"code","source":"from gemma_template import Template, gemma_template, vietnamese_gemma_template\nfrom gemma_template.__version__ import __version__\nprint(__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:43.279728Z","iopub.execute_input":"2025-01-01T12:36:43.280114Z","iopub.status.idle":"2025-01-01T12:36:44.858543Z","shell.execute_reply.started":"2025-01-01T12:36:43.280085Z","shell.execute_reply":"2025-01-01T12:36:44.857669Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import Gemma Template\nImport default Gemma Template for generate template.","metadata":{}},{"cell_type":"code","source":"from gemma_template import Template, FieldPosition, GEMMA_TEMPLATE, INPUT_TEMPLATE, OUTPUT_TEMPLATE, INSTRUCTION_TEMPLATE, PROMPT_TEMPLATE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:44.859316Z","iopub.execute_input":"2025-01-01T12:36:44.859765Z","iopub.status.idle":"2025-01-01T12:36:44.864319Z","shell.execute_reply.started":"2025-01-01T12:36:44.859739Z","shell.execute_reply":"2025-01-01T12:36:44.863133Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Gemma Template\n\nDefault templates by Gemma Template using Jinja2.\n\nUsing Gemma formatting: [https://ai.google.dev/gemma/docs/formatting](https://ai.google.dev/gemma/docs/formatting)","metadata":{}},{"cell_type":"markdown","source":"### **GEMMA_TEMPLATE**\nDefault input template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"<start_of_turn>user\n{{ input }}<end_of_turn>\n<start_of_turn>model\n{{ output }}<end_of_turn>\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:44.865515Z","iopub.execute_input":"2025-01-01T12:36:44.865957Z","iopub.status.idle":"2025-01-01T12:36:45.166983Z","shell.execute_reply.started":"2025-01-01T12:36:44.865903Z","shell.execute_reply":"2025-01-01T12:36:45.166024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **INPUT_TEMPLATE**\nDefault input template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"{{ system_prompt }}\n{% if instruction %}\\n{{ instruction }}\\n{% endif %}\n{% if prompt_structure %}{{ prompt_structure }}\\n{% else %}{{ prompt }}\\n{% endif %}\n# Text:\n{{ input }}\n{% if topic_value %}\\nTopics: {{ topic_value }}\\n{% endif %}{% if keyword_value %}Keywords: {{ keyword_value }}\\n{% endif %}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.167851Z","iopub.execute_input":"2025-01-01T12:36:45.168172Z","iopub.status.idle":"2025-01-01T12:36:45.183947Z","shell.execute_reply.started":"2025-01-01T12:36:45.168144Z","shell.execute_reply":"2025-01-01T12:36:45.182941Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **OUTPUT_TEMPLATE**\nDefault output template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"{% if structure_fields %}{% for field in structure_fields %}## **{{ field.label.custom or field.label.default }}:**\\n{% if field.key == 'title' %}### {% endif%}{{ field.value }}\\n\\n{% endfor %}{% else %}{{ output }}{% endif %}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.184961Z","iopub.execute_input":"2025-01-01T12:36:45.185380Z","iopub.status.idle":"2025-01-01T12:36:45.202012Z","shell.execute_reply.started":"2025-01-01T12:36:45.185340Z","shell.execute_reply":"2025-01-01T12:36:45.201023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **INSTRUCTION_TEMPLATE**\nDefault instruction template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"# Role:\nYou are a highly skilled professional content writer, linguistic analyst, and multilingual expert specializing in structured writing and advanced text processing.\n\n# Task:\nYour primary objectives are:\n1. Simplification: Rewrite the input text or document to ensure it is accessible and easy to understand for a general audience while preserving the original meaning and essential details.\n2. Lexical and Grammatical Analysis: Analyze and refine vocabulary and grammar using unigrams (single words), bigrams (two words), and trigrams (three words) to enhance readability and depth.\n3. Structure and Organization: Ensure your response adheres strictly to the prescribed structure format.\n4. Language Consistency: Respond in the same language as the input text unless explicitly directed otherwise.\n\n# Additional Guidelines:\n1. Provide a rewritten, enhanced version of the input text, ensuring professionalism, clarity, and improved structure.\n2. Focus on multilingual proficiency, using complex vocabulary, grammar to improve your responses.\n3. Preserve the context and cultural nuances of the original text when rewriting.\n\n# Text Analysis:\nExample 1: Unigrams (single words){% for word in unigrams %}\\n{{ word }} => {{ language }}{% endfor %}\nText Analysis 3: These are common {{ language }} words, indicating the text is in {{ language }}.\n\nExample 2: Bigrams (two words){% for word in bigrams %}\\n{{ word }} => {{ language }}{% endfor %}\nText Analysis 2: Frequent bigrams in {{ language }} confirm the language context.\n\nExample 3: Trigrams (three words){% for word in trigrams %}\\n{{ word }} => {{ language }}{% endfor %}\nText Analysis 3: Trigrams further validate the linguistic analysis and the necessity to respond in {{ language }}.\n\n# Conclusion of Text Analysis:\nThe linguistic analysis confirms the text is predominantly in {{ language }}. Consequently, the response should be structured and written in {{ language }} to align with the original text and context.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.203129Z","iopub.execute_input":"2025-01-01T12:36:45.203461Z","iopub.status.idle":"2025-01-01T12:36:45.223188Z","shell.execute_reply.started":"2025-01-01T12:36:45.203420Z","shell.execute_reply":"2025-01-01T12:36:45.222208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **PROMPT_TEMPLATE**\nDefault prompt template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"{% if prompt %}\\n\\n# Input Text:\\n{{ prompt }}\\n\\n{% endif %}{% if structure_fields %}# Response Structure Format:\nYou must follow the response structure:\n\n{% for field in structure_fields %}{{ field.label }}\\n{% endfor %}\nBy adhering to this format, the response will maintain linguistic integrity while enhancing professionalism, structure and alignment with user expectations.\\n\n{% endif %}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.224511Z","iopub.execute_input":"2025-01-01T12:36:45.224852Z","iopub.status.idle":"2025-01-01T12:36:45.242822Z","shell.execute_reply.started":"2025-01-01T12:36:45.224823Z","shell.execute_reply":"2025-01-01T12:36:45.241856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Example Output: Document Text with Hidden Mask (`document`)**","metadata":{}},{"cell_type":"code","source":"\"\"\"Gemma open models are built _____ the same _____ and technology as Gemini models. Gemma 2 comes in 2B, 9B _____ 27B and Gemma 1 comes in 2B and 7B sizes.\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.243933Z","iopub.execute_input":"2025-01-01T12:36:45.244313Z","iopub.status.idle":"2025-01-01T12:36:45.261192Z","shell.execute_reply.started":"2025-01-01T12:36:45.244273Z","shell.execute_reply":"2025-01-01T12:36:45.260118Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Example Output: Document Text without Hidden Mask  (`document`)**","metadata":{}},{"cell_type":"code","source":"\"\"\"Gemma open models are built from the same research and technology as Gemini models. Gemma 2 comes in 2B, 9B and 27B and Gemma 1 comes in 2B and 7B sizes.\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.262227Z","iopub.execute_input":"2025-01-01T12:36:45.262496Z","iopub.status.idle":"2025-01-01T12:36:45.278140Z","shell.execute_reply.started":"2025-01-01T12:36:45.262471Z","shell.execute_reply":"2025-01-01T12:36:45.277020Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Process Function Documentation\n\n## Overview\n\nThis repository contains tools for processing datasets to fine-tune language models. The `process_fn` function is central to preparing datasets in various formats such as `text`, `alpaca` and `openai`. It supports frameworks like `unsloth` and `LLaMA-Factory`.\n\n## Features\n\n- Generate unigrams, bigrams, and trigrams with customizable configurations.\n- Automatically exclude invalid keywords containing numbers or punctuation.\n- Supports both English and Vietnamese datasets, with specific configurations for each language.\n- Creates datasets for conversation-based models by dynamically generating conversational structures.\n\n## How It Works\n\n- **Language Distribution**:\n  - 50% English Dataset, with 10% used for evaluation.\n  - 50% Vietnamese Dataset, with 10% used for evaluation.\n\n- **Word Configuration**:\n  - **Unigrams**: Unique words with 2-8 characters. Reason: 1 character will match English like `I`, `a`, 8 characters because the longest word in Vietnamese is 7 characters.\n  - **Bigrams**: Unique bigrams words, not included words of unigrams list.\n  - **Trigrams**: Pairs of unigrams and bigrams.\n  - Ensures the AI vocabulary grows by at least 15 words per document.\n\n- **Special Features**:\n  - Hidden mask words applied to **15%** of the dataset, at a **5% ratio of words** per document.\n  - Generate conversations dataset with configurable fields and output format.","metadata":{}},{"cell_type":"markdown","source":"### Load Dataset\nLoad Dataset from Json File Kaggle host or HuggingFace Hub. See also: [https://huggingface.co/datasets/twodev/gemma-template](https://huggingface.co/datasets/twodev/gemma-template)","metadata":{}},{"cell_type":"code","source":"import json\nfrom datasets import Dataset, DatasetDict, load_dataset\n\ndef load_from_json_file(path: str = \"/kaggle/input/gemma-template/train-gemma-template.json\") -> Dataset:\n    with open(path, encoding=\"utf-8\") as f:\n        data = json.load(f)\n        return Dataset.from_list(data)\n\n\ndef load_from_huggingface_hub(path: str = \"twodev/gemma-template\", split: str = \"train\") -> Dataset:\n    dataset = load_dataset(\"twodev/gemma-template\")\n    return dataset[split]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.279261Z","iopub.execute_input":"2025-01-01T12:36:45.279594Z","iopub.status.idle":"2025-01-01T12:36:49.445342Z","shell.execute_reply.started":"2025-01-01T12:36:45.279556Z","shell.execute_reply":"2025-01-01T12:36:49.444384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_conversations_dataset(data: Dataset, mapping_field: dict[str, list[str]]) -> Dataset:\n    \"\"\"\n    Converts a dataset into a conversational format suitable for fine-tuning language models.\n\n    Notes:\n        - The `gemma_template._gen_bullet_list_style` method is used to format `openai` responses as a `number`, `dash` and `asterisk` bullet list when the field value is a list.\n    \"\"\"\n    \n    template = \"\"\"<start_of_turn>user\\n{input}<end_of_turn>\\n<start_of_turn>model\\n{output}<end_of_turn>\"\"\"\n    outputs = []\n    for item in data:\n        messages = item[\"messages\"]\n        for field in mapping_field:\n            if field in item[\"origin_data\"] and mapping_field[field]:\n                value = item[\"origin_data\"][field]\n                messages.append(\n                    {\n                        \"role\": \"user\",\n                        \"content\": random.choice(mapping_field[field]),\n                    }\n                )\n                messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": (\n                            gemma_template._generate_bullet_style(value, \"asterisk\")\n                            if isinstance(value, list)\n                            else value\n                        ),\n                    }\n                )\n\n        outputs_ = [\n            template.format(\n                input=\"\\n\\n\".join([messages[0]['content'], messages[1]['content']]),\n                output=messages[2]['content']\n            )\n        ]\n\n        if len(messages) > 3:\n            prompts = []\n            for idx in range(3, len(messages), 2):\n                try:\n                    prompt = template.format(\n                        input=messages[idx][\"content\"],\n                        output=messages[idx + 1][\"content\"],\n                    )\n                    prompts.append(prompt.strip())\n                except IndexError:\n                    pass\n\n            random.shuffle(prompts)\n            outputs_.extend(prompts)\n\n        outputs_.append(\"\")\n        sep = tokenizer.eos_token + \"\\n\"\n        outputs.append({\"text\": sep.join(outputs_)})\n\n    if outputs:\n        return Dataset.from_list(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.446344Z","iopub.execute_input":"2025-01-01T12:36:49.446605Z","iopub.status.idle":"2025-01-01T12:36:49.455121Z","shell.execute_reply.started":"2025-01-01T12:36:49.446582Z","shell.execute_reply":"2025-01-01T12:36:49.454024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_fn(\n    instance: Template,\n    data: Dataset, \n    excluded_fields: list[str] = (), \n    output_format = 'text',\n    max_hidden_ratio = 0.15, \n    max_hidden_words = 0.05, \n    min_chars_length = 2, \n    max_chars_length = 8,\n    max_concurrency: int = 4,\n    n_words = 5,\n    is_remove_data = True,\n    **kwargs\n) -> Dataset:\n    \"\"\"\n    Processes a dataset for fine-tuning language models, supporting formats like `text`, `alpaca`, and `gpt`.\n\n    Args:\n        instance (Template): A template instance for dataset processing.\n        data (Dataset): The input dataset to be processed.\n        excluded_fields (list[str]): Fields to exclude when generating conversational datasets.\n        output_format (str): Format of the processed dataset. Options are 'text', 'alpaca', and 'gpt'.\n        max_hidden_ratio (Union[float]):\n            Percentage of documents that need to be word masked. Min: 0, Max: 1. Default: 0.\n        max_hidden_words (Optional[str]):\n            Replace words in the document with '____'. The `max_hidden` parameter must be greater than 0.\n            Use `int`: exact number of words to be masked, `float`: percentage of number of words to be masked.\n        min_chars_length (int):\n            Minimum character of a word, used to create unigrams, bigrams, and trigrams. Default is 2.\n        max_chars_length (int):\n            Maximum character of a word, used to create unigrams, bigrams and trigrams. Default is 0.\n        max_concurrency (int):\n            Maximum number of concurrent threads for processing data. Default is 4.\n        n_words (int): Number of words frequently used to create unigrams, bigrams and trigrams.\n        is_remove_data (bool): Whether to remove specific fields from the dataset. Defaults to True.\n        **kwargs: Additional configuration parameters.\n\n    Returns:\n        Dataset or DatasetDict: The processed dataset in the specified format.\n\n    Notes:\n        - The `output_format` parameter determines the dataset's structure:\n            - `'text'`: Standard format for unsloth fine-tuning.\n            - `'alpaca'` or `'openai'`: Formats for frameworks like LLaMA-Factory.\n        - Using `output_format='openai'` and `is_remove_data=False` with `excluded_fields` generates conversational datasets.\n\n    \"\"\"\n    ds = instance.load_dataset(\n        data, \n        output_format=output_format, \n        excluded_fields=excluded_fields,\n        max_hidden_ratio=max_hidden_ratio, \n        max_hidden_words=max_hidden_words, \n        min_chars_length=min_chars_length, \n        max_chars_length=max_chars_length,\n        max_concurrency=max_concurrency,\n        n_words=n_words,\n        is_close_async_loop=False,  # Avoid `RuntimeError` by Notebook\n        is_remove_data=is_remove_data,\n    )\n    if output_format == 'openai' and not is_remove_data:\n        mapping_field = {\n            field: getattr(instance, field, None)\n            for field in excluded_fields\n            if getattr(instance, field, None)\n        }\n        ds = convert_to_conversations_dataset(ds, mapping_field=mapping_field)\n    else:\n        ds = ds.map(lambda x: {\"text\": [text + tokenizer.eos_token for text in x[\"text\"]]}, batched = True)  # Append eos token.\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.455995Z","iopub.execute_input":"2025-01-01T12:36:49.456272Z","iopub.status.idle":"2025-01-01T12:36:49.480898Z","shell.execute_reply.started":"2025-01-01T12:36:49.456246Z","shell.execute_reply":"2025-01-01T12:36:49.479648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_verify(data, is_masked: bool = True, task_name: str = \"TASK\"):\n    print(task_name + \"*\" * 45)\n    for item in data:\n        if item.get(\"is_masked\") == is_masked:\n            if is_masked:\n                print(\"HIDDEN TEXT: YES\" + \"*\" * 45)\n            else:\n                print(\"HIDDEN TEXT: NO\" + \"*\" * 45)\n                \n            print(\"\\n\")\n            print(item['text'])\n            print(\"=\" * 60)\n            print(\"*\" * 30, \" DATA ATTRS \", \"*\" * 30)\n            print(\"Masked Text:\", item['is_masked'])\n            print(\"Language Code:\", item['analysis']['language_code'])\n            print(\"Language:\", item['analysis']['language'])\n            print(\"Categories:\", item['analysis']['topic_value'])\n            print(\"Keywords:\", item['analysis']['keyword_value'])\n            print(\"Unigrams:\", item['analysis']['unigrams'])\n            print(\"Bigrams:\", item['analysis']['bigrams'])\n            print(\"Trigrams:\", item['analysis']['trigrams'])\n            print(\"VALID TASK: YES\")\n            print(\"*\" * 30, \" TASK DONE \", \"*\" * 30)\n            print(\"=\" * 60)\n            print(\"\\n\")\n            \n            return\n\n    print(\"VALID TASK: NO\")\n    print(\"*\" * 30, \" TASK DONE \", \"*\" * 30)\n    print(\"=\" * 60)\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.485269Z","iopub.execute_input":"2025-01-01T12:36:49.485659Z","iopub.status.idle":"2025-01-01T12:36:49.503313Z","shell.execute_reply.started":"2025-01-01T12:36:49.485611Z","shell.execute_reply":"2025-01-01T12:36:49.502245Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cases Handled\n\nThe dataset processing involves various configurations to handle diverse cases effectively:\n\n1. **Combined Responses**:\n   - **10%**: Responses include the `title` and `document`.\n   - **10%**: Responses include the `title`, `document`, and `description`.\n   - **10%**: Responses include the `title`, `document`, and `main points`.\n   - **10%**: Responses include the `title`, `document`, `categories`, and `tags`.\n\n2. **Masked Words**:\n   - **15%**: Masked words are applied to the dataset, with a hidden ratio of **5% of words** for each document.\n\n3. **Language Detection**:\n   - Language is automatically detected using the [`langdetect`](https://github.com/Mimino666/langdetect) library.","metadata":{}},{"cell_type":"code","source":"try:\n    dataset = load_from_json_file(\"/kaggle/input/gemma-template/train-gemma-template.json\")\nexcept FileNotFoundError:\n    dataset = load_from_huggingface_hub(\"twodev/gemma-template\")\n\ntotal_rows = len(dataset)\nfive_percent_idx = int(total_rows * 0.1)\n\ndataset_mapping = {\n    \"Combined response including title and document\": {\n        \"data\": dataset.select(range(0, five_percent_idx)),\n        \"excluded_fields\": [\"description\", \"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and description\": {\n        \"data\": dataset.select(range(five_percent_idx, five_percent_idx * 2)),\n        \"excluded_fields\": [\"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and main points\": {\n        \"data\": dataset.select(range(five_percent_idx * 2, five_percent_idx * 3)),\n        \"excluded_fields\": [\"description\", \"categories\", \"tags\"]\n        \n    },\n    \"Combined response including title, document and categories and tags\": {\n        \"data\": dataset.select(range(five_percent_idx * 3, five_percent_idx * 4)),\n        \"excluded_fields\": [\"description\", \"main_points\"]\n        \n    },\n}\n\nprint(dataset_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.505376Z","iopub.execute_input":"2025-01-01T12:36:49.505766Z","iopub.status.idle":"2025-01-01T12:36:49.538271Z","shell.execute_reply.started":"2025-01-01T12:36:49.505734Z","shell.execute_reply":"2025-01-01T12:36:49.537308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language_ratio_size = 0.5  # Ratio between English and local language.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.539436Z","iopub.execute_input":"2025-01-01T12:36:49.539732Z","iopub.status.idle":"2025-01-01T12:36:49.544475Z","shell.execute_reply.started":"2025-01-01T12:36:49.539704Z","shell.execute_reply":"2025-01-01T12:36:49.543603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_datasets = []\nfor task, item in dataset_mapping.items():\n    print(\"Prepare dataset for task:\", task)\n    split_dataset = item['data'].train_test_split(test_size=language_ratio_size)\n\n    # prepare dataset use instruction and structure English language.\n    english_dataset = process_fn(\n        gemma_template, \n        split_dataset[\"train\"], \n        excluded_fields=item['excluded_fields']\n    )\n    input_datasets.append(english_dataset)\n                                                       \n    # prepare dataset use instruction and structure Vietnamese language.\n    vietnamese_dataset = process_fn(\n        vietnamese_gemma_template, \n        split_dataset[\"test\"], \n        excluded_fields=item['excluded_fields']\n    )\n    \n    input_datasets.append(vietnamese_dataset)\n    \n# Test with hidden mask test\nprint_verify(english_dataset, is_masked=True, task_name=\"ENGLISH VERSION: {}\".format(task.upper()))\n# Test without hidden mask\nprint_verify(vietnamese_dataset, is_masked=False, task_name=\"VIETNAMESE VERSION: {}\".format(task.upper()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.545509Z","iopub.execute_input":"2025-01-01T12:36:49.545861Z","iopub.status.idle":"2025-01-01T12:45:30.063757Z","shell.execute_reply.started":"2025-01-01T12:36:49.545822Z","shell.execute_reply":"2025-01-01T12:45:30.062749Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n**Hidden Mask**:  The first **15%** of batches will include hidden mask words.\n\nThe dataset processing works perfectly across both language versions:\n\n1. **English**: Includes hidden word masks as expected.\n2. **Vietnamese**: No hidden word masks applied.\n\n## Common Observations\n- **Advanced Features**:\n  - Instruction included: **Unigrams**, **bigrams**, and **trigrams** are generated correctly.\n  - Structured formatting functions as intended.","metadata":{}},{"cell_type":"code","source":"print(input_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:45:30.064965Z","iopub.execute_input":"2025-01-01T12:45:30.065281Z","iopub.status.idle":"2025-01-01T12:45:30.070532Z","shell.execute_reply.started":"2025-01-01T12:45:30.065251Z","shell.execute_reply":"2025-01-01T12:45:30.069474Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare No Instruction Dataset\n- **10%**: Responses without instructional templates, fully structured format only.\n\nTo create a dataset without instructions:\n\n1. **Template Configuration**:  \n   - The creation process is similar to the standard template.  \n   - Set `instance.instruction_template=[]` to omit the instruction template during generation.\n\n2. **Dataset Merging**:  \n   - Append the generated dataset to `input_datasets` to allow merging of all datasets together.","metadata":{}},{"cell_type":"code","source":"# empty instruction template\ngemma_template.instruction_template = []  \nvietnamese_gemma_template.instruction_template = []\n\nno_instruction_dataset = dataset.select(range(five_percent_idx*4, five_percent_idx*5))\nprint(no_instruction_dataset)\n\nsplit_dataset = no_instruction_dataset.train_test_split(test_size=language_ratio_size)\n\n# prepare dataset use instruction and structure English language.\nenglish_no_instruction_dataset = process_fn(gemma_template, split_dataset[\"train\"])\ninput_datasets.append(english_no_instruction_dataset)\n                                                   \n# prepare dataset use instruction and structure Vietnamese language.\nvietnamese_no_instruction_dataset = process_fn(vietnamese_gemma_template,  split_dataset[\"test\"])\ninput_datasets.append(vietnamese_no_instruction_dataset)\n\n# print verify\nprint_verify(english_no_instruction_dataset, is_masked=False, task_name=\"ENGLISH NO INSTRUCTION VERSION\")\nprint_verify(vietnamese_no_instruction_dataset, is_masked=False, task_name=\"VIETNAMESE NO INSTRUCTION VERSION\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:45:30.071750Z","iopub.execute_input":"2025-01-01T12:45:30.072125Z","iopub.status.idle":"2025-01-01T12:51:50.265549Z","shell.execute_reply.started":"2025-01-01T12:45:30.072095Z","shell.execute_reply":"2025-01-01T12:51:50.264438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n- The instruction template has been deleted.\n- The structure template still works.","metadata":{}},{"cell_type":"markdown","source":"## Prepare Conversations Dataset\n- **10%**: Converted into GPT-style conversations with default output as a response document and random additional fields (e.g., title, description, main points, categories, tags).\n\n### Template Customization\n\n- Modify the template to follow the conversational format as outlined in the [Gemma documentation](https://ai.google.dev/gemma/docs/formatting).\n- I overwrite by adding eos token at the end of each model response.\n\n### Example Template\n\n```text\n<start_of_turn>user\n\nknock knock<end_of_turn><eos>\n\n<start_of_turn>model\n\nwho is there<end_of_turn><eos>\n\n<start_of_turn>user\n\nGemma<end_of_turn><eos>\n\n<start_of_turn>model\n\nGemma who?<end_of_turn><eos>\n```\n\n### Implementation","metadata":{}},{"cell_type":"code","source":"from gemma_template.constants import INSTRUCTION_TEMPLATE, VIETNAMESE_INSTRUCTION_TEMPLATE\n\n# Reset the template as instructed.\ngemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\nvietnamese_gemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\n\nconversations_dataset = dataset.select(range(five_percent_idx*5, five_percent_idx*6))\nsplit_dataset = conversations_dataset.train_test_split(test_size=language_ratio_size)\nexcluded_fields = [\"title\", \"description\", \"main_points\", \"categories\", \"tags\"]\n\n# prepare dataset use instruction and structure English language.\nenglish_conversations_dataset = process_fn(\n    gemma_template, \n    split_dataset[\"train\"], \n    excluded_fields=excluded_fields,\n    output_format=\"openai\",\n    is_remove_data=False,\n)\ninput_datasets.append(english_conversations_dataset)\n\n                                                   \n# prepare dataset use instruction and structure Vietnamese language.\nvietnamese_conversations_dataset = process_fn(\n    vietnamese_gemma_template, \n    split_dataset[\"test\"], \n    excluded_fields=excluded_fields,\n    output_format=\"openai\",\n    is_remove_data=False,\n)\ninput_datasets.append(vietnamese_conversations_dataset)\n\n# print verify\nprint(english_conversations_dataset['text'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:51:50.266706Z","iopub.execute_input":"2025-01-01T12:51:50.267134Z","iopub.status.idle":"2025-01-01T12:58:24.421252Z","shell.execute_reply.started":"2025-01-01T12:51:50.267091Z","shell.execute_reply":"2025-01-01T12:58:24.420048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n\n- The structure template still works. The GPT conversations structure matches.\n","metadata":{}},{"cell_type":"markdown","source":"## Prepare Fully Instruction and Prompt Structure Format\n- **40%**: Responses using fully instructions and a prompt structure format.\n","metadata":{}},{"cell_type":"code","source":"from gemma_template.constants import INSTRUCTION_TEMPLATE, VIETNAMESE_INSTRUCTION_TEMPLATE\n\n# Reset the template as instructed.\ngemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\nvietnamese_gemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\n\ninstruction_dataset = dataset.select(range(five_percent_idx*6, len(dataset)))\nprint(instruction_dataset)\n\nsplit_dataset = instruction_dataset.train_test_split(test_size=language_ratio_size)\n\n# prepare dataset use instruction and structure English language.\nenglish_instruction_dataset = process_fn(gemma_template, split_dataset[\"train\"])\ninput_datasets.append(english_instruction_dataset)\n                                                   \n# prepare dataset use instruction and structure Vietnamese language.\nvietnamese_instruction_dataset = process_fn(vietnamese_gemma_template,  split_dataset[\"test\"])\ninput_datasets.append(vietnamese_instruction_dataset)\n\n# print verify\nprint_verify(english_instruction_dataset, is_masked=True, task_name=\"ENGLISH INSTRUCTION VERSION\")\nprint_verify(vietnamese_instruction_dataset, is_masked=False, task_name=\"VIETNAMESE INSTRUCTION VERSION\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:58:24.422625Z","iopub.execute_input":"2025-01-01T12:58:24.423112Z","iopub.status.idle":"2025-01-01T13:21:38.633292Z","shell.execute_reply.started":"2025-01-01T12:58:24.423079Z","shell.execute_reply":"2025-01-01T13:21:38.632038Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n\n- The instruction and structure template still works.\n","metadata":{}},{"cell_type":"markdown","source":"## Merged Dataset\n- A special token representing the end of a text template.\n- Split 5% for warm up and merge all dataset together.\n- **Shuffle dataset**: shuffling the data helps prevent bias during training, ensures randomness in batch selection, and prevents the model from learning patterns based on the order of the data.","metadata":{}},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\nwramup_dataset, train_dataset = [], []\nfor input_dataset in input_datasets:\n    split_dataset = input_dataset.train_test_split(test_size=0.05)\n    wramup_dataset.append(split_dataset['test'])\n    train_dataset.append(split_dataset['train'])\n    \nwramup_dataset = concatenate_datasets(wramup_dataset).shuffle(seed=seed)\ntrain_dataset = concatenate_datasets(train_dataset).shuffle(seed=seed)\ntrain_dataset = concatenate_datasets([wramup_dataset, train_dataset])\n\n# verify dataset\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.634538Z","iopub.execute_input":"2025-01-01T13:21:38.634983Z","iopub.status.idle":"2025-01-01T13:21:38.767498Z","shell.execute_reply.started":"2025-01-01T13:21:38.634938Z","shell.execute_reply":"2025-01-01T13:21:38.766619Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **All Good! Now we begin to configure the Trainer for fine-tuning.**","metadata":{}},{"cell_type":"code","source":"elapsed = stopped_at - datetime.now()\nprint(\"Elapsed prepared dataset: %s. Took: %.2f seconds\" % (str(elapsed), elapsed.total_seconds()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.768496Z","iopub.execute_input":"2025-01-01T13:21:38.768764Z","iopub.status.idle":"2025-01-01T13:21:38.774330Z","shell.execute_reply.started":"2025-01-01T13:21:38.768739Z","shell.execute_reply":"2025-01-01T13:21:38.773269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# You can customize your local language configuration by following these instructions.\n\n### Tricks\n\nIf you want a static prompt loop, configure all prompts with equal length. For example:\n\n```python\ntitle = [\"a\", \"b\"]\ndescription = [\"1\", \"2\"]\n```\n\nThe template will be created as follows:\n- **Document 1:** title = `a`, description = `1`\n- **Document 2:** title = `b`, description = `2`\n- **Document 3:** title = `a`, description = `1`\n\nTo use a fully random set, configure different sizes of title and description. For example:\n\n```python\ntitle = [\"a\", \"b\"]\ndescription = [\"1\", \"2\", \"3\"]\n```\n\nThe template will be created as follows:\n- **Document 1:** title = `a`, description = `1`\n- **Document 2:** title = `b`, description = `2`\n- **Document 3:** title = `a`, description = `3`\n- **Document 4:** title = `b`, description = `1`\n\n## Sample Customizing Local Language Configuration","metadata":{}},{"cell_type":"markdown","source":"```python\nfrom gemma_template import *\n\nGEMMA_TEMPLATE = \"\"\"<start_of_turn>user\n\n{user_template}<end_of_turn>\n<start_of_turn>model\n\n{model_template}<end_of_turn>\n\n\"\"\"\n\nGEMMA_PROMPT_TEMPLATE = \"\"\"<start_of_turn>user\n\n{user_template}<end_of_turn>\n<start_of_turn>model\n\n\"\"\"\n\nVIETNAMESE_USER_TEMPLATE = \"\"\"{system_template}\n\n{prompt_template}\n\n{instruction_template}\n{structure_template}\n# Vn Bn:\n{document}\n\"\"\"\n\nVIETNAMESE_INSTRUCTION_TEMPLATE = \"\"\"# Vai tr:\nBn l mt bin tp vin ni dung chuyn nghip, nh phn tch ngn ng v chuyn gia a ngn ng, chuyn v vit c cu trc v x l vn bn nng cao.\n\n# Nhim V:\nMc tiu chnh ca bn l:\n1. Nhim v chnh ca bn l vit li ni dung c cung cp theo nh dng c cu trc, chuyn nghip hn, ng thi vn gi nguyn  nh v  ngha ban u.\n2. Nng cao kh nng hiu t vng bng cch phn tch vn bn vi unigrams (t n), bigrams (hai t) v trigrams (ba t).\n3. m bo phn hi ca bn tun th nghim ngt nh dng cu trc c quy nh.\n4. Phn hi bng ngn ng chnh ca vn bn u vo tr khi c hng dn thay th r rng.\n\n# K Vng B Sung:\n1. Cung cp phin bn vn bn u vo c vit li, nng cao, m bo tnh chuyn nghip, r rng v cu trc c ci thin.\n2. Tp trung vo kh nng a ngn ng, s dng vn t vng phc tp, ng php  ci thin phn hi ca bn.\n3. Gi nguyn ng cnh v sc thi vn ha ca vn bn gc khi vit li.\n\nDanh mc: {topic_values}\nT kho: {keyword_values}\n\n# Phn Tch Vn Bn:\nV D 1: Unigrams (nhm 1 ch ci)\n{unigrams}\n\nPhn Tch Vn Bn 1: y l nhng t thng dng trong ting Vit ({language}), cho bit vn bn c vit bng Ting Vit ({language}).\n\nV D 2: Bigrams (nhm 2 ch ci)\n{bigrams}\n\nPhn Tch Vn Bn 2: cc t ghp thng gp trong ting Vit ({language}) xc nhn bi cnh ngn ng.\n\nV D 3: Trigrams (nhm 3 ch ci)\n{trigrams}\n\nPhn Tch Vn Bn 3: cc t ghp 3 ch lin tip l nhng t ting Vit s dng thng xuyn, xc nhn s cn thit phi phn hi bng Ting Vit ({language}).\n\n# Kt Lun Phn Tch Vn Bn:\nPhn tch ngn ng xc nhn vn bn ch yu bng Ting Vit ({language}). Do , phn hi phi c cu trc v vit bng Ting Vit ({language}).  ph hp vi vn bn v ng cnh gc.\n\"\"\"  # noqa: E501\n\nVIETNAMESE_STRUCTURE_TEMPLATE = \"\"\"# nh Dng Cu Trc Phn Hi:\nBn phi tun theo cu trc phn hi:\n{structure_template}\n\nBng cch tun th nh dng ny, phn hi s duy tr tnh ton vn v mt ngn ng ng thi tng cng tnh chuyn nghip, cu trc v s ph hp vi mong i ca ngi dng.\n\"\"\"  # noqa: E501\n\n\ngemma_template = Template()\nvietnamese_template = Template(\n    end_sep=\"v\",\n    system_prompts=[\n        (\n            \"Bn l mt nh sng to ni dung, vit ni dung chuyn nghip bit nhiu\"\n            \" ngn ng.\"\n        ),\n    ],\n    user_prompts=[\n        (\n            \"Vit li vn bn  thn thin hn vi cng c tm kim. Kt hp cc t kha\"\n            \" c lin quan mt cch t nhin, ci thin kh nng c v m bo ph hp\"\n            \" vi cc phng php hay nht ca SEO.\"\n        ),\n        (\n            \"Vit li vn bn vi ging vn hp dn v sng to hn. S dng hnh nh\"\n            \" sng ng, ngn ng m t v phong cch m thoi  thu ht ngi c.\"\n        ),\n        (\n            \"Vit li vn bn  lm cho n sc tch hn m khng lm mt i  ngha hoc\"\n            \" tc ng ca n. Loi b cc t v cm t khng cn thit trong khi vn gi\"\n            \" nguyn thng ip ct li.\"\n        ),\n    ],\n    structure_field=StructureField(\n        title=[\"Tiu \"],\n        description=[\"M t\"],\n        document=[\"Bi vit chnh sa\"],\n        main_points=[\"im ni bt\", \"im chnh\"],\n        categories=[\"Danh mc\", \"Ch \"],\n        tags=[\"T kho\"],\n    ),\n    title=[\n        (\n            \"Vit li tiu   ngn gn, hp dn v c ti u ha cho SEO bng cc t\"\n            \" kha c lin quan.\"\n        ),\n        \"To tiu  ngn gn, thu ht s ch  v c ti u ha cho SEO.\",\n        \"Vit li tiu , kt hp cc t kha hoc cm t thnh hnh vo tiu .\",\n    ],\n    description=[\n        \"Tm tt bi vit trong mt cu, lm ni bt so v thu ht s t m.\",\n        (\n            \"To m t bng mt s tht hoc s liu thng k ng ngc nhin  thu ht\"\n            \" s ch   thu ht ngi c.\"\n        ),\n        (\n            \"Tm tt bi vit trong mt hoc hai cu tp trung vo  chnh, kt hp cc\"\n            \" t kha chnh mt cch t nhin.\"\n        ),\n    ],\n    document=[\n        (\n            \"Vit li bi vit vi ging iu chuyn nghip hn v cu trc hp l, d\"\n            \" c hn.\"\n        ),\n        \"Vit li bi vit  lm cho chng hp dn hn v c phong cch chuyn nghip.\",\n        \"n gin ha thut ng k thut  lm cho bi vit d hiu vi tt c c gi.\",\n    ],\n    main_points=[\n        (\n            \"To im chnh di dng danh sch, thm v d hoc gii thch ngn gn cho\"\n            \" tng im chnh.\"\n        ),\n        \"Tm tt cc  chnh thnh cc im chnh ngn gn, thu ht ngi c.\",\n        (\n            \"m bo tt c cc im chnh u c s lin kt hp l t im ny sang\"\n            \" im khc.\"\n        ),\n    ],\n    categories=[\n        \"Vit li cc danh mc  ph hp vi ch  ph bin theo bi vit.\",\n        \"To danh sch danh mc  ph hp vi cc t kha c s dng trong bi vit.\",\n        \"Chn cc danh mc ci thin SEO v kh nng khm ph theo ni dung bi vit.\",\n    ],\n    tags=[\n        \"To danh sch t kha thnh hnh gip SEO tt hn.\",\n        \"To danh sch t kha c lin quan ph hp vi truy vn tm kim ph bin.\",\n        \"Tp trung vo cc t kha ph bin trong bi vit  SEO tt hn.\",\n    ],\n)\n\nresponse = vietnamese_template.template(\n    template=GEMMA_TEMPLATE,\n    user_template=VIETNAMESE_USER_TEMPLATE,\n    instruction_template=VIETNAMESE_INSTRUCTION_TEMPLATE,\n    structure_template=VIETNAMESE_STRUCTURE_TEMPLATE,\n    title=\"Gemma open models\",\n    description=\"Gemma: Introducing new state-of-the-art open models.\",\n    document=\"Gemma open models are built from the same research and technology as Gemini models. Gemma 2 comes in 2B, 9B and 27B and Gemma 1 comes in 2B and 7B sizes.\",\n    main_points=[\"Main point 1\", \"Main point 2\"],\n    categories=[\"Artificial Intelligence\", \"Gemma\"],\n    tags=[\"AI\", \"LLM\", \"Google\"],\n    output=\"A new family of open language models demonstrating strong performance across academic benchmarks for language understanding, reasoning, and safety.\",\n )  # remove kwargs if not used.\nprint(response)\n```","metadata":{}},{"cell_type":"markdown","source":"# Automatic Model Save Every Hour\n\nThis repository includes an implementation for automatically saving model checkpoints during training on Kaggle. Due to Kaggle's training time limit, the `TrainerCallback` class has been extended to handle periodic backups. The maximum running time is set to 11 hours 30 minutes, and you can configure it using the `stopped_at` parameter (configuration on top of this notebook).\n\n## Automatic Backup Implementation\n\nThe following code demonstrates how the `HubCallback` class is implemented to save checkpoints every hour:","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom pathlib import Path\nfrom transformers import TrainerCallback, Trainer\nfrom transformers.trainer_callback import TrainerControl, TrainerState\nfrom transformers.training_args import TrainingArguments\n\nTRAINING_ARGS_NAME = \"training_args.bin\"\nTRAINER_STATE_NAME = \"trainer_state.json\"\nOPTIMIZER_NAME = \"optimizer.pt\"\nOPTIMIZER_NAME_BIN = \"optimizer.bin\"\nSCHEDULER_NAME = \"scheduler.pt\"\nSCALER_NAME = \"scaler.pt\"\nFSDP_MODEL_NAME = \"pytorch_model_fsdp\"\n\nREADME = \"\"\"\n# Gemma Template\n\nThis library was developed for the Kaggle challenge:\n[**Google - Unlocking Global Communication with Gemma**](https://www.kaggle.com/competitions/gemma-language-tuning), sponsored by Google.\n\n## Credit Requirement\n\n**Important:** If you are a participant in the competition and wish to use this source code in your submission,\nyou must clearly credit the original author before the competition's end date, **January 14, 2025**.\n\nPlease include the following information in your submission:\n\n```text\nAuthor: Tu Pham\nKaggle Username: [bigfishdev](https://www.kaggle.com/bigfishdev)\nGitHub: [https://github.com/thewebscraping/gemma-template/](https://github.com/thewebscraping/gemma-template)\nLinkedIn: [https://www.linkedin.com/in/thetwofarm](https://www.linkedin.com/in/thetwofarm)\n```\n\n# Overview\n\nGemma Template is a lightweight and efficient Python library for generating templates to fine-tune models and craft prompts.\nDesigned for flexibility, it seamlessly supports Gemma, LLaMA, and other language frameworks, offering fast, user-friendly customization.\nWith multilingual capabilities and advanced configuration options, it ensures precise, professional, and dynamic template creation.\n\n### Learning Process and Acknowledgements\nAs a newbie, I created Gemma Template based on what I read and learned from the following sources:\n\n- Google Gemma Cookbook: [Advanced Prompting Techniques](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Advanced_Prompting_Techniques.ipynb)\n- Google Gemma Cookbook: [Finetune_with_LLaMA_Factory](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetune_with_LLaMA_Factory.ipynb)\n- Google Gemma Cookbook: [Finetuning Gemma for Function Calling](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetuning_Gemma_for_Function_Calling.ipynb)\n- Alpaca: [Alpaca Lora Documentation](https://github.com/tloen/alpaca-lora)\n- Unsloth: [Finetune Llama 3.2, Mistral, Phi-3.5, Qwen 2.5 & Gemma 2-5x faster with 80% less memory!](https://github.com/unslothai/unsloth)\n\n\nGemma Template supports exporting dataset files in three formats: `Text`, `Alpaca`, and `GPT conversions`.\n\n# Multilingual Content Writing Assistant\n\nThis writing assistant is a multilingual professional writer specializing in crafting structured, engaging, and SEO-optimized content.\nIt enhances text readability, aligns with linguistic nuances, and preserves original context across various languages.\n\n---\n\n## Key Features:\n#### 1. **Creative and Engaging Rewrites**\n- Transforms input text into captivating and reader-friendly content.\n- Utilizes vivid imagery and descriptive language to enhance engagement.\n\n#### 2. **Advanced Text Analysis**\n- Processes text with unigrams, bigrams, and trigrams to understand linguistic patterns.\n- Ensures language-specific nuances and cultural integrity are preserved.\n\n#### 3. **SEO-Optimized Responses**\n- Incorporates keywords naturally to improve search engine visibility.\n- Aligns rewritten content with SEO best practices for discoverability.\n\n#### 4. **Professional and Multilingual Expertise**\n- Full support for creating templates in local languages.\n- Supports multiple languages with advanced prompting techniques.\n- Vocabulary and grammar enhancement with unigrams, bigrams, and trigrams instruction template.\n- Supports hidden mask input text. Adapts tone and style to maintain professionalism and clarity.\n- Full documentation with easy configuration prompts and examples.\n\n#### 5. **Customize Advanced Response Structure and Dataset Format**\n- Supports advanced response structure format customization.\n- Compatible with other models such as LLaMa.\n- Enhances dynamic prompts using Round-Robin loops.\n- Outputs multiple formats such as Text, Alpaca and GPT conversions.\n\n**Installation**\n----------------\n\nTo install the library, you can choose between two methods:\n\n#### **1\\. Install via PyPI:**\n\n```shell\npip install gemma-template\n```\n\n#### **2\\. Install via GitHub Repository:**\n\n```shell\npip install git+https://github.com/thewebscraping/gemma-template.git\n```\n\n# Training Arguments\nProject Id: {project_id}\nTraining Steps: {step}\n\n### Hyperparameters:\n{hyperparameters}\n\"\"\"\n\nclass HubCallback(TrainerCallback):\n    def __init__(self, trainer: Trainer, project_id, stopped_at, save_every_n_minutes=60):\n        super().__init__()\n        self.trainer = trainer\n        self.project_id = project_id\n        self.stopped_at = stopped_at\n        self.save_every_n_minutes = save_every_n_minutes\n        self.save_after = datetime.now() + timedelta(minutes=save_every_n_minutes)\n\n    def on_step_begin(self, train_args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n        if datetime.now().timestamp() > self.stopped_at.timestamp():\n            self.save_checkpoint(train_args)\n            control.should_training_stop = True\n            control.should_save = True\n        else:\n            if datetime.now().timestamp() > self.save_after.timestamp():\n                self.save_after = datetime.now() + timedelta(minutes=self.save_every_n_minutes)\n                self.save_checkpoint(train_args)\n\n    def save_checkpoint(self, train_args: TrainingArguments, output_dir: str = \".checkpoint/\"):\n        try:\n            print(\"*\" * 30, \" SAVE CHECKPOINT \", \"*\" * 30)\n            Path(output_dir).mkdir(parents=True, exist_ok=True)\n            self.trainer.save_model(output_dir, _internal_call=True)\n            self.trainer.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n            torch.save(self.trainer.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n            torch.save(self.trainer.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))\n            with open(os.path.join(output_dir, \"README.md\"), \"w\", encoding=\"utf-8\") as fp:\n                readme_doc = README.format(\n                    project_id=self.project_id,\n                    step=self.trainer.state.global_step,\n                    hyperparameters=json.dumps(train_args.to_dict(), ensure_ascii=False, indent=4)\n                )\n                fp.write(readme_doc)\n\n            print(\"*\" * 30, \" PROCESSED PUSH TO HUB \", \"*\" * 30)\n            self.push_to_hub(train_args, output_dir)\n            print(\"*\" * 30, \" COMPLETED PUSH TO HUB \", \"*\" * 30)\n\n        except Exception as e:\n            print(\"FAILED TO SAVE CHECKPOINT:\", e)\n\n    def push_to_hub(self, train_args: TrainingArguments, output_dir: str = \".checkpoint/\"):\n        self.trainer.tokenizer.save_pretrained(output_dir)\n        self.trainer.model.save_pretrained(output_dir)\n        self.trainer.tokenizer.push_to_hub(\n            self.project_id,\n            private=train_args.hub_private_repo,\n            token=train_args.hub_token\n        )\n        self.trainer.model.push_to_hub(\n            self.project_id,\n            commit_message=\"Training steps: {}\".format(self.trainer.state.global_step),\n            private=train_args.hub_private_repo,\n            token=train_args.hub_token\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.775435Z","iopub.execute_input":"2025-01-01T13:21:38.775700Z","iopub.status.idle":"2025-01-01T13:21:38.793897Z","shell.execute_reply.started":"2025-01-01T13:21:38.775677Z","shell.execute_reply":"2025-01-01T13:21:38.792758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure WanDB","metadata":{}},{"cell_type":"code","source":"import os\nimport wandb\n\nrun = wandb.init(\n    project=project_id,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.795057Z","iopub.execute_input":"2025-01-01T13:21:38.795440Z","iopub.status.idle":"2025-01-01T13:21:38.813466Z","shell.execute_reply.started":"2025-01-01T13:21:38.795400Z","shell.execute_reply":"2025-01-01T13:21:38.812415Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure Hyperparameters\n\nBelow is an configuration for `TrainingArguments`:","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrain_args = TrainingArguments(\n    # ---Output settings--\n    # Output directory where model predictions and checkpoints will be stored\n    output_dir = f\".results/{project_id}\",\n    logging_dir = f\".results/{project_id}/logs\",\n    overwrite_output_dir = True,\n    # No eval running\n    do_eval = False,\n    # Save strategy\n    save_strategy = \"steps\",\n    # Save steps\n    save_steps = 300,\n    # Save total limit\n    save_total_limit = 1,\n    # Batch size per GPU core for training\n    per_device_train_batch_size = 2,\n    # Number of update steps to accumulate the gradients for\n    gradient_accumulation_steps = 4,\n    # Train epochs\n    num_train_epochs = 1,\n    # Learning rate\n    learning_rate = 2e-4,\n    # Enable float16 precision\n    fp16 = not torch.cuda.is_bf16_supported(),\n    # Enable bfloat16 precision. False then fp16 is True\n    bf16 = torch.cuda.is_bf16_supported(),\n    # Logging: Log every X update step\n    logging_steps = 3,\n    # Optimizer to use\n    optim = \"adamw_8bit\",\n    # Weight decay\n    weight_decay = 0.1,\n    lr_scheduler_type = \"linear\",\n    # Ratio of steps for a linear warmup (from 0 to learning rate)\n    warmup_ratio = 0.05,\n    hub_private_repo = True,\n    remove_unused_columns = True,\n    seed = seed,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.814557Z","iopub.execute_input":"2025-01-01T13:21:38.814952Z","iopub.status.idle":"2025-01-01T13:21:51.394342Z","shell.execute_reply.started":"2025-01-01T13:21:38.814885Z","shell.execute_reply":"2025-01-01T13:21:51.392832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure SFT Trainer and Trainer","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,\n    args=train_args,\n)\n\n# Add HubCallback to save model every hour and stop after 11 hours 30 minutes\ntrainer.add_callback(HubCallback(trainer, project_id, stopped_at))\n\n# Start training\ntrainer = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.395036Z","iopub.status.idle":"2025-01-01T13:21:51.395387Z","shell.execute_reply":"2025-01-01T13:21:51.395243Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save Model to Kaggle Host","metadata":{}},{"cell_type":"code","source":"try:\n    model.save_pretrained(project_id)\n    tokenizer.save_pretrained(project_id)\nexcept:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.396361Z","iopub.status.idle":"2025-01-01T13:21:51.396757Z","shell.execute_reply":"2025-01-01T13:21:51.396562Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Push to HuggingFace","metadata":{}},{"cell_type":"code","source":"try:\n    model.push_to_hub(project_id)\n    tokenizer.push_to_hub(project_id)\nexcept:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.397607Z","iopub.status.idle":"2025-01-01T13:21:51.397960Z","shell.execute_reply":"2025-01-01T13:21:51.397792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"elapsed = stopped_at - datetime.now()\nprint(\"Elapsed:\", str(elapsed))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.398625Z","iopub.status.idle":"2025-01-01T13:21:51.399038Z","shell.execute_reply":"2025-01-01T13:21:51.398867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model Evaluation Metric\n=======================\n\nThis repository provides tools for evaluating models using **GoogleBLEU** and **ROUGE**, following the guidelines outlined in the [HuggingFace documentation on choosing a metric](https://huggingface.co/docs/evaluate/choosing_a_metric).\n\nOverview\n--------\n\nThe evaluation metrics implemented in this project include:\n\n1.  **GoogleBLEU**: A variant of BLEU designed for evaluating machine translation quality.\n    \n2.  **ROUGE**: Widely used for evaluating text summarization and machine translation models.\n    \n\nThese metrics ensure a comprehensive assessment of model performance across various natural language processing tasks.\n\nInstructions\n------------\n\n1.  Follow the instructions on the [HuggingFace Evaluation Metric Guide](https://huggingface.co/docs/evaluate/choosing_a_metric) to integrate these metrics into your workflow.\n    \n2.  Due to the runtime limitations of Kaggle trainers, the complete dataset results are hosted externally. You can view the full benchmark results here: [Benchmark Results](https://github.com/thewebscraping/gemma-template/blob/main/docs/benchmark.md).\n    \n\nResults\n-------\n\nA summary of evaluation results can be found in the [benchmark documentation](https://github.com/thewebscraping/gemma-template/blob/main/docs/benchmark.md).\n\nReferences\n----------\n\n*   HuggingFace Documentation: [Choosing a Metric](https://huggingface.co/docs/evaluate/choosing_a_metric)\n    \n*   Benchmark Data: [Benchmark Results](https://github.com/thewebscraping/gemma-template/blob/main/docs/benchmark.md)\n    \n\n### Example Code\n-------------\n","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nFastLanguageModel.for_inference(model)\n\nstopped_at = stopped_at + timedelta(minutes=15)\n\ndef is_expired(expired_at):\n    if datetime.now().timestamp() > expired_at.timestamp():\n        return True\n    return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prepare Test Dataset\n\n1. **Combined Prompt**:\n   - **10%**: Prompt include the `title` and `document`.\n   - **10%**: Prompt include the `title`, `document`, and `description`.\n   - **10%**: Prompt include the `title`, `document`, and `main points`.\n   - **10%**: Prompt include the `title`, `document`, `categories`, and `tags`.\n   - **60%**: Prompt fully structure format.\n   - **Remove instruction template for evaluate.**\n   - **Overwrite default prompt structure template.**","metadata":{}},{"cell_type":"code","source":"try:\n    test_dataset = load_from_json_file(\"/kaggle/input/gemma-template/test-gemma-template.json\")\nexcept FileNotFoundError:\n    test_dataset = load_from_huggingface_hub(\"twodev/gemma-template\", split='test')\n\ntotal_rows = len(test_dataset)\nfive_percent_idx = int(total_rows * 1)\n\ndataset_mapping = {\n    \"Combined response including title and document\": {\n        \"data\": test_dataset.select(range(0, five_percent_idx)),\n        \"excluded_fields\": [\"description\", \"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and description\": {\n        \"data\": test_dataset.select(range(five_percent_idx, five_percent_idx * 2)),\n        \"excluded_fields\": [\"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and main points\": {\n        \"data\": test_dataset.select(range(five_percent_idx * 2, five_percent_idx * 3)),\n        \"excluded_fields\": [\"description\", \"categories\", \"tags\"]\n        \n    },\n    \"Combined response including title, document and categories and tags\": {\n        \"data\": test_dataset.select(range(five_percent_idx * 3, five_percent_idx * 4)),\n        \"excluded_fields\": [\"description\", \"main_points\"]\n        \n    },\n    \"Prompt Structure Format\": {\n        \"data\": test_dataset.select(range(five_percent_idx * 4, len(test_dataset))),\n        \"excluded_fields\": []\n        \n    },\n}\n\nprint(dataset_mapping)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Configure Template\n- Remove instruction template for evaluate.\n- Overwrite default prompt structure template.","metadata":{}},{"cell_type":"code","source":"GEMMA_PROMPT_TEMPLATE = \"\"\"<start_of_turn>user\n{input}<end_of_turn>\n<start_of_turn>model\n\n\"\"\"\n\nPROMPT_TEMPLATE = \"\"\"{% if prompt %}\\n\\n{{ prompt }}\\n\\n{% endif %}{% if structure_fields %}You must follow the response structure:\n\n{% for field in structure_fields %}{{ field.label }}\\n{% endfor %}\n{% endif %}\"\"\"\n\ngemma_template.instruction_template = []  \ngemma_template.prompt_template = [PROMPT_TEMPLATE]\nvietnamese_gemma_template.instruction_template = []\nvietnamese_gemma_template.prompt_template = [PROMPT_TEMPLATE]\n\neval_dataset = []\ninput_datasets = []\nfor task, item in dataset_mapping.items():\n    if is_expired(stopped_at):\n        break\n        \n    print(\"Prepare dataset for task:\", task)\n    split_dataset = item['data'].train_test_split(test_size=language_ratio_size)\n\n    # prepare dataset use instruction and structure English language.\n    english_dataset = gemma_template.load_dataset(\n        split_dataset[\"train\"], \n        excluded_fields=item['excluded_fields'],\n        output_format='alpaca',\n    )\n    english_dataset = english_dataset.map(lambda x: {\"task\": [\"english_dataset\" for _ in x[\"input\"]], }, batched=True)\n    input_datasets.append(english_dataset)\n                                                       \n    # prepare dataset use instruction and structure Vietnamese language.\n    vietnamese_dataset = vietnamese_gemma_template.load_dataset(\n        split_dataset[\"test\"], \n        excluded_fields=item['excluded_fields'],\n        output_format='alpaca',\n    )\n    vietnamese_dataset = vietnamese_dataset.map(lambda x: {\"task\": [\"vietnamese\" for _ in x[\"input\"]], }, batched=True)\n    input_datasets.append(vietnamese_dataset)\n\nfrom datasets import concatenate_datasets\n\nif input_datasets:\n    eval_dataset = concatenate_datasets(input_datasets).shuffle(seed=42)\n    eval_dataset = eval_dataset.map(lambda x: {\"prompt\": [GEMMA_PROMPT_TEMPLATE.format(input=input_str) for input_str in x[\"input\"]], }, batched=True)\n    print(eval_dataset)\n    print(eval_dataset[0]['prompt'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run Evaluate","metadata":{}},{"cell_type":"code","source":"import json\nimport evaluate\n\ndef clean_response(response: str):\n    response = response.split(\"<start_of_turn>model\")[-1].split(\"<end_of_turn>\")\n    return response[0].strip()\n\n\ngoogle_bleu = evaluate.load(\"google_bleu\")\nrouge = evaluate.load('rouge')\neval_responses = []\n\nfor idx, item in enumerate(eval_dataset):\n    if is_expired(stopped_at):\n        break\n\n    task = str(item['task']).upper()\n    input_str = item['prompt']\n    output_str = item['output'].strip()\n    predictions, references = [output_str], []\n    input_ids = tokenizer(input_str, return_tensors=\"pt\").to(device)\n    outputs = model.generate(**input_ids, max_new_tokens=1024)\n\n    model_references = []\n    rouge_score, google_bleu_score = {}, {}\n\n    try:\n        for output in outputs:\n            model_response = tokenizer.decode(output)\n            model_references.append(model_response)\n            references.append(clean_response(model_response))\n        \n        if not (predictions and references):\n            continue\n\n        try:\n            rouge_score = rouge.compute(predictions=predictions, references=references)\n            rouge_score = {k: float(v) for k, v in rouge_score.items()}\n            print(\"Rouge Score:\", rouge_score)\n        except:\n            pass\n\n        try:\n            google_bleu_score = google_bleu.compute(predictions=predictions, references=references)\n            print(\"Google BLEU Score:\", google_bleu_score)\n        except:\n            pass\n            \n        print(\"*\" * 30, f\" MODEL OUTPUT - {task} PROMPT \", \"*\" * 30)\n        print(references[0])\n        print(\"=\" * 90)\n    except:\n        pass\n    \n    try:\n        item.update({\"rouge\": rouge_score, \"google_bleu\": google_bleu_score, \"model_references\": model_references})\n        eval_responses.append(json.loads(json.dumps(item, default=str)))\n    except:\n        pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save Evaluate Data ","metadata":{}},{"cell_type":"code","source":"def is_valid_score(r: dict, field: str):\n    if isinstance(r, dict):\n        if r.get(field):\n            return True\n        \ndef write_json(obj, path: str = \"dump.json\", *, ensure_ascii=False, indent=4):\n    with open(path, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump(obj, json_file, ensure_ascii=False, indent=4, default = str)\n\n\ntry:\n    write_json(eval_responses, \"eval_score_data.json\")\nexcept:\n    pass\n\n\ntotal_rows = len(eval_responses)\ntry:\n    rouge_mapping = {\n        \"rouge1\": sum([r['rouge']['rouge1'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n        \"rouge2\": sum([r['rouge']['rouge2'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n        \"rougeL\": sum([r['rouge']['rougeL'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n        \"rougeLSum\": sum([r['rouge']['rougeLsum'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n    }\n    print(\"ROUGE SCORE:\", str(rouge_mapping))\nexcept:\n    pass\n\ntry:\n    google_bleu_mapping = {\"google_bleu\": sum([r['google_bleu']['google_bleu'] for r in eval_responses if is_valid_score(r, \"google_bleu\")]) / total_rows}\n    print(\"GOOGLE BLEU:\", str(google_bleu_mapping))\nexcept:\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Took: %.2f seconds.\", (time.perf_counter() - start_at))\nprint(\"Task End:\", str(datetime.now() - stopped_at))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
