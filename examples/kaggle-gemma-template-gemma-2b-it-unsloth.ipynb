{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10353768,"sourceType":"datasetVersion","datasetId":6411678},{"sourceId":104623,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72254,"modelId":76277}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma Template\n\nThis library was developed for the Kaggle challenge:\n[**Google - Unlocking Global Communication with Gemma**](https://www.kaggle.com/competitions/gemma-language-tuning), sponsored by Google.\n\n## Credit Requirement\n\n**Important:** If you are a participant in the competition and wish to use this source code in your submission,\nyou must clearly credit the original author before the competition's end date, **January 14, 2025**.\n\nPlease include the following information in your submission:\n\n```text\nAuthor: Tu Pham\nKaggle Username: [bigfishdev](https://www.kaggle.com/bigfishdev)\nGitHub: [https://github.com/thewebscraping/gemma-template/](https://github.com/thewebscraping/gemma-template)\nLinkedIn: [https://www.linkedin.com/in/thetwofarm](https://www.linkedin.com/in/thetwofarm)\n```\n\n# Overview\n\nGemma Template is a lightweight and efficient Python library for generating templates to fine-tune models and craft prompts.\nDesigned for flexibility, it seamlessly supports Gemma, LLaMA, and other language frameworks, offering fast, user-friendly customization.\nWith multilingual capabilities and advanced configuration options, it ensures precise, professional, and dynamic template creation.\n\n### Learning Process and Acknowledgements\nAs a newbie, I created Gemma Template based on what I read and learned from the following sources:\n\n- Google Gemma Cookbook: [Advanced Prompting Techniques](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Advanced_Prompting_Techniques.ipynb)\n- Google Gemma Cookbook: [Finetune with LLaMA Factory](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetune_with_LLaMA_Factory.ipynb)\n- Google Gemma Cookbook: [Fine tuning Gemma for Function Calling](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetuning_Gemma_for_Function_Calling.ipynb)\n- Alpaca: [Alpaca Lora Documentation](https://github.com/tloen/alpaca-lora)\n- Unsloth: [Finetune Llama 3.2, Mistral, Phi-3.5, Qwen 2.5 & Gemma 2-5x faster with 80% less memory!](https://github.com/unslothai/unsloth)\n\n\nGemma Template supports exporting dataset files in three formats: `Text`, `Alpaca`, and `OpenAI`.\n\n# Multilingual Content Writing Assistant\n\nThis writing assistant is a multilingual professional writer specializing in crafting structured, engaging, and SEO-optimized content.\nIt enhances text readability, aligns with linguistic nuances, and preserves original context across various languages.\n\n---\n\n## Key Features:\n#### 1. **Creative and Engaging Rewrites**\n- Transforms input text into captivating and reader-friendly content.\n- Utilizes vivid imagery and descriptive language to enhance engagement.\n\n#### 2. **Advanced Text Analysis**\n- Processes text with unigrams, bigrams, and trigrams to understand linguistic patterns.\n- Ensures language-specific nuances and cultural integrity are preserved.\n\n#### 3. **SEO-Optimized Responses**\n- Incorporates keywords naturally to improve search engine visibility.\n- Aligns rewritten content with SEO best practices for discoverability.\n\n#### 4. **Professional and Multilingual Expertise**\n- Full support for creating templates in local languages.\n- Supports multiple languages with advanced prompting techniques.\n- Vocabulary and grammar enhancement with unigrams, bigrams, and trigrams instruction template.\n- Supports hidden mask input text. Adapts tone and style to maintain professionalism and clarity.\n- Full documentation with easy configuration prompts and examples.\n\n#### 5. **Customize Advanced Response Structure and Dataset Format**\n- Supports advanced response structure format customization.\n- Compatible with other models such as LLaMa.\n- Enhances dynamic prompts using Round-Robin loops.\n- Outputs multiple formats such as Text, Alpaca and OpenAI.\n\n**Installation**\n----------------\n\nTo install the library, you can choose between two methods:\n\n#### **1\\. Install via PyPI:**\n\n```shell\npip install gemma-template\n```\n\n#### **2\\. Install via GitHub Repository:**\n\n```shell\npip install git+https://github.com/thewebscraping/gemma-template.git\n```\n\n**Quick Start**\n----------------\nStart using Gemma Template with just a few lines of code:\n\n## Load Dataset\nReturns: A Hugging Face Dataset or DatasetDict object containing the processed prompts.\n\n**Load Dataset from data dict**\n```python\nprompt_instance = Template()\ndata_dict = [\n    {\n        \"id\": \"JnZJolR76_u2\",\n        \"title\": \"Sample title\",\n        \"description\": \"Sample description\",\n        \"document\": \"Sample document\",\n        \"categories\": [\"Topic 1\", \"Topic 2\"],\n        \"tags\": [\"Tag 1\", \"Tag 2\"],\n        \"output\": \"Sample output\",\n        \"main_points\": [\"Main point 1\", \"Main point 2\"],\n    }\n]\ndataset = prompt_instance.load_dataset(data_dict, output_format='text')   # enum: `text`, `alpaca` and `openai`.\nprint(dataset['text'][0])\n```\n\n**Load Dataset from HuggingFace**\n```python\ndataset = gemma_template.load_dataset(\n    \"YOUR_JSON_FILE_PATH_OR_HUGGINGFACE_DATASET\",\n    # enum: `text`, `alpaca` and `openai`.\n    output_format='text',\n    # Percentage of documents that need to be word masked.\n    # Min: 0, Max: 1. Default: 0.\n    max_hidden_ratio=.1,\n    # Replace 10% of words in the input document with '_____'.\n    # Use int to extract the correct number of words. The `max_hidden_ratio` parameter must be greater than 0.\n    max_hidden_words=.1,\n    # Minimum character of a word, used to create unigrams, bigrams, and trigrams. Default is 2.\n    min_chars_length=2,\n    # Maximum character of a word, used to create unigrams, bigrams and trigrams. Default is 0.\n    max_chars_length=8,\n)\n```\n\n## Fully Customized Template\n\n```python\nfrom gemma_template import Template, FieldPosition, INPUT_TEMPLATE, OUTPUT_TEMPLATE, INSTRUCTION_TEMPLATE, PROMPT_TEMPLATE\n\ntemplate_instance = Template(\n    instruction_template=[INSTRUCTION_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    prompt_template=[PROMPT_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    input_template=[INPUT_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    output_template=[OUTPUT_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    position=FieldPosition(\n            title=[\"Custom Title\"],\n            description=[\"Custom Description\"],\n            document=[\"Custom Article\"],\n            main_points=[\"Custom Main Points\"],\n            categories=[\"Custom Categories\"],\n            tags=[\"Custom Tags\"],\n    ),  # Optional: dynamic Round-Robin loops\n)\n\nresponse = template_instance.apply_template(\n    title=\"Gemma open models\",\n    description=\"Gemma: Introducing new state-of-the-art open models.\",\n    main_points=[\"Main point 1\", \"Main point 2\"],\n    categories=[\"Artificial Intelligence\", \"Gemma\"],\n    tags=[\"AI\", \"LLM\", \"Google\"],\n    document=\"Gemma open models are built from the same research and technology as Gemini models. Gemma 2 comes in 2B, 9B and 27B and Gemma 1 comes in 2B and 7B sizes.\",\n    output=\"A new family of open language models demonstrating strong performance across academic benchmarks for language understanding, reasoning, and safety.\",\n    max_hidden_words=.1,  # set 0 if you don't want to hide words.\n    min_chars_length=2,  # Minimum character of a word, used to create unigrams, bigrams, and trigrams. Default is 2.\n    max_chars_length=0,  # Maximum character of a word, used to create unigrams, bigrams and trigrams. Default is 0.\n)  # remove kwargs if not used.\n\nprint(response)\n```\n\n### Output:\n\n```text\n<start_of_turn>user\nYou are a multilingual professional writer.\n\n# Role:\nYou are a highly skilled professional content writer, linguistic analyst, and multilingual expert specializing in structured writing and advanced text processing.\n\n# Task:\nYour primary objectives are:\n1. Simplification: Rewrite the input text or document to ensure it is accessible and easy to understand for a general audience while preserving the original meaning and essential details.\n2. Lexical and Grammatical Analysis: Analyze and refine vocabulary and grammar using unigrams (single words), bigrams (two words), and trigrams (three words) to enhance readability and depth.\n3. Structure and Organization: Ensure your response adheres strictly to the prescribed structure format.\n4. Language Consistency: Respond in the same language as the input text unless explicitly directed otherwise.\n\n# Additional Guidelines:\n1. Provide a rewritten, enhanced version of the input text, ensuring professionalism, clarity, and improved structure.\n2. Focus on multilingual proficiency, using complex vocabulary, grammar to improve your responses.\n3. Preserve the context and cultural nuances of the original text when rewriting.\n\n# Text Analysis:\nExample 1: Unigrams (single words)\nand => English\nbuilt => English\nfrom => English\nthe => English\nresearch => English\nText Analysis 3: These are common English words, indicating the text is in English.\n\nExample 2: Bigrams (two words)\ntechnology as => English\nText Analysis 2: Frequent bigrams in English confirm the language context.\n\nExample 3: Trigrams (three words)\ntechnology as Gemini => English\nText Analysis 3: Trigrams further validate the linguistic analysis and the necessity to respond in English.\n\n# Conclusion of Text Analysis:\nThe linguistic analysis confirms the text is predominantly in English. Consequently, the response should be structured and written in English to align with the original text and context.\n\n# Input Text:\nRewrite the input text or document to highlight its unique value proposition while ensuring it ranks well for targeted keywords.\n\n# Response Structure Format:\nYou must follow the response structure:\n\n**Custom Title (Title):** Rewrite the title to maximize clarity, appeal, and relevance to the content.\n**Custom Description (Description):** Create a description focusing on how the article addresses a common problem or challenge readers face.\n**Custom Article (Article):** Rewrite the input text or document with an authoritative tone, incorporating credible sources, data, and references to boost trustworthiness and SEO ranking.\n**Custom Main Points (Main Points):** Ensure all key points flow logically from one to the next.\n**Custom Categories (Categories):** Use categories that align with similar articles on the topic and improve SEO and discoverability.\n**Custom Tags (Tags):** Rewrite tags to make them more specific and targeted.\n\nBy adhering to this format, the response will maintain linguistic integrity while enhancing professionalism, structure and alignment with user expectations.\n\n# Text:\nGemma open models are built _____ the same _____ and technology as Gemini models. Gemma 2 comes in 2B, 9B _____ 27B and Gemma 1 comes in 2B and 7B sizes.<end_of_turn>\n<start_of_turn>model\n## **Custom Title:**\n### Gemma open models\n\n## **Custom Description:**\nGemma: Introducing new state-of-the-art open models.\n\n## **Custom Article:**\nA new family of open language models demonstrating strong performance across academic benchmarks for language understanding, reasoning, and safety.\n\n## **Custom Main Points:**\n* Main point 1\n* Main point 2\n\n## **Custom Categories:**\n* Artificial Intelligence\n* Gemma\n\n## **Custom Tags:**\n* AI\n* LLM\n* Google<end_of_turn>\n\n```\n","metadata":{}},{"cell_type":"code","source":"import time\nfrom datetime import datetime, timedelta\nstopped_at = datetime.now() + timedelta(hours=11, minutes=30)\nstart_at = time.perf_counter()\nprint(\"Start time:\", str(stopped_at))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:32:21.427856Z","iopub.execute_input":"2025-01-01T12:32:21.428213Z","iopub.status.idle":"2025-01-01T12:32:21.442386Z","shell.execute_reply.started":"2025-01-01T12:32:21.428180Z","shell.execute_reply":"2025-01-01T12:32:21.441282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Install Gemma Template dependencies to generate template","metadata":{}},{"cell_type":"code","source":"!pip install -q -U gemma-template\n!pip install -q evaluate rouge_score sacrebleu nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:32:21.443495Z","iopub.execute_input":"2025-01-01T12:32:21.443842Z","iopub.status.idle":"2025-01-01T12:32:29.956542Z","shell.execute_reply.started":"2025-01-01T12:32:21.443804Z","shell.execute_reply":"2025-01-01T12:32:29.955256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Install Unsloth dependencies for Fine Tuning","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:32:29.957696Z","iopub.execute_input":"2025-01-01T12:32:29.958009Z","iopub.status.idle":"2025-01-01T12:33:34.379834Z","shell.execute_reply.started":"2025-01-01T12:32:29.957982Z","shell.execute_reply":"2025-01-01T12:33:34.376157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install -q unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:33:34.382500Z","iopub.execute_input":"2025-01-01T12:33:34.383058Z","iopub.status.idle":"2025-01-01T12:36:35.830496Z","shell.execute_reply.started":"2025-01-01T12:33:34.382997Z","shell.execute_reply":"2025-01-01T12:36:35.829039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport json\nimport random\nfrom pathlib import Path\n\nmodel_name = \"/kaggle/input/gemma-2/transformers/gemma-2-2b-it/2/\"\nproject_id = \"gemma-template-gemma-2-2b-it-test-v2\"\n\nseed = 3407\n\nif 'google.colab' in sys.modules:\n    # Running on Colab\n    from google.colab import userdata\n    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_TOKEN')\nelif os.path.exists('/kaggle/working'):\n    # Running on Kaggle\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    os.environ['HF_TOKEN'] = user_secrets.get_secret(\"HF_TOKEN\")\n    os.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_TOKEN\")\nelse:\n    # Not running on Colab or Kaggle\n    raise EnvironmentError('This notebook is designed to run on Google Colab or Kaggle.')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:35.832034Z","iopub.execute_input":"2025-01-01T12:36:35.832440Z","iopub.status.idle":"2025-01-01T12:36:36.061679Z","shell.execute_reply.started":"2025-01-01T12:36:35.832393Z","shell.execute_reply":"2025-01-01T12:36:36.060714Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Configure Unsloth 4bit-quantized\n\nRead document here: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)","metadata":{}},{"cell_type":"code","source":"try:\n    from unsloth import FastLanguageModel\n    import torch\n    max_seq_length = 3072 # Choose any! We auto support RoPE Scaling internally!\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = model_name,\n        max_seq_length = max_seq_length,\n        dtype = None,\n        load_in_4bit = True,\n    )\n    \n    model = FastLanguageModel.get_peft_model(\n        model,\n        r = 16,\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n        lora_alpha = 16,\n        lora_dropout = 0,\n        bias = \"none\",\n        use_gradient_checkpointing = \"unsloth\",\n        random_state = 3407,\n        use_rslora = False,\n        loftq_config = None,\n    )\n    \n    model.config.use_cache = False\n    model.print_trainable_parameters()\nexcept (Exception, RuntimeError):\n    # Test template, tokenizer required.\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:36.065343Z","iopub.execute_input":"2025-01-01T12:36:36.065653Z","iopub.status.idle":"2025-01-01T12:36:43.272205Z","shell.execute_reply.started":"2025-01-01T12:36:36.065627Z","shell.execute_reply":"2025-01-01T12:36:43.271253Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Configure Tokenizer Padding Side\n\nFix overflow issue with bf16/fp16 training. See also: [Google Gemma Cookbook: Finetuning Gemma for Function Calling](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetuning_Gemma_for_Function_Calling.ipynb)","metadata":{}},{"cell_type":"code","source":"tokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:43.274143Z","iopub.execute_input":"2025-01-01T12:36:43.274737Z","iopub.status.idle":"2025-01-01T12:36:43.278717Z","shell.execute_reply.started":"2025-01-01T12:36:43.274696Z","shell.execute_reply":"2025-01-01T12:36:43.277784Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Preparation\n\nThe train dataset consists of 10,000 rows processed to fine-tune models for content writing. The eval dataset consists of 10,000 rows.\n\n**Example:**\n\n| ID             | Title         | Description        | Document         | Categories        | Tags         | Output          | Main Points            |\n|----------------|---------------|--------------------|------------------|-------------------|--------------|-----------------|------------------------|\n| JnZJolR76_u2   | Sample title  | Sample description | Sample document  | [\"Topic 1\", \"Topic 2\"]  | [\"Tag 1\", \"Tag 2\"] | Sample output   | [\"MP 1\", \"MP 2\"]       |\n\n\n## Data Source and Timeline\n\nThe data was collected between September 2024 and December 2024 from 50 news pages. The output response data is generate by LLaMa 3, Gemma 7B, and Gemma 9B.\n\n## Language Distribution\n\nThe dataset maintains a balanced language ratio:\n- **50% English**\n- **50% Vietnamese**\n\n## Dataset Composition\n\nThe data distribution is as follows:\n\n- **10%**: Combined responses including title and document.\n- **10%**: Combined responses including title, document, and description.\n- **10%**: Combined responses including title, document, and main points.\n- **10%**: Combined responses including title, document, categories, and tags.\n- **10%**: Responses without instructional templates, structured format only.\n- **10%**: Converted into GPT-style conversations with default output as a response document and random additional fields (e.g., title, description, main points, categories, tags).\n- **40%**: Responses using fully instructions and a prompt structure format.\n- `warmup_ratio`: 5%.\n\n## Additional Features\n\n- **15%** of the dataset incorporates hidden masked words, applied at a ratio of **5% of words** within each document.","metadata":{}},{"cell_type":"code","source":"from gemma_template import Template, gemma_template, vietnamese_gemma_template\nfrom gemma_template.__version__ import __version__\nprint(__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:43.279728Z","iopub.execute_input":"2025-01-01T12:36:43.280114Z","iopub.status.idle":"2025-01-01T12:36:44.858543Z","shell.execute_reply.started":"2025-01-01T12:36:43.280085Z","shell.execute_reply":"2025-01-01T12:36:44.857669Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import Gemma Template\nImport default Gemma Template for generate template.","metadata":{}},{"cell_type":"code","source":"from gemma_template import Template, FieldPosition, GEMMA_TEMPLATE, INPUT_TEMPLATE, OUTPUT_TEMPLATE, INSTRUCTION_TEMPLATE, PROMPT_TEMPLATE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:44.859316Z","iopub.execute_input":"2025-01-01T12:36:44.859765Z","iopub.status.idle":"2025-01-01T12:36:44.864319Z","shell.execute_reply.started":"2025-01-01T12:36:44.859739Z","shell.execute_reply":"2025-01-01T12:36:44.863133Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Gemma Template\n\nDefault templates by Gemma Template using Jinja2.\n\nUsing Gemma formatting: [https://ai.google.dev/gemma/docs/formatting](https://ai.google.dev/gemma/docs/formatting)","metadata":{}},{"cell_type":"markdown","source":"### **GEMMA_TEMPLATE**\nDefault input template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"<start_of_turn>user\n{{ input }}<end_of_turn>\n<start_of_turn>model\n{{ output }}<end_of_turn>\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:44.865515Z","iopub.execute_input":"2025-01-01T12:36:44.865957Z","iopub.status.idle":"2025-01-01T12:36:45.166983Z","shell.execute_reply.started":"2025-01-01T12:36:44.865903Z","shell.execute_reply":"2025-01-01T12:36:45.166024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **INPUT_TEMPLATE**\nDefault input template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"{{ system_prompt }}\n{% if instruction %}\\n{{ instruction }}\\n{% endif %}\n{% if prompt_structure %}{{ prompt_structure }}\\n{% else %}{{ prompt }}\\n{% endif %}\n# Text:\n{{ input }}\n{% if topic_value %}\\nTopics: {{ topic_value }}\\n{% endif %}{% if keyword_value %}Keywords: {{ keyword_value }}\\n{% endif %}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.167851Z","iopub.execute_input":"2025-01-01T12:36:45.168172Z","iopub.status.idle":"2025-01-01T12:36:45.183947Z","shell.execute_reply.started":"2025-01-01T12:36:45.168144Z","shell.execute_reply":"2025-01-01T12:36:45.182941Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **OUTPUT_TEMPLATE**\nDefault output template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"{% if structure_fields %}{% for field in structure_fields %}## **{{ field.label.custom or field.label.default }}:**\\n{% if field.key == 'title' %}### {% endif%}{{ field.value }}\\n\\n{% endfor %}{% else %}{{ output }}{% endif %}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.184961Z","iopub.execute_input":"2025-01-01T12:36:45.185380Z","iopub.status.idle":"2025-01-01T12:36:45.202012Z","shell.execute_reply.started":"2025-01-01T12:36:45.185340Z","shell.execute_reply":"2025-01-01T12:36:45.201023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **INSTRUCTION_TEMPLATE**\nDefault instruction template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"# Role:\nYou are a highly skilled professional content writer, linguistic analyst, and multilingual expert specializing in structured writing and advanced text processing.\n\n# Task:\nYour primary objectives are:\n1. Simplification: Rewrite the input text or document to ensure it is accessible and easy to understand for a general audience while preserving the original meaning and essential details.\n2. Lexical and Grammatical Analysis: Analyze and refine vocabulary and grammar using unigrams (single words), bigrams (two words), and trigrams (three words) to enhance readability and depth.\n3. Structure and Organization: Ensure your response adheres strictly to the prescribed structure format.\n4. Language Consistency: Respond in the same language as the input text unless explicitly directed otherwise.\n\n# Additional Guidelines:\n1. Provide a rewritten, enhanced version of the input text, ensuring professionalism, clarity, and improved structure.\n2. Focus on multilingual proficiency, using complex vocabulary, grammar to improve your responses.\n3. Preserve the context and cultural nuances of the original text when rewriting.\n\n# Text Analysis:\nExample 1: Unigrams (single words){% for word in unigrams %}\\n{{ word }} => {{ language }}{% endfor %}\nText Analysis 3: These are common {{ language }} words, indicating the text is in {{ language }}.\n\nExample 2: Bigrams (two words){% for word in bigrams %}\\n{{ word }} => {{ language }}{% endfor %}\nText Analysis 2: Frequent bigrams in {{ language }} confirm the language context.\n\nExample 3: Trigrams (three words){% for word in trigrams %}\\n{{ word }} => {{ language }}{% endfor %}\nText Analysis 3: Trigrams further validate the linguistic analysis and the necessity to respond in {{ language }}.\n\n# Conclusion of Text Analysis:\nThe linguistic analysis confirms the text is predominantly in {{ language }}. Consequently, the response should be structured and written in {{ language }} to align with the original text and context.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.203129Z","iopub.execute_input":"2025-01-01T12:36:45.203461Z","iopub.status.idle":"2025-01-01T12:36:45.223188Z","shell.execute_reply.started":"2025-01-01T12:36:45.203420Z","shell.execute_reply":"2025-01-01T12:36:45.222208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **PROMPT_TEMPLATE**\nDefault prompt template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"{% if prompt %}\\n\\n# Input Text:\\n{{ prompt }}\\n\\n{% endif %}{% if structure_fields %}# Response Structure Format:\nYou must follow the response structure:\n\n{% for field in structure_fields %}{{ field.label }}\\n{% endfor %}\nBy adhering to this format, the response will maintain linguistic integrity while enhancing professionalism, structure and alignment with user expectations.\\n\n{% endif %}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.224511Z","iopub.execute_input":"2025-01-01T12:36:45.224852Z","iopub.status.idle":"2025-01-01T12:36:45.242822Z","shell.execute_reply.started":"2025-01-01T12:36:45.224823Z","shell.execute_reply":"2025-01-01T12:36:45.241856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Example Output: Document Text with Hidden Mask (`document`)**","metadata":{}},{"cell_type":"code","source":"\"\"\"Gemma open models are built _____ the same _____ and technology as Gemini models. Gemma 2 comes in 2B, 9B _____ 27B and Gemma 1 comes in 2B and 7B sizes.\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.243933Z","iopub.execute_input":"2025-01-01T12:36:45.244313Z","iopub.status.idle":"2025-01-01T12:36:45.261192Z","shell.execute_reply.started":"2025-01-01T12:36:45.244273Z","shell.execute_reply":"2025-01-01T12:36:45.260118Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Example Output: Document Text without Hidden Mask  (`document`)**","metadata":{}},{"cell_type":"code","source":"\"\"\"Gemma open models are built from the same research and technology as Gemini models. Gemma 2 comes in 2B, 9B and 27B and Gemma 1 comes in 2B and 7B sizes.\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.262227Z","iopub.execute_input":"2025-01-01T12:36:45.262496Z","iopub.status.idle":"2025-01-01T12:36:45.278140Z","shell.execute_reply.started":"2025-01-01T12:36:45.262471Z","shell.execute_reply":"2025-01-01T12:36:45.277020Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Process Function Documentation\n\n## Overview\n\nThis repository contains tools for processing datasets to fine-tune language models. The `process_fn` function is central to preparing datasets in various formats such as `text`, `alpaca` and `openai`. It supports frameworks like `unsloth` and `LLaMA-Factory`.\n\n## Features\n\n- Generate unigrams, bigrams, and trigrams with customizable configurations.\n- Automatically exclude invalid keywords containing numbers or punctuation.\n- Supports both English and Vietnamese datasets, with specific configurations for each language.\n- Creates datasets for conversation-based models by dynamically generating conversational structures.\n\n## How It Works\n\n- **Language Distribution**:\n  - 50% English Dataset, with 10% used for evaluation.\n  - 50% Vietnamese Dataset, with 10% used for evaluation.\n\n- **Word Configuration**:\n  - **Unigrams**: Unique words with 2-8 characters. Reason: 1 character will match English like `I`, `a`, 8 characters because the longest word in Vietnamese is 7 characters.\n  - **Bigrams**: Unique bigrams words, not included words of unigrams list.\n  - **Trigrams**: Pairs of unigrams and bigrams.\n  - Ensures the AI vocabulary grows by at least 15 words per document.\n\n- **Special Features**:\n  - Hidden mask words applied to **15%** of the dataset, at a **5% ratio of words** per document.\n  - Generate conversations dataset with configurable fields and output format.","metadata":{}},{"cell_type":"markdown","source":"### Load Dataset\nLoad Dataset from Json File Kaggle host or HuggingFace Hub. See also: [https://huggingface.co/datasets/twodev/gemma-template](https://huggingface.co/datasets/twodev/gemma-template)","metadata":{}},{"cell_type":"code","source":"import json\nfrom datasets import Dataset, DatasetDict, load_dataset\n\ndef load_from_json_file(path: str = \"/kaggle/input/gemma-template/train-gemma-template.json\") -> Dataset:\n    with open(path, encoding=\"utf-8\") as f:\n        data = json.load(f)\n        return Dataset.from_list(data)\n\n\ndef load_from_huggingface_hub(path: str = \"twodev/gemma-template\", split: str = \"train\") -> Dataset:\n    dataset = load_dataset(\"twodev/gemma-template\")\n    return dataset[split]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.279261Z","iopub.execute_input":"2025-01-01T12:36:45.279594Z","iopub.status.idle":"2025-01-01T12:36:49.445342Z","shell.execute_reply.started":"2025-01-01T12:36:45.279556Z","shell.execute_reply":"2025-01-01T12:36:49.444384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_conversations_dataset(data: Dataset, mapping_field: dict[str, list[str]]) -> Dataset:\n    \"\"\"\n    Converts a dataset into a conversational format suitable for fine-tuning language models.\n\n    Notes:\n        - The `gemma_template._gen_bullet_list_style` method is used to format `openai` responses as a `number`, `dash` and `asterisk` bullet list when the field value is a list.\n    \"\"\"\n    \n    template = \"\"\"<start_of_turn>user\\n{input}<end_of_turn>\\n<start_of_turn>model\\n{output}<end_of_turn>\"\"\"\n    outputs = []\n    for item in data:\n        messages = item[\"messages\"]\n        for field in mapping_field:\n            if field in item[\"origin_data\"] and mapping_field[field]:\n                value = item[\"origin_data\"][field]\n                messages.append(\n                    {\n                        \"role\": \"user\",\n                        \"content\": random.choice(mapping_field[field]),\n                    }\n                )\n                messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": (\n                            gemma_template._generate_bullet_style(value, \"asterisk\")\n                            if isinstance(value, list)\n                            else value\n                        ),\n                    }\n                )\n\n        outputs_ = [\n            template.format(\n                input=\"\\n\\n\".join([messages[0]['content'], messages[1]['content']]),\n                output=messages[2]['content']\n            )\n        ]\n\n        if len(messages) > 3:\n            prompts = []\n            for idx in range(3, len(messages), 2):\n                try:\n                    prompt = template.format(\n                        input=messages[idx][\"content\"],\n                        output=messages[idx + 1][\"content\"],\n                    )\n                    prompts.append(prompt.strip())\n                except IndexError:\n                    pass\n\n            random.shuffle(prompts)\n            outputs_.extend(prompts)\n\n        outputs_.append(\"\")\n        sep = tokenizer.eos_token + \"\\n\"\n        outputs.append({\"text\": sep.join(outputs_)})\n\n    if outputs:\n        return Dataset.from_list(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.446344Z","iopub.execute_input":"2025-01-01T12:36:49.446605Z","iopub.status.idle":"2025-01-01T12:36:49.455121Z","shell.execute_reply.started":"2025-01-01T12:36:49.446582Z","shell.execute_reply":"2025-01-01T12:36:49.454024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_fn(\n    instance: Template,\n    data: Dataset, \n    excluded_fields: list[str] = (), \n    output_format = 'text',\n    max_hidden_ratio = 0.15, \n    max_hidden_words = 0.05, \n    min_chars_length = 2, \n    max_chars_length = 8,\n    max_concurrency: int = 4,\n    n_words = 5,\n    is_remove_data = True,\n    **kwargs\n) -> Dataset:\n    \"\"\"\n    Processes a dataset for fine-tuning language models, supporting formats like `text`, `alpaca`, and `gpt`.\n\n    Args:\n        instance (Template): A template instance for dataset processing.\n        data (Dataset): The input dataset to be processed.\n        excluded_fields (list[str]): Fields to exclude when generating conversational datasets.\n        output_format (str): Format of the processed dataset. Options are 'text', 'alpaca', and 'gpt'.\n        max_hidden_ratio (Union[float]):\n            Percentage of documents that need to be word masked. Min: 0, Max: 1. Default: 0.\n        max_hidden_words (Optional[str]):\n            Replace words in the document with '____'. The `max_hidden` parameter must be greater than 0.\n            Use `int`: exact number of words to be masked, `float`: percentage of number of words to be masked.\n        min_chars_length (int):\n            Minimum character of a word, used to create unigrams, bigrams, and trigrams. Default is 2.\n        max_chars_length (int):\n            Maximum character of a word, used to create unigrams, bigrams and trigrams. Default is 0.\n        max_concurrency (int):\n            Maximum number of concurrent threads for processing data. Default is 4.\n        n_words (int): Number of words frequently used to create unigrams, bigrams and trigrams.\n        is_remove_data (bool): Whether to remove specific fields from the dataset. Defaults to True.\n        **kwargs: Additional configuration parameters.\n\n    Returns:\n        Dataset or DatasetDict: The processed dataset in the specified format.\n\n    Notes:\n        - The `output_format` parameter determines the dataset's structure:\n            - `'text'`: Standard format for unsloth fine-tuning.\n            - `'alpaca'` or `'openai'`: Formats for frameworks like LLaMA-Factory.\n        - Using `output_format='openai'` and `is_remove_data=False` with `excluded_fields` generates conversational datasets.\n\n    \"\"\"\n    ds = instance.load_dataset(\n        data, \n        output_format=output_format, \n        excluded_fields=excluded_fields,\n        max_hidden_ratio=max_hidden_ratio, \n        max_hidden_words=max_hidden_words, \n        min_chars_length=min_chars_length, \n        max_chars_length=max_chars_length,\n        max_concurrency=max_concurrency,\n        n_words=n_words,\n        is_close_async_loop=False,  # Avoid `RuntimeError` by Notebook\n        is_remove_data=is_remove_data,\n    )\n    if output_format == 'openai' and not is_remove_data:\n        mapping_field = {\n            field: getattr(instance, field, None)\n            for field in excluded_fields\n            if getattr(instance, field, None)\n        }\n        ds = convert_to_conversations_dataset(ds, mapping_field=mapping_field)\n    else:\n        ds = ds.map(lambda x: {\"text\": [text + tokenizer.eos_token for text in x[\"text\"]]}, batched = True)  # Append eos token.\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.455995Z","iopub.execute_input":"2025-01-01T12:36:49.456272Z","iopub.status.idle":"2025-01-01T12:36:49.480898Z","shell.execute_reply.started":"2025-01-01T12:36:49.456246Z","shell.execute_reply":"2025-01-01T12:36:49.479648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_verify(data, is_masked: bool = True, task_name: str = \"TASK\"):\n    print(task_name + \"*\" * 45)\n    for item in data:\n        if item.get(\"is_masked\") == is_masked:\n            if is_masked:\n                print(\"HIDDEN TEXT: YES\" + \"*\" * 45)\n            else:\n                print(\"HIDDEN TEXT: NO\" + \"*\" * 45)\n                \n            print(\"\\n\")\n            print(item['text'])\n            print(\"=\" * 60)\n            print(\"*\" * 30, \" DATA ATTRS \", \"*\" * 30)\n            print(\"Masked Text:\", item['is_masked'])\n            print(\"Language Code:\", item['analysis']['language_code'])\n            print(\"Language:\", item['analysis']['language'])\n            print(\"Categories:\", item['analysis']['topic_value'])\n            print(\"Keywords:\", item['analysis']['keyword_value'])\n            print(\"Unigrams:\", item['analysis']['unigrams'])\n            print(\"Bigrams:\", item['analysis']['bigrams'])\n            print(\"Trigrams:\", item['analysis']['trigrams'])\n            print(\"VALID TASK: YES\")\n            print(\"*\" * 30, \" TASK DONE \", \"*\" * 30)\n            print(\"=\" * 60)\n            print(\"\\n\")\n            \n            return\n\n    print(\"VALID TASK: NO\")\n    print(\"*\" * 30, \" TASK DONE \", \"*\" * 30)\n    print(\"=\" * 60)\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.485269Z","iopub.execute_input":"2025-01-01T12:36:49.485659Z","iopub.status.idle":"2025-01-01T12:36:49.503313Z","shell.execute_reply.started":"2025-01-01T12:36:49.485611Z","shell.execute_reply":"2025-01-01T12:36:49.502245Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cases Handled\n\nThe dataset processing involves various configurations to handle diverse cases effectively:\n\n1. **Combined Responses**:\n   - **10%**: Responses include the `title` and `document`.\n   - **10%**: Responses include the `title`, `document`, and `description`.\n   - **10%**: Responses include the `title`, `document`, and `main points`.\n   - **10%**: Responses include the `title`, `document`, `categories`, and `tags`.\n\n2. **Masked Words**:\n   - **15%**: Masked words are applied to the dataset, with a hidden ratio of **5% of words** for each document.\n\n3. **Language Detection**:\n   - Language is automatically detected using the [`langdetect`](https://github.com/Mimino666/langdetect) library.","metadata":{}},{"cell_type":"code","source":"try:\n    dataset = load_from_json_file(\"/kaggle/input/gemma-template/train-gemma-template.json\")\nexcept FileNotFoundError:\n    dataset = load_from_huggingface_hub(\"twodev/gemma-template\")\n\ntotal_rows = len(dataset)\nfive_percent_idx = int(total_rows * 0.1)\n\ndataset_mapping = {\n    \"Combined response including title and document\": {\n        \"data\": dataset.select(range(0, five_percent_idx)),\n        \"excluded_fields\": [\"description\", \"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and description\": {\n        \"data\": dataset.select(range(five_percent_idx, five_percent_idx * 2)),\n        \"excluded_fields\": [\"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and main points\": {\n        \"data\": dataset.select(range(five_percent_idx * 2, five_percent_idx * 3)),\n        \"excluded_fields\": [\"description\", \"categories\", \"tags\"]\n        \n    },\n    \"Combined response including title, document and categories and tags\": {\n        \"data\": dataset.select(range(five_percent_idx * 3, five_percent_idx * 4)),\n        \"excluded_fields\": [\"description\", \"main_points\"]\n        \n    },\n}\n\nprint(dataset_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.505376Z","iopub.execute_input":"2025-01-01T12:36:49.505766Z","iopub.status.idle":"2025-01-01T12:36:49.538271Z","shell.execute_reply.started":"2025-01-01T12:36:49.505734Z","shell.execute_reply":"2025-01-01T12:36:49.537308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language_ratio_size = 0.5  # Ratio between English and local language.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.539436Z","iopub.execute_input":"2025-01-01T12:36:49.539732Z","iopub.status.idle":"2025-01-01T12:36:49.544475Z","shell.execute_reply.started":"2025-01-01T12:36:49.539704Z","shell.execute_reply":"2025-01-01T12:36:49.543603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_datasets = []\nfor task, item in dataset_mapping.items():\n    print(\"Prepare dataset for task:\", task)\n    split_dataset = item['data'].train_test_split(test_size=language_ratio_size)\n\n    # prepare dataset use instruction and structure English language.\n    english_dataset = process_fn(\n        gemma_template, \n        split_dataset[\"train\"], \n        excluded_fields=item['excluded_fields']\n    )\n    input_datasets.append(english_dataset)\n                                                       \n    # prepare dataset use instruction and structure Vietnamese language.\n    vietnamese_dataset = process_fn(\n        vietnamese_gemma_template, \n        split_dataset[\"test\"], \n        excluded_fields=item['excluded_fields']\n    )\n    \n    input_datasets.append(vietnamese_dataset)\n    \n# Test with hidden mask test\nprint_verify(english_dataset, is_masked=True, task_name=\"ENGLISH VERSION: {}\".format(task.upper()))\n# Test without hidden mask\nprint_verify(vietnamese_dataset, is_masked=False, task_name=\"VIETNAMESE VERSION: {}\".format(task.upper()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.545509Z","iopub.execute_input":"2025-01-01T12:36:49.545861Z","iopub.status.idle":"2025-01-01T12:45:30.063757Z","shell.execute_reply.started":"2025-01-01T12:36:49.545822Z","shell.execute_reply":"2025-01-01T12:45:30.062749Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n**Hidden Mask**:  The first **15%** of batches will include hidden mask words.\n\nThe dataset processing works perfectly across both language versions:\n\n1. **English**: Includes hidden word masks as expected.\n2. **Vietnamese**: No hidden word masks applied.\n\n## Common Observations\n- **Advanced Features**:\n  - Instruction included: **Unigrams**, **bigrams**, and **trigrams** are generated correctly.\n  - Structured formatting functions as intended.","metadata":{}},{"cell_type":"code","source":"print(input_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:45:30.064965Z","iopub.execute_input":"2025-01-01T12:45:30.065281Z","iopub.status.idle":"2025-01-01T12:45:30.070532Z","shell.execute_reply.started":"2025-01-01T12:45:30.065251Z","shell.execute_reply":"2025-01-01T12:45:30.069474Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare No Instruction Dataset\n- **10%**: Responses without instructional templates, fully structured format only.\n\nTo create a dataset without instructions:\n\n1. **Template Configuration**:  \n   - The creation process is similar to the standard template.  \n   - Set `instance.instruction_template=[]` to omit the instruction template during generation.\n\n2. **Dataset Merging**:  \n   - Append the generated dataset to `input_datasets` to allow merging of all datasets together.","metadata":{}},{"cell_type":"code","source":"# empty instruction template\ngemma_template.instruction_template = []  \nvietnamese_gemma_template.instruction_template = []\n\nno_instruction_dataset = dataset.select(range(five_percent_idx*4, five_percent_idx*5))\nprint(no_instruction_dataset)\n\nsplit_dataset = no_instruction_dataset.train_test_split(test_size=language_ratio_size)\n\n# prepare dataset use instruction and structure English language.\nenglish_no_instruction_dataset = process_fn(gemma_template, split_dataset[\"train\"])\ninput_datasets.append(english_no_instruction_dataset)\n                                                   \n# prepare dataset use instruction and structure Vietnamese language.\nvietnamese_no_instruction_dataset = process_fn(vietnamese_gemma_template,  split_dataset[\"test\"])\ninput_datasets.append(vietnamese_no_instruction_dataset)\n\n# print verify\nprint_verify(english_no_instruction_dataset, is_masked=False, task_name=\"ENGLISH NO INSTRUCTION VERSION\")\nprint_verify(vietnamese_no_instruction_dataset, is_masked=False, task_name=\"VIETNAMESE NO INSTRUCTION VERSION\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:45:30.071750Z","iopub.execute_input":"2025-01-01T12:45:30.072125Z","iopub.status.idle":"2025-01-01T12:51:50.265549Z","shell.execute_reply.started":"2025-01-01T12:45:30.072095Z","shell.execute_reply":"2025-01-01T12:51:50.264438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n- The instruction template has been deleted.\n- The structure template still works.","metadata":{}},{"cell_type":"markdown","source":"## Prepare Conversations Dataset\n- **10%**: Converted into GPT-style conversations with default output as a response document and random additional fields (e.g., title, description, main points, categories, tags).\n\n### Template Customization\n\n- Modify the template to follow the conversational format as outlined in the [Gemma documentation](https://ai.google.dev/gemma/docs/formatting).\n- I overwrite by adding eos token at the end of each model response.\n\n### Example Template\n\n```text\n<start_of_turn>user\n\nknock knock<end_of_turn><eos>\n\n<start_of_turn>model\n\nwho is there<end_of_turn><eos>\n\n<start_of_turn>user\n\nGemma<end_of_turn><eos>\n\n<start_of_turn>model\n\nGemma who?<end_of_turn><eos>\n```\n\n### Implementation","metadata":{}},{"cell_type":"code","source":"from gemma_template.constants import INSTRUCTION_TEMPLATE, VIETNAMESE_INSTRUCTION_TEMPLATE\n\n# Reset the template as instructed.\ngemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\nvietnamese_gemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\n\nconversations_dataset = dataset.select(range(five_percent_idx*5, five_percent_idx*6))\nsplit_dataset = conversations_dataset.train_test_split(test_size=language_ratio_size)\nexcluded_fields = [\"title\", \"description\", \"main_points\", \"categories\", \"tags\"]\n\n# prepare dataset use instruction and structure English language.\nenglish_conversations_dataset = process_fn(\n    gemma_template, \n    split_dataset[\"train\"], \n    excluded_fields=excluded_fields,\n    output_format=\"openai\",\n    is_remove_data=False,\n)\ninput_datasets.append(english_conversations_dataset)\n\n                                                   \n# prepare dataset use instruction and structure Vietnamese language.\nvietnamese_conversations_dataset = process_fn(\n    vietnamese_gemma_template, \n    split_dataset[\"test\"], \n    excluded_fields=excluded_fields,\n    output_format=\"openai\",\n    is_remove_data=False,\n)\ninput_datasets.append(vietnamese_conversations_dataset)\n\n# print verify\nprint(english_conversations_dataset['text'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:51:50.266706Z","iopub.execute_input":"2025-01-01T12:51:50.267134Z","iopub.status.idle":"2025-01-01T12:58:24.421252Z","shell.execute_reply.started":"2025-01-01T12:51:50.267091Z","shell.execute_reply":"2025-01-01T12:58:24.420048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n\n- The structure template still works. The GPT conversations structure matches.\n","metadata":{}},{"cell_type":"markdown","source":"## Prepare Fully Instruction and Prompt Structure Format\n- **40%**: Responses using fully instructions and a prompt structure format.\n","metadata":{}},{"cell_type":"code","source":"from gemma_template.constants import INSTRUCTION_TEMPLATE, VIETNAMESE_INSTRUCTION_TEMPLATE\n\n# Reset the template as instructed.\ngemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\nvietnamese_gemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\n\ninstruction_dataset = dataset.select(range(five_percent_idx*6, len(dataset)))\nprint(instruction_dataset)\n\nsplit_dataset = instruction_dataset.train_test_split(test_size=language_ratio_size)\n\n# prepare dataset use instruction and structure English language.\nenglish_instruction_dataset = process_fn(gemma_template, split_dataset[\"train\"])\ninput_datasets.append(english_instruction_dataset)\n                                                   \n# prepare dataset use instruction and structure Vietnamese language.\nvietnamese_instruction_dataset = process_fn(vietnamese_gemma_template,  split_dataset[\"test\"])\ninput_datasets.append(vietnamese_instruction_dataset)\n\n# print verify\nprint_verify(english_instruction_dataset, is_masked=True, task_name=\"ENGLISH INSTRUCTION VERSION\")\nprint_verify(vietnamese_instruction_dataset, is_masked=False, task_name=\"VIETNAMESE INSTRUCTION VERSION\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:58:24.422625Z","iopub.execute_input":"2025-01-01T12:58:24.423112Z","iopub.status.idle":"2025-01-01T13:21:38.633292Z","shell.execute_reply.started":"2025-01-01T12:58:24.423079Z","shell.execute_reply":"2025-01-01T13:21:38.632038Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n\n- The instruction and structure template still works.\n","metadata":{}},{"cell_type":"markdown","source":"## Merged Dataset\n- A special token representing the end of a text template.\n- Split 5% for warm up and merge all dataset together.\n- **Shuffle dataset**: shuffling the data helps prevent bias during training, ensures randomness in batch selection, and prevents the model from learning patterns based on the order of the data.","metadata":{}},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\nwramup_dataset, train_dataset = [], []\nfor input_dataset in input_datasets:\n    split_dataset = input_dataset.train_test_split(test_size=0.05)\n    wramup_dataset.append(split_dataset['test'])\n    train_dataset.append(split_dataset['train'])\n    \nwramup_dataset = concatenate_datasets(wramup_dataset).shuffle(seed=seed)\ntrain_dataset = concatenate_datasets(train_dataset).shuffle(seed=seed)\ntrain_dataset = concatenate_datasets([wramup_dataset, train_dataset])\n\n# verify dataset\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.634538Z","iopub.execute_input":"2025-01-01T13:21:38.634983Z","iopub.status.idle":"2025-01-01T13:21:38.767498Z","shell.execute_reply.started":"2025-01-01T13:21:38.634938Z","shell.execute_reply":"2025-01-01T13:21:38.766619Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **All Good! Now we begin to configure the Trainer for fine-tuning.**","metadata":{}},{"cell_type":"code","source":"elapsed = stopped_at - datetime.now()\nprint(\"Elapsed prepared dataset: %s. Took: %.2f seconds\" % (str(elapsed), elapsed.total_seconds()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.768496Z","iopub.execute_input":"2025-01-01T13:21:38.768764Z","iopub.status.idle":"2025-01-01T13:21:38.774330Z","shell.execute_reply.started":"2025-01-01T13:21:38.768739Z","shell.execute_reply":"2025-01-01T13:21:38.773269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# You can customize your local language configuration by following these instructions.\n\n### Tricks\n\nIf you want a static prompt loop, configure all prompts with equal length. For example:\n\n```python\ntitle = [\"a\", \"b\"]\ndescription = [\"1\", \"2\"]\n```\n\nThe template will be created as follows:\n- **Document 1:** title = `a`, description = `1`\n- **Document 2:** title = `b`, description = `2`\n- **Document 3:** title = `a`, description = `1`\n\nTo use a fully random set, configure different sizes of title and description. For example:\n\n```python\ntitle = [\"a\", \"b\"]\ndescription = [\"1\", \"2\", \"3\"]\n```\n\nThe template will be created as follows:\n- **Document 1:** title = `a`, description = `1`\n- **Document 2:** title = `b`, description = `2`\n- **Document 3:** title = `a`, description = `3`\n- **Document 4:** title = `b`, description = `1`\n\n## Sample Customizing Local Language Configuration","metadata":{}},{"cell_type":"markdown","source":"```python\nfrom gemma_template import *\n\nGEMMA_TEMPLATE = \"\"\"<start_of_turn>user\n\n{user_template}<end_of_turn>\n<start_of_turn>model\n\n{model_template}<end_of_turn>\n\n\"\"\"\n\nGEMMA_PROMPT_TEMPLATE = \"\"\"<start_of_turn>user\n\n{user_template}<end_of_turn>\n<start_of_turn>model\n\n\"\"\"\n\nVIETNAMESE_USER_TEMPLATE = \"\"\"{system_template}\n\n{prompt_template}\n\n{instruction_template}\n{structure_template}\n# Văn Bản:\n{document}\n\"\"\"\n\nVIETNAMESE_INSTRUCTION_TEMPLATE = \"\"\"# Vai trò:\nBạn là một biên tập viên nội dung chuyên nghiệp, nhà phân tích ngôn ngữ và chuyên gia đa ngôn ngữ, chuyên về viết có cấu trúc và xử lý văn bản nâng cao.\n\n# Nhiệm Vụ:\nMục tiêu chính của bạn là:\n1. Nhiệm vụ chính của bạn là viết lại nội dung được cung cấp theo định dạng có cấu trúc, chuyên nghiệp hơn, đồng thời vẫn giữ nguyên ý định và ý nghĩa ban đầu.\n2. Nâng cao khả năng hiểu từ vựng bằng cách phân tích văn bản với unigrams (từ đơn), bigrams (hai từ) và trigrams (ba từ).\n3. Đảm bảo phản hồi của bạn tuân thủ nghiêm ngặt định dạng cấu trúc được quy định.\n4. Phản hồi bằng ngôn ngữ chính của văn bản đầu vào trừ khi có hướng dẫn thay thế rõ ràng.\n\n# Kỳ Vọng Bổ Sung:\n1. Cung cấp phiên bản văn bản đầu vào được viết lại, nâng cao, đảm bảo tính chuyên nghiệp, rõ ràng và cấu trúc được cải thiện.\n2. Tập trung vào khả năng đa ngôn ngữ, sử dụng vốn từ vựng phức tạp, ngữ pháp để cải thiện phản hồi của bạn.\n3. Giữ nguyên ngữ cảnh và sắc thái văn hóa của văn bản gốc khi viết lại.\n\nDanh mục: {topic_values}\nTừ khoá: {keyword_values}\n\n# Phân Tích Văn Bản:\nVí Dụ 1: Unigrams (nhóm 1 chữ cái)\n{unigrams}\n\nPhân Tích Văn Bản 1: đây là những từ thông dụng trong tiếng Việt ({language}), cho biết văn bản được viết bằng Tiếng Việt ({language}).\n\nVí Dụ 2: Bigrams (nhóm 2 chữ cái)\n{bigrams}\n\nPhân Tích Văn Bản 2: các từ ghép thường gặp trong tiếng Việt ({language}) xác nhận bối cảnh ngôn ngữ.\n\nVí Dụ 3: Trigrams (nhóm 3 chữ cái)\n{trigrams}\n\nPhân Tích Văn Bản 3: các từ ghép 3 chữ liên tiếp là những từ tiếng Việt sử dụng thường xuyên, xác nhận sự cần thiết phải phản hồi bằng Tiếng Việt ({language}).\n\n# Kết Luận Phân Tích Văn Bản:\nPhân tích ngôn ngữ xác nhận văn bản chủ yếu bằng Tiếng Việt ({language}). Do đó, phản hồi phải được cấu trúc và viết bằng Tiếng Việt ({language}). để phù hợp với văn bản và ngữ cảnh gốc.\n\"\"\"  # noqa: E501\n\nVIETNAMESE_STRUCTURE_TEMPLATE = \"\"\"# Định Dạng Cấu Trúc Phản Hồi:\nBạn phải tuân theo cấu trúc phản hồi:\n{structure_template}\n\nBằng cách tuân thủ định dạng này, phản hồi sẽ duy trì tính toàn vẹn về mặt ngôn ngữ đồng thời tăng cường tính chuyên nghiệp, cấu trúc và sự phù hợp với mong đợi của người dùng.\n\"\"\"  # noqa: E501\n\n\ngemma_template = Template()\nvietnamese_template = Template(\n    end_sep=\"và\",\n    system_prompts=[\n        (\n            \"Bạn là một nhà sáng tạo nội dung, viết nội dung chuyên nghiệp biết nhiều\"\n            \" ngôn ngữ.\"\n        ),\n    ],\n    user_prompts=[\n        (\n            \"Viết lại văn bản để thân thiện hơn với công cụ tìm kiếm. Kết hợp các từ khóa\"\n            \" có liên quan một cách tự nhiên, cải thiện khả năng đọc và đảm bảo phù hợp\"\n            \" với các phương pháp hay nhất của SEO.\"\n        ),\n        (\n            \"Viết lại văn bản với giọng văn hấp dẫn và sáng tạo hơn. Sử dụng hình ảnh\"\n            \" sống động, ngôn ngữ mô tả và phong cách đàm thoại để thu hút người đọc.\"\n        ),\n        (\n            \"Viết lại văn bản để làm cho nó súc tích hơn mà không làm mất đi ý nghĩa hoặc\"\n            \" tác động của nó. Loại bỏ các từ và cụm từ không cần thiết trong khi vẫn giữ\"\n            \" nguyên thông điệp cốt lõi.\"\n        ),\n    ],\n    structure_field=StructureField(\n        title=[\"Tiêu đề\"],\n        description=[\"Mô tả\"],\n        document=[\"Bài viết chỉnh sửa\"],\n        main_points=[\"Điểm nổi bật\", \"Điểm chính\"],\n        categories=[\"Danh mục\", \"Chủ đề\"],\n        tags=[\"Từ khoá\"],\n    ),\n    title=[\n        (\n            \"Viết lại tiêu đề để ngắn gọn, hấp dẫn và được tối ưu hóa cho SEO bằng các từ\"\n            \" khóa có liên quan.\"\n        ),\n        \"Tạo tiêu đề ngắn gọn, thu hút sự chú ý và được tối ưu hóa cho SEO.\",\n        \"Viết lại tiêu đề, kết hợp các từ khóa hoặc cụm từ thịnh hành vào tiêu đề.\",\n    ],\n    description=[\n        \"Tóm tắt bài viết trong một câu, làm nổi bật so và thu hút sự tò mò.\",\n        (\n            \"Tạo mô tả bằng một sự thật hoặc số liệu thống kê đáng ngạc nhiên để thu hút\"\n            \" sự chú ý để thu hút người đọc.\"\n        ),\n        (\n            \"Tóm tắt bài viết trong một hoặc hai câu tập trung vào ý chính, kết hợp các\"\n            \" từ khóa chính một cách tự nhiên.\"\n        ),\n    ],\n    document=[\n        (\n            \"Viết lại bài viết với giọng điệu chuyên nghiệp hơn và cấu trúc hợp lý, dễ\"\n            \" đọc hơn.\"\n        ),\n        \"Viết lại bài viết để làm cho chúng hấp dẫn hơn và có phong cách chuyên nghiệp.\",\n        \"Đơn giản hóa thuật ngữ kỹ thuật để làm cho bài viết dễ hiểu với tất cả độc giả.\",\n    ],\n    main_points=[\n        (\n            \"Tạo điểm chính dưới dạng danh sách, thêm ví dụ hoặc giải thích ngắn gọn cho\"\n            \" từng điểm chính.\"\n        ),\n        \"Tóm tắt các ý chính thành các điểm chính ngắn gọn, thu hút người đọc.\",\n        (\n            \"Đảm bảo tất cả các điểm chính đều có sự liên kết hợp lý từ điểm này sang\"\n            \" điểm khác.\"\n        ),\n    ],\n    categories=[\n        \"Viết lại các danh mục để phù hợp với chủ đề phổ biến theo bài viết.\",\n        \"Tạo danh sách danh mục để phù hợp với các từ khóa được sử dụng trong bài viết.\",\n        \"Chọn các danh mục cải thiện SEO và khả năng khám phá theo nội dung bài viết.\",\n    ],\n    tags=[\n        \"Tạo danh sách từ khóa thịnh hành giúp SEO tốt hơn.\",\n        \"Tạo danh sách từ khóa có liên quan phù hợp với truy vấn tìm kiếm phổ biến.\",\n        \"Tập trung vào các từ khóa phổ biến trong bài viết để SEO tốt hơn.\",\n    ],\n)\n\nresponse = vietnamese_template.template(\n    template=GEMMA_TEMPLATE,\n    user_template=VIETNAMESE_USER_TEMPLATE,\n    instruction_template=VIETNAMESE_INSTRUCTION_TEMPLATE,\n    structure_template=VIETNAMESE_STRUCTURE_TEMPLATE,\n    title=\"Gemma open models\",\n    description=\"Gemma: Introducing new state-of-the-art open models.\",\n    document=\"Gemma open models are built from the same research and technology as Gemini models. Gemma 2 comes in 2B, 9B and 27B and Gemma 1 comes in 2B and 7B sizes.\",\n    main_points=[\"Main point 1\", \"Main point 2\"],\n    categories=[\"Artificial Intelligence\", \"Gemma\"],\n    tags=[\"AI\", \"LLM\", \"Google\"],\n    output=\"A new family of open language models demonstrating strong performance across academic benchmarks for language understanding, reasoning, and safety.\",\n )  # remove kwargs if not used.\nprint(response)\n```","metadata":{}},{"cell_type":"markdown","source":"# Automatic Model Save Every Hour\n\nThis repository includes an implementation for automatically saving model checkpoints during training on Kaggle. Due to Kaggle's training time limit, the `TrainerCallback` class has been extended to handle periodic backups. The maximum running time is set to 11 hours 30 minutes, and you can configure it using the `stopped_at` parameter (configuration on top of this notebook).\n\n## Automatic Backup Implementation\n\nThe following code demonstrates how the `HubCallback` class is implemented to save checkpoints every hour:","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom pathlib import Path\nfrom transformers import TrainerCallback, Trainer\nfrom transformers.trainer_callback import TrainerControl, TrainerState\nfrom transformers.training_args import TrainingArguments\n\nTRAINING_ARGS_NAME = \"training_args.bin\"\nTRAINER_STATE_NAME = \"trainer_state.json\"\nOPTIMIZER_NAME = \"optimizer.pt\"\nOPTIMIZER_NAME_BIN = \"optimizer.bin\"\nSCHEDULER_NAME = \"scheduler.pt\"\nSCALER_NAME = \"scaler.pt\"\nFSDP_MODEL_NAME = \"pytorch_model_fsdp\"\n\nREADME = \"\"\"\n# Gemma Template\n\nThis library was developed for the Kaggle challenge:\n[**Google - Unlocking Global Communication with Gemma**](https://www.kaggle.com/competitions/gemma-language-tuning), sponsored by Google.\n\n## Credit Requirement\n\n**Important:** If you are a participant in the competition and wish to use this source code in your submission,\nyou must clearly credit the original author before the competition's end date, **January 14, 2025**.\n\nPlease include the following information in your submission:\n\n```text\nAuthor: Tu Pham\nKaggle Username: [bigfishdev](https://www.kaggle.com/bigfishdev)\nGitHub: [https://github.com/thewebscraping/gemma-template/](https://github.com/thewebscraping/gemma-template)\nLinkedIn: [https://www.linkedin.com/in/thetwofarm](https://www.linkedin.com/in/thetwofarm)\n```\n\n# Overview\n\nGemma Template is a lightweight and efficient Python library for generating templates to fine-tune models and craft prompts.\nDesigned for flexibility, it seamlessly supports Gemma, LLaMA, and other language frameworks, offering fast, user-friendly customization.\nWith multilingual capabilities and advanced configuration options, it ensures precise, professional, and dynamic template creation.\n\n### Learning Process and Acknowledgements\nAs a newbie, I created Gemma Template based on what I read and learned from the following sources:\n\n- Google Gemma Cookbook: [Advanced Prompting Techniques](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Advanced_Prompting_Techniques.ipynb)\n- Google Gemma Cookbook: [Finetune_with_LLaMA_Factory](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetune_with_LLaMA_Factory.ipynb)\n- Google Gemma Cookbook: [Finetuning Gemma for Function Calling](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetuning_Gemma_for_Function_Calling.ipynb)\n- Alpaca: [Alpaca Lora Documentation](https://github.com/tloen/alpaca-lora)\n- Unsloth: [Finetune Llama 3.2, Mistral, Phi-3.5, Qwen 2.5 & Gemma 2-5x faster with 80% less memory!](https://github.com/unslothai/unsloth)\n\n\nGemma Template supports exporting dataset files in three formats: `Text`, `Alpaca`, and `GPT conversions`.\n\n# Multilingual Content Writing Assistant\n\nThis writing assistant is a multilingual professional writer specializing in crafting structured, engaging, and SEO-optimized content.\nIt enhances text readability, aligns with linguistic nuances, and preserves original context across various languages.\n\n---\n\n## Key Features:\n#### 1. **Creative and Engaging Rewrites**\n- Transforms input text into captivating and reader-friendly content.\n- Utilizes vivid imagery and descriptive language to enhance engagement.\n\n#### 2. **Advanced Text Analysis**\n- Processes text with unigrams, bigrams, and trigrams to understand linguistic patterns.\n- Ensures language-specific nuances and cultural integrity are preserved.\n\n#### 3. **SEO-Optimized Responses**\n- Incorporates keywords naturally to improve search engine visibility.\n- Aligns rewritten content with SEO best practices for discoverability.\n\n#### 4. **Professional and Multilingual Expertise**\n- Full support for creating templates in local languages.\n- Supports multiple languages with advanced prompting techniques.\n- Vocabulary and grammar enhancement with unigrams, bigrams, and trigrams instruction template.\n- Supports hidden mask input text. Adapts tone and style to maintain professionalism and clarity.\n- Full documentation with easy configuration prompts and examples.\n\n#### 5. **Customize Advanced Response Structure and Dataset Format**\n- Supports advanced response structure format customization.\n- Compatible with other models such as LLaMa.\n- Enhances dynamic prompts using Round-Robin loops.\n- Outputs multiple formats such as Text, Alpaca and GPT conversions.\n\n**Installation**\n----------------\n\nTo install the library, you can choose between two methods:\n\n#### **1\\. Install via PyPI:**\n\n```shell\npip install gemma-template\n```\n\n#### **2\\. Install via GitHub Repository:**\n\n```shell\npip install git+https://github.com/thewebscraping/gemma-template.git\n```\n\n# Training Arguments\nProject Id: {project_id}\nTraining Steps: {step}\n\n### Hyperparameters:\n{hyperparameters}\n\"\"\"\n\nclass HubCallback(TrainerCallback):\n    def __init__(self, trainer: Trainer, project_id, stopped_at, save_every_n_minutes=60):\n        super().__init__()\n        self.trainer = trainer\n        self.project_id = project_id\n        self.stopped_at = stopped_at\n        self.save_every_n_minutes = save_every_n_minutes\n        self.save_after = datetime.now() + timedelta(minutes=save_every_n_minutes)\n\n    def on_step_begin(self, train_args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n        if datetime.now().timestamp() > self.stopped_at.timestamp():\n            self.save_checkpoint(train_args)\n            control.should_training_stop = True\n            control.should_save = True\n        else:\n            if datetime.now().timestamp() > self.save_after.timestamp():\n                self.save_after = datetime.now() + timedelta(minutes=self.save_every_n_minutes)\n                self.save_checkpoint(train_args)\n\n    def save_checkpoint(self, train_args: TrainingArguments, output_dir: str = \".checkpoint/\"):\n        try:\n            print(\"*\" * 30, \" SAVE CHECKPOINT \", \"*\" * 30)\n            Path(output_dir).mkdir(parents=True, exist_ok=True)\n            self.trainer.save_model(output_dir, _internal_call=True)\n            self.trainer.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n            torch.save(self.trainer.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n            torch.save(self.trainer.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))\n            with open(os.path.join(output_dir, \"README.md\"), \"w\", encoding=\"utf-8\") as fp:\n                readme_doc = README.format(\n                    project_id=self.project_id,\n                    step=self.trainer.state.global_step,\n                    hyperparameters=json.dumps(train_args.to_dict(), ensure_ascii=False, indent=4)\n                )\n                fp.write(readme_doc)\n\n            print(\"*\" * 30, \" PROCESSED PUSH TO HUB \", \"*\" * 30)\n            self.push_to_hub(train_args, output_dir)\n            print(\"*\" * 30, \" COMPLETED PUSH TO HUB \", \"*\" * 30)\n\n        except Exception as e:\n            print(\"FAILED TO SAVE CHECKPOINT:\", e)\n\n    def push_to_hub(self, train_args: TrainingArguments, output_dir: str = \".checkpoint/\"):\n        self.trainer.tokenizer.save_pretrained(output_dir)\n        self.trainer.model.save_pretrained(output_dir)\n        self.trainer.tokenizer.push_to_hub(\n            self.project_id,\n            private=train_args.hub_private_repo,\n            token=train_args.hub_token\n        )\n        self.trainer.model.push_to_hub(\n            self.project_id,\n            commit_message=\"Training steps: {}\".format(self.trainer.state.global_step),\n            private=train_args.hub_private_repo,\n            token=train_args.hub_token\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.775435Z","iopub.execute_input":"2025-01-01T13:21:38.775700Z","iopub.status.idle":"2025-01-01T13:21:38.793897Z","shell.execute_reply.started":"2025-01-01T13:21:38.775677Z","shell.execute_reply":"2025-01-01T13:21:38.792758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure WanDB","metadata":{}},{"cell_type":"code","source":"import os\nimport wandb\n\nrun = wandb.init(\n    project=project_id,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.795057Z","iopub.execute_input":"2025-01-01T13:21:38.795440Z","iopub.status.idle":"2025-01-01T13:21:38.813466Z","shell.execute_reply.started":"2025-01-01T13:21:38.795400Z","shell.execute_reply":"2025-01-01T13:21:38.812415Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure Hyperparameters\n\nBelow is an configuration for `TrainingArguments`:","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrain_args = TrainingArguments(\n    # ---Output settings--\n    # Output directory where model predictions and checkpoints will be stored\n    output_dir = f\".results/{project_id}\",\n    logging_dir = f\".results/{project_id}/logs\",\n    overwrite_output_dir = True,\n    # No eval running\n    do_eval = False,\n    # Save strategy\n    save_strategy = \"steps\",\n    # Save steps\n    save_steps = 300,\n    # Save total limit\n    save_total_limit = 1,\n    # Batch size per GPU core for training\n    per_device_train_batch_size = 2,\n    # Number of update steps to accumulate the gradients for\n    gradient_accumulation_steps = 4,\n    # Train epochs\n    num_train_epochs = 1,\n    # Learning rate\n    learning_rate = 2e-4,\n    # Enable float16 precision\n    fp16 = not torch.cuda.is_bf16_supported(),\n    # Enable bfloat16 precision. False then fp16 is True\n    bf16 = torch.cuda.is_bf16_supported(),\n    # Logging: Log every X update step\n    logging_steps = 3,\n    # Optimizer to use\n    optim = \"adamw_8bit\",\n    # Weight decay\n    weight_decay = 0.1,\n    lr_scheduler_type = \"linear\",\n    # Ratio of steps for a linear warmup (from 0 to learning rate)\n    warmup_ratio = 0.05,\n    hub_private_repo = True,\n    remove_unused_columns = True,\n    seed = seed,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.814557Z","iopub.execute_input":"2025-01-01T13:21:38.814952Z","iopub.status.idle":"2025-01-01T13:21:51.394342Z","shell.execute_reply.started":"2025-01-01T13:21:38.814885Z","shell.execute_reply":"2025-01-01T13:21:51.392832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure SFT Trainer and Trainer","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,\n    args=train_args,\n)\n\n# Add HubCallback to save model every hour and stop after 11 hours 30 minutes\ntrainer.add_callback(HubCallback(trainer, project_id, stopped_at))\n\n# Start training\ntrainer = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.395036Z","iopub.status.idle":"2025-01-01T13:21:51.395387Z","shell.execute_reply":"2025-01-01T13:21:51.395243Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save Model to Kaggle Host","metadata":{}},{"cell_type":"code","source":"try:\n    model.save_pretrained(project_id)\n    tokenizer.save_pretrained(project_id)\nexcept:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.396361Z","iopub.status.idle":"2025-01-01T13:21:51.396757Z","shell.execute_reply":"2025-01-01T13:21:51.396562Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Push to HuggingFace","metadata":{}},{"cell_type":"code","source":"try:\n    model.push_to_hub(project_id)\n    tokenizer.push_to_hub(project_id)\nexcept:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.397607Z","iopub.status.idle":"2025-01-01T13:21:51.397960Z","shell.execute_reply":"2025-01-01T13:21:51.397792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"elapsed = stopped_at - datetime.now()\nprint(\"Elapsed:\", str(elapsed))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.398625Z","iopub.status.idle":"2025-01-01T13:21:51.399038Z","shell.execute_reply":"2025-01-01T13:21:51.398867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model Evaluation Metric\n=======================\n\nThis repository provides tools for evaluating models using **GoogleBLEU** and **ROUGE**, following the guidelines outlined in the [HuggingFace documentation on choosing a metric](https://huggingface.co/docs/evaluate/choosing_a_metric).\n\nOverview\n--------\n\nThe evaluation metrics implemented in this project include:\n\n1.  **GoogleBLEU**: A variant of BLEU designed for evaluating machine translation quality.\n    \n2.  **ROUGE**: Widely used for evaluating text summarization and machine translation models.\n    \n\nThese metrics ensure a comprehensive assessment of model performance across various natural language processing tasks.\n\nInstructions\n------------\n\n1.  Follow the instructions on the [HuggingFace Evaluation Metric Guide](https://huggingface.co/docs/evaluate/choosing_a_metric) to integrate these metrics into your workflow.\n    \n2.  Due to the runtime limitations of Kaggle trainers, the complete dataset results are hosted externally. You can view the full benchmark results here: [Benchmark Results](https://github.com/thewebscraping/gemma-template/blob/main/docs/benchmark.md).\n    \n\nResults\n-------\n\nA summary of evaluation results can be found in the [benchmark documentation](https://github.com/thewebscraping/gemma-template/blob/main/docs/benchmark.md).\n\nReferences\n----------\n\n*   HuggingFace Documentation: [Choosing a Metric](https://huggingface.co/docs/evaluate/choosing_a_metric)\n    \n*   Benchmark Data: [Benchmark Results](https://github.com/thewebscraping/gemma-template/blob/main/docs/benchmark.md)\n    \n\n### Example Code\n-------------\n","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nFastLanguageModel.for_inference(model)\n\nstopped_at = stopped_at + timedelta(minutes=15)\n\ndef is_expired(expired_at):\n    if datetime.now().timestamp() > expired_at.timestamp():\n        return True\n    return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prepare Test Dataset\n\n1. **Combined Prompt**:\n   - **10%**: Prompt include the `title` and `document`.\n   - **10%**: Prompt include the `title`, `document`, and `description`.\n   - **10%**: Prompt include the `title`, `document`, and `main points`.\n   - **10%**: Prompt include the `title`, `document`, `categories`, and `tags`.\n   - **60%**: Prompt fully structure format.\n   - **Remove instruction template for evaluate.**\n   - **Overwrite default prompt structure template.**","metadata":{}},{"cell_type":"code","source":"try:\n    test_dataset = load_from_json_file(\"/kaggle/input/gemma-template/test-gemma-template.json\")\nexcept FileNotFoundError:\n    test_dataset = load_from_huggingface_hub(\"twodev/gemma-template\", split='test')\n\ntotal_rows = len(test_dataset)\nfive_percent_idx = int(total_rows * 1)\n\ndataset_mapping = {\n    \"Combined response including title and document\": {\n        \"data\": test_dataset.select(range(0, five_percent_idx)),\n        \"excluded_fields\": [\"description\", \"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and description\": {\n        \"data\": test_dataset.select(range(five_percent_idx, five_percent_idx * 2)),\n        \"excluded_fields\": [\"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and main points\": {\n        \"data\": test_dataset.select(range(five_percent_idx * 2, five_percent_idx * 3)),\n        \"excluded_fields\": [\"description\", \"categories\", \"tags\"]\n        \n    },\n    \"Combined response including title, document and categories and tags\": {\n        \"data\": test_dataset.select(range(five_percent_idx * 3, five_percent_idx * 4)),\n        \"excluded_fields\": [\"description\", \"main_points\"]\n        \n    },\n    \"Prompt Structure Format\": {\n        \"data\": test_dataset.select(range(five_percent_idx * 4, len(test_dataset))),\n        \"excluded_fields\": []\n        \n    },\n}\n\nprint(dataset_mapping)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Configure Template\n- Remove instruction template for evaluate.\n- Overwrite default prompt structure template.","metadata":{}},{"cell_type":"code","source":"GEMMA_PROMPT_TEMPLATE = \"\"\"<start_of_turn>user\n{input}<end_of_turn>\n<start_of_turn>model\n\n\"\"\"\n\nPROMPT_TEMPLATE = \"\"\"{% if prompt %}\\n\\n{{ prompt }}\\n\\n{% endif %}{% if structure_fields %}You must follow the response structure:\n\n{% for field in structure_fields %}{{ field.label }}\\n{% endfor %}\n{% endif %}\"\"\"\n\ngemma_template.instruction_template = []  \ngemma_template.prompt_template = [PROMPT_TEMPLATE]\nvietnamese_gemma_template.instruction_template = []\nvietnamese_gemma_template.prompt_template = [PROMPT_TEMPLATE]\n\neval_dataset = []\ninput_datasets = []\nfor task, item in dataset_mapping.items():\n    if is_expired(stopped_at):\n        break\n        \n    print(\"Prepare dataset for task:\", task)\n    split_dataset = item['data'].train_test_split(test_size=language_ratio_size)\n\n    # prepare dataset use instruction and structure English language.\n    english_dataset = gemma_template.load_dataset(\n        split_dataset[\"train\"], \n        excluded_fields=item['excluded_fields'],\n        output_format='alpaca',\n    )\n    english_dataset = english_dataset.map(lambda x: {\"task\": [\"english_dataset\" for _ in x[\"input\"]], }, batched=True)\n    input_datasets.append(english_dataset)\n                                                       \n    # prepare dataset use instruction and structure Vietnamese language.\n    vietnamese_dataset = vietnamese_gemma_template.load_dataset(\n        split_dataset[\"test\"], \n        excluded_fields=item['excluded_fields'],\n        output_format='alpaca',\n    )\n    vietnamese_dataset = vietnamese_dataset.map(lambda x: {\"task\": [\"vietnamese\" for _ in x[\"input\"]], }, batched=True)\n    input_datasets.append(vietnamese_dataset)\n\nfrom datasets import concatenate_datasets\n\nif input_datasets:\n    eval_dataset = concatenate_datasets(input_datasets).shuffle(seed=42)\n    eval_dataset = eval_dataset.map(lambda x: {\"prompt\": [GEMMA_PROMPT_TEMPLATE.format(input=input_str) for input_str in x[\"input\"]], }, batched=True)\n    print(eval_dataset)\n    print(eval_dataset[0]['prompt'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run Evaluate","metadata":{}},{"cell_type":"code","source":"import json\nimport evaluate\n\ndef clean_response(response: str):\n    response = response.split(\"<start_of_turn>model\")[-1].split(\"<end_of_turn>\")\n    return response[0].strip()\n\n\ngoogle_bleu = evaluate.load(\"google_bleu\")\nrouge = evaluate.load('rouge')\neval_responses = []\n\nfor idx, item in enumerate(eval_dataset):\n    if is_expired(stopped_at):\n        break\n\n    task = str(item['task']).upper()\n    input_str = item['prompt']\n    output_str = item['output'].strip()\n    predictions, references = [output_str], []\n    input_ids = tokenizer(input_str, return_tensors=\"pt\").to(device)\n    outputs = model.generate(**input_ids, max_new_tokens=1024)\n\n    model_references = []\n    rouge_score, google_bleu_score = {}, {}\n\n    try:\n        for output in outputs:\n            model_response = tokenizer.decode(output)\n            model_references.append(model_response)\n            references.append(clean_response(model_response))\n        \n        if not (predictions and references):\n            continue\n\n        try:\n            rouge_score = rouge.compute(predictions=predictions, references=references)\n            rouge_score = {k: float(v) for k, v in rouge_score.items()}\n            print(\"Rouge Score:\", rouge_score)\n        except:\n            pass\n\n        try:\n            google_bleu_score = google_bleu.compute(predictions=predictions, references=references)\n            print(\"Google BLEU Score:\", google_bleu_score)\n        except:\n            pass\n            \n        print(\"*\" * 30, f\" MODEL OUTPUT - {task} PROMPT \", \"*\" * 30)\n        print(references[0])\n        print(\"=\" * 90)\n    except:\n        pass\n    \n    try:\n        item.update({\"rouge\": rouge_score, \"google_bleu\": google_bleu_score, \"model_references\": model_references})\n        eval_responses.append(json.loads(json.dumps(item, default=str)))\n    except:\n        pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save Evaluate Data ","metadata":{}},{"cell_type":"code","source":"def is_valid_score(r: dict, field: str):\n    if isinstance(r, dict):\n        if r.get(field):\n            return True\n        \ndef write_json(obj, path: str = \"dump.json\", *, ensure_ascii=False, indent=4):\n    with open(path, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump(obj, json_file, ensure_ascii=False, indent=4, default = str)\n\n\ntry:\n    write_json(eval_responses, \"eval_score_data.json\")\nexcept:\n    pass\n\n\ntotal_rows = len(eval_responses)\ntry:\n    rouge_mapping = {\n        \"rouge1\": sum([r['rouge']['rouge1'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n        \"rouge2\": sum([r['rouge']['rouge2'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n        \"rougeL\": sum([r['rouge']['rougeL'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n        \"rougeLSum\": sum([r['rouge']['rougeLsum'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n    }\n    print(\"ROUGE SCORE:\", str(rouge_mapping))\nexcept:\n    pass\n\ntry:\n    google_bleu_mapping = {\"google_bleu\": sum([r['google_bleu']['google_bleu'] for r in eval_responses if is_valid_score(r, \"google_bleu\")]) / total_rows}\n    print(\"GOOGLE BLEU:\", str(google_bleu_mapping))\nexcept:\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Took: %.2f seconds.\", (time.perf_counter() - start_at))\nprint(\"Task End:\", str(datetime.now() - stopped_at))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
