{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":10336622,"sourceType":"datasetVersion","datasetId":6400592},{"sourceId":10385697,"sourceType":"datasetVersion","datasetId":6433916},{"sourceId":104623,"sourceType":"modelInstanceVersion","modelInstanceId":72254,"modelId":76277},{"sourceId":104625,"sourceType":"modelInstanceVersion","modelInstanceId":72253,"modelId":76277}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma Template\n\nThis library was developed for the Kaggle challenge:\n[**Google - Unlocking Global Communication with Gemma**](https://www.kaggle.com/competitions/gemma-language-tuning), sponsored by Google.\n\n## Credit Requirement\n\n**Important:** If you are a participant in the competition and wish to use this source code in your submission,\nyou must clearly credit the original author before the competition's end date, **January 14, 2025**.\n\nPlease include the following information in your submission:\n\n```text\nAuthor: Tu Pham\nKaggle Username: [bigfishdev](https://www.kaggle.com/bigfishdev)\nGitHub: [https://github.com/thewebscraping/gemma-template/](https://github.com/thewebscraping/gemma-template)\nLinkedIn: [https://www.linkedin.com/in/thetwofarm](https://www.linkedin.com/in/thetwofarm)\n```\n\n# Overview\n\nGemma Template is a lightweight and efficient Python library for generating templates to fine-tune models and craft prompts.\nDesigned for flexibility, it seamlessly supports Gemma, LLaMA, and other language frameworks, offering fast, user-friendly customization.\nWith multilingual capabilities and advanced configuration options, it ensures precise, professional, and dynamic template creation.\n\n### Learning Process and Acknowledgements\nAs a newbie, I created Gemma Template based on what I read and learned from the following sources:\n\n- Google Gemma Cookbook: [Advanced Prompting Techniques](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Advanced_Prompting_Techniques.ipynb)\n- Google Gemma Cookbook: [Finetune with LLaMA Factory](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetune_with_LLaMA_Factory.ipynb)\n- Google Gemma Cookbook: [Fine tuning Gemma for Function Calling](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetuning_Gemma_for_Function_Calling.ipynb)\n- Alpaca: [Alpaca Lora Documentation](https://github.com/tloen/alpaca-lora)\n- Unsloth: [Finetune Llama 3.2, Mistral, Phi-3.5, Qwen 2.5 & Gemma 2-5x faster with 80% less memory!](https://github.com/unslothai/unsloth)\n\n\nGemma Template supports exporting dataset files in three formats: `Text`, `Alpaca`, and `OpenAI`.\n\n# Multilingual Content Writing Assistant\n\nThis writing assistant is a multilingual professional writer specializing in crafting structured, engaging, and SEO-optimized content.\nIt enhances text readability, aligns with linguistic nuances, and preserves original context across various languages.\n\n---\n\n## Key Features:\n#### 1. **Creative and Engaging Rewrites**\n- Transforms input text into captivating and reader-friendly content.\n- Utilizes vivid imagery and descriptive language to enhance engagement.\n\n#### 2. **Advanced Text Analysis**\n- Processes text with unigrams, bigrams, and trigrams to understand linguistic patterns.\n- Ensures language-specific nuances and cultural integrity are preserved.\n\n#### 3. **SEO-Optimized Responses**\n- Incorporates keywords naturally to improve search engine visibility.\n- Aligns rewritten content with SEO best practices for discoverability.\n\n#### 4. **Professional and Multilingual Expertise**\n- Full support for creating templates in local languages.\n- Supports multiple languages with advanced prompting techniques.\n- Vocabulary and grammar enhancement with unigrams, bigrams, and trigrams instruction template.\n- Supports hidden mask input text. Adapts tone and style to maintain professionalism and clarity.\n- Full documentation with easy configuration prompts and examples.\n\n#### 5. **Customize Advanced Response Structure and Dataset Format**\n- Supports advanced response structure format customization.\n- Compatible with other models such as LLaMa.\n- Enhances dynamic prompts using Round-Robin loops.\n- Outputs multiple formats such as Text, Alpaca and OpenAI.\n\n**Installation**\n----------------\n\nTo install the library, you can choose between two methods:\n\n#### **1\\. Install via PyPI:**\n\n```shell\npip install gemma-template\n```\n\n#### **2\\. Install via GitHub Repository:**\n\n```shell\npip install git+https://github.com/thewebscraping/gemma-template.git\n```\n\n**Quick Start**\n----------------\nStart using Gemma Template with just a few lines of code:\n\n## Load Dataset\nReturns: A Hugging Face Dataset or DatasetDict object containing the processed prompts.\n\n**Load Dataset from data dict**\n```python\nfrom gemma_template import gemma_template\n\ndata_dict = [\n    {\n        \"id\": \"JnZJolR76_u2\",\n        \"title\": \"Sample title\",\n        \"description\": \"Sample description\",\n        \"document\": \"Sample document\",\n        \"categories\": [\"Topic 1\", \"Topic 2\"],\n        \"tags\": [\"Tag 1\", \"Tag 2\"],\n        \"output\": \"Sample output\",\n        \"main_points\": [\"Main point 1\", \"Main point 2\"],\n    }\n]\ndataset = gemma_template.load_dataset(data_dict, output_format='text')   # enum: `text`, `alpaca` and `openai`.\nprint(dataset['text'][0])\n```\n\n**Load Dataset from HuggingFace**\n```python\nfrom gemma_template import gemma_template\n\ndataset = gemma_template.load_dataset(\n    \"YOUR_JSON_FILE_PATH_OR_HUGGINGFACE_DATASET\",\n    # enum: `text`, `alpaca` and `openai`.\n    output_format='text',\n    # Percentage of documents that need to be word masked.\n    # Min: 0, Max: 1. Default: 0.\n    max_hidden_ratio=.1,\n    # Replace 10% of words in the input document with '_____'.\n    # Use int to extract the correct number of words. The `max_hidden_ratio` parameter must be greater than 0.\n    max_hidden_words=.1,\n    # Minimum character of a word, used to create unigrams, bigrams, and trigrams. Default is 2.\n    min_chars_length=2,\n    # Maximum character of a word, used to create unigrams, bigrams and trigrams. Default is 0.\n    max_chars_length=8,\n)\n```\n\n## Fully Customized Template\n\n```python\nfrom gemma_template import Template, FieldPosition, INPUT_TEMPLATE, OUTPUT_TEMPLATE, INSTRUCTION_TEMPLATE, PROMPT_TEMPLATE\n\ntemplate_instance = Template(\n    instruction_template=[INSTRUCTION_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    prompt_template=[PROMPT_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    input_template=[INPUT_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    output_template=[OUTPUT_TEMPLATE],  # Optional: dynamic Round-Robin loops\n    position=FieldPosition(\n            title=[\"Custom Title\"],\n            description=[\"Custom Description\"],\n            document=[\"Custom Article\"],\n            main_points=[\"Custom Main Points\"],\n            categories=[\"Custom Categories\"],\n            tags=[\"Custom Tags\"],\n    ),  # Optional: dynamic Round-Robin loops\n)\n\nresponse = template_instance.apply_template(\n    title=\"Gemma open models\",\n    description=\"Gemma: Introducing new state-of-the-art open models.\",\n    main_points=[\"Main point 1\", \"Main point 2\"],\n    categories=[\"Artificial Intelligence\", \"Gemma\"],\n    tags=[\"AI\", \"LLM\", \"Google\"],\n    document=\"Gemma open models are built from the same research and technology as Gemini models. Gemma 2 comes in 2B, 9B and 27B and Gemma 1 comes in 2B and 7B sizes.\",\n    output=\"A new family of open language models demonstrating strong performance across academic benchmarks for language understanding, reasoning, and safety.\",\n    max_hidden_words=.1,  # set 0 if you don't want to hide words.\n    min_chars_length=2,  # Minimum character of a word, used to create unigrams, bigrams, and trigrams. Default is 2.\n    max_chars_length=0,  # Maximum character of a word, used to create unigrams, bigrams and trigrams. Default is 0.\n)  # remove kwargs if not used.\n\nprint(response)\n```\n\n### Output:\n\n```text\n<start_of_turn>user\nYou are a multilingual professional writer.\n\n# Role:\nYou are a highly skilled professional content writer, linguistic analyst, and multilingual expert specializing in structured writing and advanced text processing.\n\n# Task:\nYour primary objectives are:\n1. Simplification: Rewrite the input text or document to ensure it is accessible and easy to understand for a general audience while preserving the original meaning and essential details.\n2. Lexical and Grammatical Analysis: Analyze and refine vocabulary and grammar using unigrams (single words), bigrams (two words), and trigrams (three words) to enhance readability and depth.\n3. Structure and Organization: Ensure your response adheres strictly to the prescribed structure format.\n4. Language Consistency: Respond in the same language as the input text unless explicitly directed otherwise.\n\n# Additional Guidelines:\n1. Provide a rewritten, enhanced version of the input text, ensuring professionalism, clarity, and improved structure.\n2. Focus on multilingual proficiency, using complex vocabulary, grammar to improve your responses.\n3. Preserve the context and cultural nuances of the original text when rewriting.\n\n# Text Analysis:\nExample 1: Unigrams (single words)\nand => English\nbuilt => English\nfrom => English\nthe => English\nresearch => English\nText Analysis 3: These are common English words, indicating the text is in English.\n\nExample 2: Bigrams (two words)\ntechnology as => English\nText Analysis 2: Frequent bigrams in English confirm the language context.\n\nExample 3: Trigrams (three words)\ntechnology as Gemini => English\nText Analysis 3: Trigrams further validate the linguistic analysis and the necessity to respond in English.\n\n# Conclusion of Text Analysis:\nThe linguistic analysis confirms the text is predominantly in English. Consequently, the response should be structured and written in English to align with the original text and context.\n\n# Input Text:\nRewrite the input text or document to highlight its unique value proposition while ensuring it ranks well for targeted keywords.\n\n# Response Structure Format:\nYou must follow the response structure:\n\n**Custom Title (Title):** Rewrite the title to maximize clarity, appeal, and relevance to the content.\n**Custom Description (Description):** Create a description focusing on how the article addresses a common problem or challenge readers face.\n**Custom Article (Article):** Rewrite the input text or document with an authoritative tone, incorporating credible sources, data, and references to boost trustworthiness and SEO ranking.\n**Custom Main Points (Main Points):** Ensure all key points flow logically from one to the next.\n**Custom Categories (Categories):** Use categories that align with similar articles on the topic and improve SEO and discoverability.\n**Custom Tags (Tags):** Rewrite tags to make them more specific and targeted.\n\nBy adhering to this format, the response will maintain linguistic integrity while enhancing professionalism, structure and alignment with user expectations.\n\n# Text:\nGemma open models are built _____ the same _____ and technology as Gemini models. Gemma 2 comes in 2B, 9B _____ 27B and Gemma 1 comes in 2B and 7B sizes.<end_of_turn>\n<start_of_turn>model\n## **Custom Title:**\n### Gemma open models\n\n## **Custom Description:**\nGemma: Introducing new state-of-the-art open models.\n\n## **Custom Article:**\nA new family of open language models demonstrating strong performance across academic benchmarks for language understanding, reasoning, and safety.\n\n## **Custom Main Points:**\n* Main point 1\n* Main point 2\n\n## **Custom Categories:**\n* Artificial Intelligence\n* Gemma\n\n## **Custom Tags:**\n* AI\n* LLM\n* Google<end_of_turn>\n\n```\n","metadata":{}},{"cell_type":"code","source":"import time\nfrom datetime import datetime, timedelta\nstopped_at = datetime.now() + timedelta(hours=11, minutes=30)\nstart_at = time.perf_counter()\nprint(\"Start time:\", str(stopped_at))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:01:58.506912Z","iopub.execute_input":"2025-01-08T12:01:58.507170Z","iopub.status.idle":"2025-01-08T12:01:58.514932Z","shell.execute_reply.started":"2025-01-08T12:01:58.507147Z","shell.execute_reply":"2025-01-08T12:01:58.513787Z"}},"outputs":[{"name":"stdout","text":"Start time: 2025-01-08 23:31:58.509002\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Install Gemma Template dependencies to generate template","metadata":{}},{"cell_type":"code","source":"!pip install -q -U gemma-template\n!pip install -q evaluate rouge_score sacrebleu nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:01:58.516014Z","iopub.execute_input":"2025-01-08T12:01:58.516397Z","iopub.status.idle":"2025-01-08T12:02:15.937825Z","shell.execute_reply.started":"2025-01-08T12:01:58.516365Z","shell.execute_reply":"2025-01-08T12:02:15.936463Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Install Unsloth dependencies for Fine Tuning","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:32:29.957696Z","iopub.execute_input":"2025-01-01T12:32:29.958009Z","iopub.status.idle":"2025-01-01T12:33:34.379834Z","shell.execute_reply.started":"2025-01-01T12:32:29.957982Z","shell.execute_reply":"2025-01-01T12:33:34.376157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install -q unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:33:34.3825Z","iopub.execute_input":"2025-01-01T12:33:34.383058Z","iopub.status.idle":"2025-01-01T12:36:35.830496Z","shell.execute_reply.started":"2025-01-01T12:33:34.382997Z","shell.execute_reply":"2025-01-01T12:36:35.829039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport json\nimport random\nfrom pathlib import Path\n\nmodel_name = \"/kaggle/input/gemma-2/transformers/gemma-2-2b-it/2\"\nproject_id = \"gemma-template-gemma-2b-it-v2-competition\"\n\nseed = 3407\n\nif 'google.colab' in sys.modules:\n    # Running on Colab\n    from google.colab import userdata\n    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_TOKEN')\nelif os.path.exists('/kaggle/working'):\n    # Running on Kaggle\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    os.environ['HF_TOKEN'] = user_secrets.get_secret(\"HF_TOKEN\")\n    os.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_TOKEN\")\nelse:\n    # Not running on Colab or Kaggle\n    raise EnvironmentError('This notebook is designed to run on Google Colab or Kaggle.')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:02:15.939187Z","iopub.execute_input":"2025-01-08T12:02:15.939605Z","iopub.status.idle":"2025-01-08T12:02:16.544317Z","shell.execute_reply.started":"2025-01-08T12:02:15.939569Z","shell.execute_reply":"2025-01-08T12:02:16.543493Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Configure Unsloth 4bit-quantized\n\nRead document here: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)","metadata":{}},{"cell_type":"code","source":"try:\n    from unsloth import FastLanguageModel\n    import torch\n    max_seq_length = 3072 # Choose any! We auto support RoPE Scaling internally!\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = model_name,\n        max_seq_length = max_seq_length,\n        dtype = None,\n        load_in_4bit = True,\n    )\n    \n    model = FastLanguageModel.get_peft_model(\n        model,\n        r = 16,\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n        lora_alpha = 16,\n        lora_dropout = 0,\n        bias = \"none\",\n        use_gradient_checkpointing = \"unsloth\",\n        random_state = 3407,\n        use_rslora = False,\n        loftq_config = None,\n    )\n    \n    model.config.use_cache = False\n    model.print_trainable_parameters()\nexcept (Exception, RuntimeError):\n    # Test template, tokenizer required.\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:02:22.085337Z","iopub.execute_input":"2025-01-08T12:02:22.085734Z","iopub.status.idle":"2025-01-08T12:02:28.755127Z","shell.execute_reply.started":"2025-01-08T12:02:22.085697Z","shell.execute_reply":"2025-01-08T12:02:28.753661Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Configure Tokenizer Padding Side\n\nFix overflow issue with bf16/fp16 training. See also: [Google Gemma Cookbook: Finetuning Gemma for Function Calling](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Finetuning_Gemma_for_Function_Calling.ipynb)","metadata":{}},{"cell_type":"code","source":"tokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:02:30.253313Z","iopub.execute_input":"2025-01-08T12:02:30.253818Z","iopub.status.idle":"2025-01-08T12:02:30.258695Z","shell.execute_reply.started":"2025-01-08T12:02:30.253789Z","shell.execute_reply":"2025-01-08T12:02:30.257268Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Goal:\n\nAn article typically consists of the following elements: **title**, **description**, **main points**, **categories**, and **tags**. This fine tuning is designed to address these elements in a single request using a **custom structured format**. Additional objectives include:\n1. Enhancing the model's ability to process and respond effectively in the **local language**.\n2. Improving the structured output format for better usability.\n\n### Gemma 2B - Evaluation Results\n\nThe performance of the **Gemma 2B model** was assessed using **ROUGE** and **Google BLEU** metrics. \n\n| Rouge1  | Rouge2 | RougeL | RougeLSum | Google BLEU |\n| ------------- |:-------------:|:-------------:|:-------------:|:-------------:|\n| 0.722 | 0.524 | 0.456 | 0.703 |  0.345 |\n\n**Key Observations**:\n- The model shows significant improvements in:\n  - Handling user language responses.\n  - Structured content generation.\n- **Challenges**:\n  - Incomplete feedback for certain articles.\n  - Occasional duplication of keywords in responses.\n\nFor more details, you can read version 1 of this Notebook: ([**Version 1**](https://www.kaggle.com/code/bigfishdev/gemma-2b-it-fine-tuning-with-gemma-template?scriptVersionId=216121364)). Also, you can download the file `gemma-benchmark/gemma_2b_eval_benchmark.json` which I have attached on this notebook. Due to Kaggle limitations, I am currently unable to implement **ROUGE** and **Google BLEU** eval evaluations on the **Gemma 2B IT** ([**Version 2**](https://www.kaggle.com/code/bigfishdev/gemma-2b-it-fine-tuning-with-gemma-template?scriptVersionId=216252050)) model that I have refined. I will show a demo how I eval the dataset and the source code at the end of this notebook.\n\n- **Kaggle Gemma 2B Model:**\n  - Model: [https://www.kaggle.com/models/bigfishdev/gemma-template-gemma-2b](https://www.kaggle.com/models/bigfishdev/gemma-template-gemma-2b)\n  - Notebook Version 1: [https://www.kaggle.com/code/bigfishdev/gemma-2b-it-fine-tuning-with-gemma-template?scriptVersionId=216121364](https://www.kaggle.com/code/bigfishdev/gemma-2b-it-fine-tuning-with-gemma-template?scriptVersionId=216121364)\n- **Kaggle Gemma 2B IT Model:**\n  - Model: [https://www.kaggle.com/models/bigfishdev/gemma-template-gemma-2b-it](https://www.kaggle.com/models/bigfishdev/gemma-template-gemma-2b-it)\n  - Notebook Version 2: [https://www.kaggle.com/code/bigfishdev/gemma-2b-it-fine-tuning-with-gemma-template?scriptVersionId=216252050](https://www.kaggle.com/code/bigfishdev/gemma-2b-it-fine-tuning-with-gemma-template?scriptVersionId=216252050)\n- **Dataset:** [https://www.kaggle.com/datasets/bigfishdev/gemma-template](https://www.kaggle.com/datasets/bigfishdev/gemma-template)\n- **Benchmark:** All benchmarks will be updated at my Github repo: [https://github.com/thewebscraping/gemma-template/blob/main/docs/benchmark.md](https://github.com/thewebscraping/gemma-template/blob/main/docs/benchmark.md)\n\n### Gemma 2B - Vietnamese VMLU Evaluation Results\nVMLU is a benchmark suite designed to evaluate foundational models with a focus on the **Vietnamese language**. \n\n| ID | Created At | Stem | Social Science | Humanities | Others | AVG | Unanswered |\n|-------------|:-------------:|:-------------:|:-------------:|:-------------: |:-------------:|:-------------:|:-------------:|\n| 1624257089558187281 | 05/01/2025 17:56 | 20.14 | 29.35 | 29.84 | 25.76 | 25.61 | 1497 |\n\n#### Results:\n- Out of 9,834 attempts, 1,497 responses were unanswered.\n- The dataset and evaluation results can be downloaded from the file: `gemma-benchmark/gemma_2b_vmlu_answers.csv`. Although it is not within the scope of this fine tuning.\n\n### Gemma 2B IT - Vietnamese VMLU Evaluation Results\n\n| ID | Created At | Stem | Social Science | Humanities | Others | AVG | Unanswered |\n|-------------|:-------------:|:-------------:|:-------------:|:-------------: |:-------------:|:-------------:|:-------------:|\n| 1840435368978448913 | 06/01/2025 19:04 | 36.11 | 43.45 | 41.92 | 39.06 | 39.64 | 82 |\n\n#### Results:\n- Out of 9,834 attempts, 82 responses were unanswered.\n- The dataset and evaluation results can be downloaded from the file: `gemma-benchmark/gemma_2b_it_vmlu_benchmark.csv`. Although it is not within the scope of this fine tuning.\n\n#### My Gemma Fine Tuning VMLU Score:\n![](https://raw.githubusercontent.com/thewebscraping/gemma-template/main/docs/images/Screenshot_VMLU_Gemma_Fine_Tuning.png)\n\n#### VMLU Leaderboard Score:\nThere is a clear difference between the scores on the **Gemma 2B IT fine tuning** model, the **Gemma 2B IT fine tuning** model has a score close to the Gemma 7B IT model. Here is a screenshot of the VMLU Leaderboard:\n\n![](https://github.com/thewebscraping/gemma-template/raw/main/docs/images/Screenshot_VMLU_Leaderboard.png)\n\n#### Additional Resources:\n- VMLU Website: [https://vmlu.ai/](https://vmlu.ai/)\n- VMLU Leaderboard: [https://vmlu.ai/leaderboard](https://vmlu.ai/leaderboard)\n- VMLU Github Repository: [https://github.com/ZaloAI-Jaist/VMLU/](https://github.com/ZaloAI-Jaist/VMLU/)","metadata":{}},{"cell_type":"markdown","source":"# Dataset Preparation\n\nI followed a similar approach as with the **Gemma 2B model** to fine-tune the **Gemma 2B IT model**.\n\nThe train dataset consists of 10,000 rows processed to fine-tune models for content writing. The eval dataset consists of 1000 rows.\n\n**Example:**\n\n| ID             | Title         | Description        | Document         | Categories        | Tags         | Output          | Main Points            |\n|----------------|---------------|--------------------|------------------|-------------------|--------------|-----------------|------------------------|\n| JnZJolR76_u2   | Sample title  | Sample description | Sample document  | [\"Topic 1\", \"Topic 2\"]  | [\"Tag 1\", \"Tag 2\"] | Sample output   | [\"MP 1\", \"MP 2\"]       |\n\n\n## Data Source and Timeline\n\nThe data was collected between September to December 2024 from 50 news pages. The output response data is generate by LLaMa 3 8B, Gemma 7B, and Gemma 9B.\n\n## Language Distribution\n\nThe dataset maintains a balanced language ratio:\n- **50% English**\n- **50% Vietnamese**\n\n## Dataset Composition\n\nThe data distribution is as follows:\n\n- **10%**: Combined responses including title and document.\n- **10%**: Combined responses including title, document, and description.\n- **10%**: Combined responses including title, document, and main points.\n- **10%**: Combined responses including title, document, categories, and tags.\n- **10%**: Responses without instructional templates, structured format only.\n- **10%**: Converted into GPT-style conversations with default output as a response document and random additional fields (e.g., title, description, main points, categories, tags).\n- **40%**: Responses using fully instructions and a prompt structure format.\n- `warmup_ratio`: 5%.\n\n## Additional Features\n\n- **15%** of the dataset incorporates hidden masked words, applied at a ratio of **5% of words** within each document.","metadata":{}},{"cell_type":"code","source":"from gemma_template import Template, gemma_template, vietnamese_gemma_template\nfrom gemma_template.__version__ import __version__\nprint(__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:02:34.690083Z","iopub.execute_input":"2025-01-08T12:02:34.690520Z","iopub.status.idle":"2025-01-08T12:02:36.218850Z","shell.execute_reply.started":"2025-01-08T12:02:34.690483Z","shell.execute_reply":"2025-01-08T12:02:36.217799Z"}},"outputs":[{"name":"stdout","text":"1.0.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Import Gemma Template\nImport default Gemma Template for generate template.","metadata":{}},{"cell_type":"code","source":"from gemma_template import Template, FieldPosition, GEMMA_TEMPLATE, INPUT_TEMPLATE, OUTPUT_TEMPLATE, INSTRUCTION_TEMPLATE, PROMPT_TEMPLATE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:02:38.136194Z","iopub.execute_input":"2025-01-08T12:02:38.136738Z","iopub.status.idle":"2025-01-08T12:02:38.141569Z","shell.execute_reply.started":"2025-01-08T12:02:38.136708Z","shell.execute_reply":"2025-01-08T12:02:38.140229Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Gemma Template\n\nDefault templates by Gemma Template using Jinja2.\n\nUsing Gemma formatting: [https://ai.google.dev/gemma/docs/formatting](https://ai.google.dev/gemma/docs/formatting)","metadata":{}},{"cell_type":"markdown","source":"### **GEMMA_TEMPLATE**\nDefault input template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"<start_of_turn>user\n{{ input }}<end_of_turn>\n<start_of_turn>model\n{{ output }}<end_of_turn>\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:44.865515Z","iopub.execute_input":"2025-01-01T12:36:44.865957Z","iopub.status.idle":"2025-01-01T12:36:45.166983Z","shell.execute_reply.started":"2025-01-01T12:36:44.865903Z","shell.execute_reply":"2025-01-01T12:36:45.166024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **INPUT_TEMPLATE**\nDefault input template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"{{ system_prompt }}\n{% if instruction %}\\n{{ instruction }}\\n{% endif %}\n{% if prompt_structure %}{{ prompt_structure }}\\n{% else %}{{ prompt }}\\n{% endif %}\n# Text:\n{{ input }}\n{% if topic_value %}\\nTopics: {{ topic_value }}\\n{% endif %}{% if keyword_value %}Keywords: {{ keyword_value }}\\n{% endif %}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.167851Z","iopub.execute_input":"2025-01-01T12:36:45.168172Z","iopub.status.idle":"2025-01-01T12:36:45.183947Z","shell.execute_reply.started":"2025-01-01T12:36:45.168144Z","shell.execute_reply":"2025-01-01T12:36:45.182941Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **OUTPUT_TEMPLATE**\nDefault output template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"{% if structure_fields %}{% for field in structure_fields %}## **{{ field.label.custom or field.label.default }}:**\\n{% if field.key == 'title' %}### {% endif%}{{ field.value }}\\n\\n{% endfor %}{% else %}{{ output }}{% endif %}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.184961Z","iopub.execute_input":"2025-01-01T12:36:45.18538Z","iopub.status.idle":"2025-01-01T12:36:45.202012Z","shell.execute_reply.started":"2025-01-01T12:36:45.18534Z","shell.execute_reply":"2025-01-01T12:36:45.201023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **INSTRUCTION_TEMPLATE**\nDefault instruction template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"# Role:\nYou are a highly skilled professional content writer, linguistic analyst, and multilingual expert specializing in structured writing and advanced text processing.\n\n# Task:\nYour primary objectives are:\n1. Simplification: Rewrite the input text or document to ensure it is accessible and easy to understand for a general audience while preserving the original meaning and essential details.\n2. Lexical and Grammatical Analysis: Analyze and refine vocabulary and grammar using unigrams (single words), bigrams (two words), and trigrams (three words) to enhance readability and depth.\n3. Structure and Organization: Ensure your response adheres strictly to the prescribed structure format.\n4. Language Consistency: Respond in the same language as the input text unless explicitly directed otherwise.\n\n# Additional Guidelines:\n1. Provide a rewritten, enhanced version of the input text, ensuring professionalism, clarity, and improved structure.\n2. Focus on multilingual proficiency, using complex vocabulary, grammar to improve your responses.\n3. Preserve the context and cultural nuances of the original text when rewriting.\n\n# Text Analysis:\nExample 1: Unigrams (single words){% for word in unigrams %}\\n{{ word }} => {{ language }}{% endfor %}\nText Analysis 3: These are common {{ language }} words, indicating the text is in {{ language }}.\n\nExample 2: Bigrams (two words){% for word in bigrams %}\\n{{ word }} => {{ language }}{% endfor %}\nText Analysis 2: Frequent bigrams in {{ language }} confirm the language context.\n\nExample 3: Trigrams (three words){% for word in trigrams %}\\n{{ word }} => {{ language }}{% endfor %}\nText Analysis 3: Trigrams further validate the linguistic analysis and the necessity to respond in {{ language }}.\n\n# Conclusion of Text Analysis:\nThe linguistic analysis confirms the text is predominantly in {{ language }}. Consequently, the response should be structured and written in {{ language }} to align with the original text and context.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.203129Z","iopub.execute_input":"2025-01-01T12:36:45.203461Z","iopub.status.idle":"2025-01-01T12:36:45.223188Z","shell.execute_reply.started":"2025-01-01T12:36:45.20342Z","shell.execute_reply":"2025-01-01T12:36:45.222208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **PROMPT_TEMPLATE**\nDefault prompt template by Gemma Template.","metadata":{}},{"cell_type":"code","source":"\"\"\"{% if prompt %}\\n\\n# Input Text:\\n{{ prompt }}\\n\\n{% endif %}{% if structure_fields %}# Response Structure Format:\nYou must follow the response structure:\n\n{% for field in structure_fields %}{{ field.label }}\\n{% endfor %}\nBy adhering to this format, the response will maintain linguistic integrity while enhancing professionalism, structure and alignment with user expectations.\\n\n{% endif %}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.224511Z","iopub.execute_input":"2025-01-01T12:36:45.224852Z","iopub.status.idle":"2025-01-01T12:36:45.242822Z","shell.execute_reply.started":"2025-01-01T12:36:45.224823Z","shell.execute_reply":"2025-01-01T12:36:45.241856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Example Output: Input Document Text with Hidden Mask (`document`)**","metadata":{}},{"cell_type":"code","source":"\"\"\"Gemma open models are built _____ the same _____ and technology as Gemini models. Gemma 2 comes in 2B, 9B _____ 27B and Gemma 1 comes in 2B and 7B sizes.\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.243933Z","iopub.execute_input":"2025-01-01T12:36:45.244313Z","iopub.status.idle":"2025-01-01T12:36:45.261192Z","shell.execute_reply.started":"2025-01-01T12:36:45.244273Z","shell.execute_reply":"2025-01-01T12:36:45.260118Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Example Output: Input Document Text without Hidden Mask  (`document`)**","metadata":{}},{"cell_type":"code","source":"\"\"\"Gemma open models are built from the same research and technology as Gemini models. Gemma 2 comes in 2B, 9B and 27B and Gemma 1 comes in 2B and 7B sizes.\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:45.262227Z","iopub.execute_input":"2025-01-01T12:36:45.262496Z","iopub.status.idle":"2025-01-01T12:36:45.27814Z","shell.execute_reply.started":"2025-01-01T12:36:45.262471Z","shell.execute_reply":"2025-01-01T12:36:45.27702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Process Function Documentation\n\n## Overview\n\nThis repository contains tools for processing datasets to fine-tune language models. The `process_fn` function is central to preparing datasets in various formats such as `text`, `alpaca` and `openai`. It supports frameworks like `unsloth` and `LLaMA-Factory`.\n\n## Features\n\n- Generate unigrams, bigrams, and trigrams with customizable configurations.\n- Automatically exclude invalid keywords containing numbers or punctuation.\n- Support for multilingual datasets, language detection using langdetect with language specific configurations.\n- Creates datasets for conversation-based models by dynamically generating conversational structures.\n\n## How It Works\n\n- **Language Distribution**:\n  - 50% of Dataset uses English prompts.\n  - 50% of Dataset uses Vietnamese prompts.\n\n- **Word Configuration**:\n  - **Unigrams**: Unique words with 2-8 characters. Reason: 1 character will match English like `I`, `a`, 8 characters because the longest word in Vietnamese is 7 characters.\n  - **Bigrams**: Unique bigrams words, not included words of unigrams list.\n  - **Trigrams**: Pairs of unigrams and bigrams.\n  - Ensures the AI vocabulary grows by at least 15 words per document.\n\n- **Special Features**:\n  - Hidden mask words applied to **15%** of the dataset, at a **5% ratio of words** per document.\n  - Generate conversations dataset with configurable fields and output format.","metadata":{}},{"cell_type":"markdown","source":"### Load Dataset\nLoad Dataset from Json File Kaggle host or HuggingFace Hub. See also: [https://huggingface.co/datasets/twodev/gemma-template](https://huggingface.co/datasets/twodev/gemma-template)","metadata":{}},{"cell_type":"code","source":"import json\nfrom datasets import Dataset, DatasetDict, load_dataset\n\ndef load_from_json_file(path: str = \"/kaggle/input/gemma-template/train-gemma-template.json\") -> Dataset:\n    with open(path, encoding=\"utf-8\") as f:\n        data = json.load(f)\n        return Dataset.from_list(data)\n\n\ndef load_from_huggingface_hub(path: str = \"twodev/gemma-template\", split: str = \"train\") -> Dataset:\n    dataset = load_dataset(\"twodev/gemma-template\")\n    return dataset[split]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:02:44.745042Z","iopub.execute_input":"2025-01-08T12:02:44.745430Z","iopub.status.idle":"2025-01-08T12:02:44.751193Z","shell.execute_reply.started":"2025-01-08T12:02:44.745402Z","shell.execute_reply":"2025-01-08T12:02:44.750012Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def convert_to_conversations_dataset(data: Dataset, mapping_field: dict[str, list[str]]) -> Dataset:\n    \"\"\"\n    Converts a dataset into a conversational format suitable for fine-tuning language models.\n\n    Notes:\n        - The `gemma_template._gen_bullet_list_style` method is used to format `openai` responses as a `number`, `dash` and `asterisk` bullet list when the field value is a list.\n    \"\"\"\n    \n    template = \"\"\"<start_of_turn>user\\n{input}<end_of_turn>\\n<start_of_turn>model\\n{output}<end_of_turn>\"\"\"\n    outputs = []\n    for item in data:\n        messages = item[\"messages\"]\n        for field in mapping_field:\n            if field in item[\"origin_data\"] and mapping_field[field]:\n                value = item[\"origin_data\"][field]\n                messages.append(\n                    {\n                        \"role\": \"user\",\n                        \"content\": random.choice(mapping_field[field]),\n                    }\n                )\n                messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": (\n                            gemma_template._generate_bullet_style(value, \"asterisk\")\n                            if isinstance(value, list)\n                            else value\n                        ),\n                    }\n                )\n\n        outputs_ = [\n            template.format(\n                input=\"\\n\\n\".join([messages[0]['content'], messages[1]['content']]),\n                output=messages[2]['content']\n            )\n        ]\n\n        if len(messages) > 3:\n            prompts = []\n            for idx in range(3, len(messages), 2):\n                try:\n                    prompt = template.format(\n                        input=messages[idx][\"content\"],\n                        output=messages[idx + 1][\"content\"],\n                    )\n                    prompts.append(prompt.strip())\n                except IndexError:\n                    pass\n\n            random.shuffle(prompts)\n            outputs_.extend(prompts)\n\n        outputs_.append(\"\")\n        sep = tokenizer.eos_token + \"\\n\"\n        outputs.append({\"text\": sep.join(outputs_) + sep})\n\n    if outputs:\n        return Dataset.from_list(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:02:46.698407Z","iopub.execute_input":"2025-01-08T12:02:46.698774Z","iopub.status.idle":"2025-01-08T12:02:46.708167Z","shell.execute_reply.started":"2025-01-08T12:02:46.698748Z","shell.execute_reply":"2025-01-08T12:02:46.706907Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def process_fn(\n    instance: Template,\n    data: Dataset, \n    excluded_fields: list[str] = (), \n    output_format = 'text',\n    max_hidden_ratio = 0.15, \n    max_hidden_words = 0.05, \n    min_chars_length = 2, \n    max_chars_length = 8,\n    max_concurrency: int = 4,\n    n_words = 5,\n    is_remove_data = True,\n    **kwargs\n) -> Dataset:\n    \"\"\"\n    Processes a dataset for fine-tuning language models, supporting formats like `text`, `alpaca`, and `gpt`.\n\n    Args:\n        instance (Template): A template instance for dataset processing.\n        data (Dataset): The input dataset to be processed.\n        excluded_fields (list[str]): Fields to exclude when generating conversational datasets.\n        output_format (str): Format of the processed dataset. Options are 'text', 'alpaca', and 'gpt'.\n        max_hidden_ratio (Union[float]):\n            Percentage of documents that need to be word masked. Min: 0, Max: 1. Default: 0.\n        max_hidden_words (Optional[str]):\n            Replace words in the document with '____'. The `max_hidden` parameter must be greater than 0.\n            Use `int`: exact number of words to be masked, `float`: percentage of number of words to be masked.\n        min_chars_length (int):\n            Minimum character of a word, used to create unigrams, bigrams, and trigrams. Default is 2.\n        max_chars_length (int):\n            Maximum character of a word, used to create unigrams, bigrams and trigrams. Default is 0.\n        max_concurrency (int):\n            Maximum number of concurrent threads for processing data. Default is 4.\n        n_words (int): Number of words frequently used to create unigrams, bigrams and trigrams.\n        is_remove_data (bool): Whether to remove specific fields from the dataset. Defaults to True.\n        **kwargs: Additional configuration parameters.\n\n    Returns:\n        Dataset or DatasetDict: The processed dataset in the specified format.\n\n    Notes:\n        - The `output_format` parameter determines the dataset's structure:\n            - `'text'`: Standard format for unsloth fine-tuning.\n            - `'alpaca'` or `'openai'`: Formats for frameworks like LLaMA-Factory.\n        - Using `output_format='openai'` and `is_remove_data=False` with `excluded_fields` generates conversational datasets.\n\n    \"\"\"\n    ds = instance.load_dataset(\n        data, \n        output_format=output_format, \n        excluded_fields=excluded_fields,\n        max_hidden_ratio=max_hidden_ratio, \n        max_hidden_words=max_hidden_words, \n        min_chars_length=min_chars_length, \n        max_chars_length=max_chars_length,\n        max_concurrency=max_concurrency,\n        n_words=n_words,\n        is_close_async_loop=False,  # Avoid `RuntimeError` by Notebook\n        is_remove_data=is_remove_data,\n    )\n    if output_format == 'openai' and not is_remove_data:\n        mapping_field = {\n            field: getattr(instance, field, None)\n            for field in excluded_fields\n            if getattr(instance, field, None)\n        }\n        ds = convert_to_conversations_dataset(ds, mapping_field=mapping_field)\n    else:\n        ds = ds.map(lambda x: {\"text\": [text + tokenizer.eos_token for text in x[\"text\"]]}, batched = True)  # Append eos token.\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:02:49.132766Z","iopub.execute_input":"2025-01-08T12:02:49.133118Z","iopub.status.idle":"2025-01-08T12:02:49.141362Z","shell.execute_reply.started":"2025-01-08T12:02:49.133092Z","shell.execute_reply":"2025-01-08T12:02:49.139888Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def print_verify(data, is_masked: bool = True, task_name: str = \"TASK\"):\n    print(task_name + \"*\" * 45)\n    for item in data:\n        if item.get(\"is_masked\") == is_masked:\n            if is_masked:\n                print(\"HIDDEN TEXT: YES\" + \"*\" * 45)\n            else:\n                print(\"HIDDEN TEXT: NO\" + \"*\" * 45)\n                \n            print(\"\\n\")\n            print(item['text'])\n            print(\"=\" * 60)\n            print(\"*\" * 30, \" DATA ATTRS \", \"*\" * 30)\n            print(\"Masked Text:\", item['is_masked'])\n            print(\"Language Code:\", item['analysis']['language_code'])\n            print(\"Language:\", item['analysis']['language'])\n            print(\"Categories:\", item['analysis']['topic_value'])\n            print(\"Keywords:\", item['analysis']['keyword_value'])\n            print(\"Unigrams:\", item['analysis']['unigrams'])\n            print(\"Bigrams:\", item['analysis']['bigrams'])\n            print(\"Trigrams:\", item['analysis']['trigrams'])\n            print(\"VALID TASK: YES\")\n            print(\"*\" * 30, \" TASK DONE \", \"*\" * 30)\n            print(\"=\" * 60)\n            print(\"\\n\")\n            \n            return\n\n    print(\"VALID TASK: NO\")\n    print(\"*\" * 30, \" TASK DONE \", \"*\" * 30)\n    print(\"=\" * 60)\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:02:52.821133Z","iopub.execute_input":"2025-01-08T12:02:52.821519Z","iopub.status.idle":"2025-01-08T12:02:52.829291Z","shell.execute_reply.started":"2025-01-08T12:02:52.821490Z","shell.execute_reply":"2025-01-08T12:02:52.828175Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Cases Handled\n\nThe dataset processing involves various configurations to handle diverse cases effectively:\n\n1. **Combined Responses**:\n   - **10%**: Responses include the `title` and `document`.\n   - **10%**: Responses include the `title`, `document`, and `description`.\n   - **10%**: Responses include the `title`, `document`, and `main points`.\n   - **10%**: Responses include the `title`, `document`, `categories`, and `tags`.\n\n2. **Masked Words**:\n   - **15%**: Masked words are applied to the dataset, with a hidden ratio of **5% of words** for each document.\n\n3. **Language Detection**:\n   - Language is automatically detected using the [`langdetect`](https://github.com/Mimino666/langdetect) library.","metadata":{}},{"cell_type":"code","source":"try:\n    dataset = load_from_json_file(\"/kaggle/input/gemma-template/train-gemma-template.json\")\nexcept FileNotFoundError:\n    dataset = load_from_huggingface_hub(\"twodev/gemma-template\")\n\ntry:\n    dataset = dataset.map(lambda example: {\"categories\": list(set(example[\"categories\"][:5])) if isinstance(example[\"categories\"], list) else []})  # maximum 5 categories to avoid duplication\n    print(\"Categories:\", dataset[1]['categories'])\n    print(\"Categories:\", dataset[15]['categories'])\nexcept:\n    pass\n\ntry:\n    dataset = dataset.map(lambda example: {\"tags\": list(set(example[\"tags\"][:5])) if isinstance(example[\"tags\"], list) else []})  # maximum 5 tags to avoid duplication\n    print(\"Tags:\", dataset[1]['tags'])\n    print(\"Tags:\", dataset[15]['tags'])\nexcept:\n    pass\n\ntotal_rows = len(dataset)\nfive_percent_idx = int(total_rows * 0.1)\n\ndataset_mapping = {\n    \"Combined response including title and document\": {\n        \"data\": dataset.select(range(0, five_percent_idx)),\n        \"excluded_fields\": [\"description\", \"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and description\": {\n        \"data\": dataset.select(range(five_percent_idx, five_percent_idx * 2)),\n        \"excluded_fields\": [\"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and main points\": {\n        \"data\": dataset.select(range(five_percent_idx * 2, five_percent_idx * 3)),\n        \"excluded_fields\": [\"description\", \"categories\", \"tags\"]\n        \n    },\n    \"Combined response including title, document and categories and tags\": {\n        \"data\": dataset.select(range(five_percent_idx * 3, five_percent_idx * 4)),\n        \"excluded_fields\": [\"description\", \"main_points\"]\n        \n    },\n}\n\nprint(\"\\n\")\nprint(\"*\" * 30, \" DATASET INFO \", \"*\" * 30)\nprint(dataset_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T12:06:07.465813Z","iopub.execute_input":"2025-01-08T12:06:07.466267Z","iopub.status.idle":"2025-01-08T12:06:14.403145Z","shell.execute_reply.started":"2025-01-08T12:06:07.466230Z","shell.execute_reply":"2025-01-08T12:06:14.401178Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0129c735f74c95820aed1ca913c242"}},"metadata":{}},{"name":"stdout","text":"Categories: ['Tổ ấm']\nCategories: ['Thế giới']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b6a1589565d4c8fb67b41c3d7fef080"}},"metadata":{}},{"name":"stdout","text":"Tags: ['Cây cảnh mini', 'Phong thủy', 'May mắn', 'Giàu sang', 'Thu hút tài lộc']\nTags: ['Ông Yoon Suk Yeol', 'Hàn Quốc', 'Tổng thống Hàn Quốc', 'Tổng thống Yoon Suk Yeol']\n\n\n******************************  DATASET INFO  ******************************\n{'Combined response including title and document': {'data': Dataset({\n    features: ['id', 'title', 'description', 'document', 'output', 'categories', 'tags', 'main_points', 'from_url', 'pub_date', 'similarlities'],\n    num_rows: 1000\n}), 'excluded_fields': ['description', 'main_points', 'categories', 'tags']}, 'Combined response including title, document and description': {'data': Dataset({\n    features: ['id', 'title', 'description', 'document', 'output', 'categories', 'tags', 'main_points', 'from_url', 'pub_date', 'similarlities'],\n    num_rows: 1000\n}), 'excluded_fields': ['main_points', 'categories', 'tags']}, 'Combined response including title, document and main points': {'data': Dataset({\n    features: ['id', 'title', 'description', 'document', 'output', 'categories', 'tags', 'main_points', 'from_url', 'pub_date', 'similarlities'],\n    num_rows: 1000\n}), 'excluded_fields': ['description', 'categories', 'tags']}, 'Combined response including title, document and categories and tags': {'data': Dataset({\n    features: ['id', 'title', 'description', 'document', 'output', 'categories', 'tags', 'main_points', 'from_url', 'pub_date', 'similarlities'],\n    num_rows: 1000\n}), 'excluded_fields': ['description', 'main_points']}}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"language_ratio_size = 0.5  # Ratio between English and local language.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.539436Z","iopub.execute_input":"2025-01-01T12:36:49.539732Z","iopub.status.idle":"2025-01-01T12:36:49.544475Z","shell.execute_reply.started":"2025-01-01T12:36:49.539704Z","shell.execute_reply":"2025-01-01T12:36:49.543603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_datasets = []\nfor task, item in dataset_mapping.items():\n    print(\"Prepare dataset for task:\", task)\n    split_dataset = item['data'].train_test_split(test_size=language_ratio_size)\n\n    # prepare dataset use instruction and structure English language.\n    english_dataset = process_fn(\n        gemma_template, \n        split_dataset[\"train\"], \n        excluded_fields=item['excluded_fields']\n    )\n    input_datasets.append(english_dataset)\n                                                       \n    # prepare dataset use instruction and structure Vietnamese language.\n    vietnamese_dataset = process_fn(\n        vietnamese_gemma_template, \n        split_dataset[\"test\"], \n        excluded_fields=item['excluded_fields']\n    )\n    \n    input_datasets.append(vietnamese_dataset)\n    \n# Test with hidden mask test\nprint_verify(english_dataset, is_masked=True, task_name=\"ENGLISH VERSION: {}\".format(task.upper()))\n# Test without hidden mask\nprint_verify(vietnamese_dataset, is_masked=False, task_name=\"VIETNAMESE VERSION: {}\".format(task.upper()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:36:49.545509Z","iopub.execute_input":"2025-01-01T12:36:49.545861Z","iopub.status.idle":"2025-01-01T12:45:30.063757Z","shell.execute_reply.started":"2025-01-01T12:36:49.545822Z","shell.execute_reply":"2025-01-01T12:45:30.062749Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n**Hidden Mask**:  The first **15%** of batches will include hidden mask words.\n\nThe dataset processing works perfectly across both language versions:\n\n1. **English**: Includes hidden word masks as expected.\n2. **Vietnamese**: No hidden word masks applied.\n\n## Common Observations\n- **Advanced Features**:\n  - Instruction included: **Unigrams**, **bigrams**, and **trigrams** are generated correctly.\n  - Structured formatting functions as intended.","metadata":{}},{"cell_type":"code","source":"print(input_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:45:30.064965Z","iopub.execute_input":"2025-01-01T12:45:30.065281Z","iopub.status.idle":"2025-01-01T12:45:30.070532Z","shell.execute_reply.started":"2025-01-01T12:45:30.065251Z","shell.execute_reply":"2025-01-01T12:45:30.069474Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare No Instruction Dataset\n- **10%**: Responses without instructional templates, fully structured format only.\n\nTo create a dataset without instructions:\n\n1. **Template Configuration**:  \n   - The creation process is similar to the standard template.  \n   - Set `instance.instruction_template=[]` to omit the instruction template during generation.\n\n2. **Dataset Merging**:  \n   - Append the generated dataset to `input_datasets` to allow merging of all datasets together.","metadata":{}},{"cell_type":"code","source":"# empty instruction template\ngemma_template.instruction_template = []  \nvietnamese_gemma_template.instruction_template = []\n\nno_instruction_dataset = dataset.select(range(five_percent_idx*4, five_percent_idx*5))\nprint(no_instruction_dataset)\n\nsplit_dataset = no_instruction_dataset.train_test_split(test_size=language_ratio_size)\n\n# prepare dataset use instruction and structure English language.\nenglish_no_instruction_dataset = process_fn(gemma_template, split_dataset[\"train\"])\ninput_datasets.append(english_no_instruction_dataset)\n                                                   \n# prepare dataset use instruction and structure Vietnamese language.\nvietnamese_no_instruction_dataset = process_fn(vietnamese_gemma_template,  split_dataset[\"test\"])\ninput_datasets.append(vietnamese_no_instruction_dataset)\n\n# print verify\nprint_verify(english_no_instruction_dataset, is_masked=False, task_name=\"ENGLISH NO INSTRUCTION VERSION\")\nprint_verify(vietnamese_no_instruction_dataset, is_masked=False, task_name=\"VIETNAMESE NO INSTRUCTION VERSION\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:45:30.07175Z","iopub.execute_input":"2025-01-01T12:45:30.072125Z","iopub.status.idle":"2025-01-01T12:51:50.265549Z","shell.execute_reply.started":"2025-01-01T12:45:30.072095Z","shell.execute_reply":"2025-01-01T12:51:50.264438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n- The instruction template has been deleted.\n- The structure template still works.","metadata":{}},{"cell_type":"markdown","source":"## Prepare Conversations Dataset\n- **10%**: Converted into GPT-style conversations with default output as a response document and random additional fields (e.g., title, description, main points, categories, tags).\n\n### Template Customization\n\n- Modify the template to follow the conversational format as outlined in the [Gemma documentation](https://ai.google.dev/gemma/docs/formatting).\n- I overwrite by adding eos token at the end of each model response.\n\n### Example Template\n\n```text\n<start_of_turn>user\n\nknock knock<end_of_turn><eos>\n\n<start_of_turn>model\n\nwho is there<end_of_turn><eos>\n\n<start_of_turn>user\n\nGemma<end_of_turn><eos>\n\n<start_of_turn>model\n\nGemma who?<end_of_turn><eos>\n```\n\n### Implementation","metadata":{}},{"cell_type":"code","source":"from gemma_template.constants import INSTRUCTION_TEMPLATE, VIETNAMESE_INSTRUCTION_TEMPLATE\n\n# Reset the template as instructed.\ngemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\nvietnamese_gemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\n\nconversations_dataset = dataset.select(range(five_percent_idx*5, five_percent_idx*6))\nsplit_dataset = conversations_dataset.train_test_split(test_size=language_ratio_size)\nexcluded_fields = [\"title\", \"description\", \"main_points\", \"categories\", \"tags\"]\n\n# prepare dataset use instruction and structure English language.\nenglish_conversations_dataset = process_fn(\n    gemma_template, \n    split_dataset[\"train\"], \n    excluded_fields=excluded_fields,\n    output_format=\"openai\",\n    is_remove_data=False,\n)\ninput_datasets.append(english_conversations_dataset)\n\n                                                   \n# prepare dataset use instruction and structure Vietnamese language.\nvietnamese_conversations_dataset = process_fn(\n    vietnamese_gemma_template, \n    split_dataset[\"test\"], \n    excluded_fields=excluded_fields,\n    output_format=\"openai\",\n    is_remove_data=False,\n)\ninput_datasets.append(vietnamese_conversations_dataset)\n\n# print verify\nprint(english_conversations_dataset['text'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:51:50.266706Z","iopub.execute_input":"2025-01-01T12:51:50.267134Z","iopub.status.idle":"2025-01-01T12:58:24.421252Z","shell.execute_reply.started":"2025-01-01T12:51:50.267091Z","shell.execute_reply":"2025-01-01T12:58:24.420048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n\n- The structure template still works. The GPT conversations structure matches.\n","metadata":{}},{"cell_type":"markdown","source":"## Prepare Fully Instruction and Prompt Structure Format\n- **40%**: Responses using fully instructions and a prompt structure format.\n","metadata":{}},{"cell_type":"code","source":"from gemma_template.constants import INSTRUCTION_TEMPLATE, VIETNAMESE_INSTRUCTION_TEMPLATE\n\n# Reset the template as instructed.\ngemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\nvietnamese_gemma_template.instruction_template = [INSTRUCTION_TEMPLATE]\n\ninstruction_dataset = dataset.select(range(five_percent_idx*6, len(dataset)))\nprint(instruction_dataset)\n\nsplit_dataset = instruction_dataset.train_test_split(test_size=language_ratio_size)\n\n# prepare dataset use instruction and structure English language.\nenglish_instruction_dataset = process_fn(gemma_template, split_dataset[\"train\"])\ninput_datasets.append(english_instruction_dataset)\n                                                   \n# prepare dataset use instruction and structure Vietnamese language.\nvietnamese_instruction_dataset = process_fn(vietnamese_gemma_template,  split_dataset[\"test\"])\ninput_datasets.append(vietnamese_instruction_dataset)\n\n# print verify\nprint_verify(english_instruction_dataset, is_masked=True, task_name=\"ENGLISH INSTRUCTION VERSION\")\nprint_verify(vietnamese_instruction_dataset, is_masked=False, task_name=\"VIETNAMESE INSTRUCTION VERSION\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T12:58:24.422625Z","iopub.execute_input":"2025-01-01T12:58:24.423112Z","iopub.status.idle":"2025-01-01T13:21:38.633292Z","shell.execute_reply.started":"2025-01-01T12:58:24.423079Z","shell.execute_reply":"2025-01-01T13:21:38.632038Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Expected\n\n- The instruction and structure template still works.\n","metadata":{}},{"cell_type":"markdown","source":"## Merged Dataset\n- A special token representing the end of a text template.\n- Split 5% for warm up and merge all dataset together.\n- **Shuffle dataset**: shuffling the data helps prevent bias during training, ensures randomness in batch selection, and prevents the model from learning patterns based on the order of the data.","metadata":{}},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\nwramup_dataset, train_dataset = [], []\nfor input_dataset in input_datasets:\n    split_dataset = input_dataset.train_test_split(test_size=0.05)\n    wramup_dataset.append(split_dataset['test'])\n    train_dataset.append(split_dataset['train'])\n    \nwramup_dataset = concatenate_datasets(wramup_dataset).shuffle(seed=seed)\ntrain_dataset = concatenate_datasets(train_dataset).shuffle(seed=seed)\ntrain_dataset = concatenate_datasets([wramup_dataset, train_dataset])\n\n# verify dataset\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.634538Z","iopub.execute_input":"2025-01-01T13:21:38.634983Z","iopub.status.idle":"2025-01-01T13:21:38.767498Z","shell.execute_reply.started":"2025-01-01T13:21:38.634938Z","shell.execute_reply":"2025-01-01T13:21:38.766619Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **All Good! Now we begin to configure the Trainer for fine-tuning.**","metadata":{}},{"cell_type":"code","source":"elapsed = stopped_at - datetime.now()\nprint(\"Elapsed prepared dataset: %s. Took: %.2f seconds\" % (str(elapsed), elapsed.total_seconds()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.768496Z","iopub.execute_input":"2025-01-01T13:21:38.768764Z","iopub.status.idle":"2025-01-01T13:21:38.77433Z","shell.execute_reply.started":"2025-01-01T13:21:38.768739Z","shell.execute_reply":"2025-01-01T13:21:38.773269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# You can customize your local language configuration by following these instructions.\n\n### Tricks\n\nIf you want a static prompt loop, configure all prompts with equal length. For example:\n\n```python\ntitle = [\"a\", \"b\"]\ndescription = [\"1\", \"2\"]\n```\n\nThe template will be created as follows:\n- **Document 1:** title = `a`, description = `1`\n- **Document 2:** title = `b`, description = `2`\n- **Document 3:** title = `a`, description = `1`\n\nTo use a fully random set, configure different sizes of title and description. For example:\n\n```python\ntitle = [\"a\", \"b\"]\ndescription = [\"1\", \"2\", \"3\"]\n```\n\nThe template will be created as follows:\n- **Document 1:** title = `a`, description = `1`\n- **Document 2:** title = `b`, description = `2`\n- **Document 3:** title = `a`, description = `3`\n- **Document 4:** title = `b`, description = `1`\n\n## Sample Customizing Local Language Configuration","metadata":{}},{"cell_type":"markdown","source":"```python\nfrom gemma_template import *\n\nVIETNAMESE_INSTRUCTION_TEMPLATE = \"\"\"# Vai trò:\nBạn là một biên tập viên nội dung chuyên nghiệp, nhà phân tích ngôn ngữ và chuyên gia đa ngôn ngữ, chuyên về viết có cấu trúc và xử lý văn bản nâng cao.\n\n# Nhiệm Vụ:\nMục tiêu chính của bạn là:\n1. Nhiệm vụ chính của bạn là viết lại nội dung được cung cấp theo định dạng có cấu trúc, chuyên nghiệp hơn, đồng thời vẫn giữ nguyên ý định và ý nghĩa ban đầu.\n2. Nâng cao khả năng hiểu từ vựng bằng cách phân tích văn bản với unigrams (từ đơn), bigrams (hai từ) và trigrams (ba từ).\n3. Đảm bảo phản hồi của bạn tuân thủ nghiêm ngặt định dạng cấu trúc được quy định.\n4. Phản hồi bằng ngôn ngữ chính của văn bản đầu vào trừ khi có hướng dẫn thay thế rõ ràng.\n\n# Kỳ Vọng Bổ Sung:\n1. Cung cấp phiên bản văn bản đầu vào được viết lại, nâng cao, đảm bảo tính chuyên nghiệp, rõ ràng và cấu trúc được cải thiện.\n2. Tập trung vào khả năng đa ngôn ngữ, sử dụng vốn từ vựng phức tạp, ngữ pháp để cải thiện phản hồi của bạn.\n3. Giữ nguyên ngữ cảnh và sắc thái văn hóa của văn bản gốc khi viết lại.\n{% if topic_value %}\\nTopics: {{ topic_value }}\\n{% endif %}{% if keyword_value %}Keywords: {{ keyword_value }}\\n{% endif %}\n\n# Phân Tích Văn Bản:\nVí Dụ 1: Unigrams (nhóm 1 chữ cái){% for word in unigrams %}\\n{{ word }} => Tiếng Việt ({{ language }}){% endfor %}\n\nPhân Tích Văn Bản 1: đây là những từ thông dụng trong tiếng Việt ({{ language }}), cho biết văn bản được viết bằng tiếng Việt ({{ language }}).\n\nVí Dụ 2: Bigrams (nhóm 2 chữ cái){% for word in bigrams %}\\n{{ word }} => Tiếng Việt ({{ language }}){% endfor %}\nPhân Tích Văn Bản 2: các từ ghép thường gặp trong Tiếng Việt ({{ language }}) xác nhận bối cảnh ngôn ngữ.\n\nVí Dụ 3: Trigrams (nhóm 3 chữ cái)\\n{% for word in trigrams %}{{ word }} => Tiếng Việt ({{ language }}){% endfor %}\nPhân Tích Văn Bản 3: các từ ghép 3 chữ liên tiếp là những từ tiếng Việt sử dụng thường xuyên, xác nhận sự cần thiết phải phản hồi bằng Tiếng Việt ({{ language }}).\n\n# Kết Luận Phân Tích Văn Bản:\nPhân tích ngôn ngữ xác nhận văn bản chủ yếu bằng Tiếng Việt ({{ language }}). Do đó, phản hồi phải được cấu trúc và viết bằng Tiếng Việt ({{ language }}). để phù hợp với văn bản và ngữ cảnh gốc.\n\"\"\"  # noqa: E501\n\nVIETNAMESE_PROMPT_TEMPLATE = \"\"\"{% if prompt %}\\n\\n# Đầu Vào Văn Bản:\\n{{ prompt }}\\n\\n{% endif %}{% if structure_fields %}# Định Dạng Cấu Trúc Phản Hồi:\nBạn phải tuân theo cấu trúc phản hồi:\n{% for field in structure_fields %}{{ field.label }}\\n{% endfor %}\n\nBằng cách tuân thủ định dạng này, phản hồi sẽ duy trì tính toàn vẹn về mặt ngôn ngữ đồng thời tăng cường tính chuyên nghiệp, cấu trúc và sự phù hợp với mong đợi của người dùng.\n{% endif %}\"\"\"  # noqa: E501\n\nVIETNAMESE_INPUT_TEMPLATE = \"\"\"{{ system_prompt }}\n{% if instruction %}\\n{{ instruction }}\\n{% endif %}\n{% if prompt_structure %}{{ prompt_structure }}\\n{% else %}{{ prompt }}\\n{% endif %}\n# Văn Bản:\n{{ input }}\n{% if topic_value %}\\nDanh Mục: {{ topic_value }}\\n{% endif %}{% if keyword_value %}Từ Khoá: {{ keyword_value }}\\n{% endif %}\n\"\"\"\n\nVIETNAMESE_OUTPUT_TEMPLATE = \"\"\"{% if structure_fields %}{% for field in structure_fields %}## **{{ field.label.custom or field.label.default }}:**\\n{% if field.key == 'title' %}### {% endif%}{{ field.value }}\\n\\n{% endfor %}{% else %}{{ output }}{% endif %}\"\"\"\n\nvietnamese_gemma_template = Template(\n    input_template=[VIETNAMESE_INPUT_TEMPLATE],\n    output_template=[VIETNAMESE_OUTPUT_TEMPLATE],\n    instruction_template=[VIETNAMESE_INSTRUCTION_TEMPLATE],\n    prompt_template=[VIETNAMESE_PROMPT_TEMPLATE],\n    end_sep=\"và\",\n    system_prompts=[\n        (\n            \"Bạn là một nhà sáng tạo nội dung, viết nội dung chuyên nghiệp biết nhiều\"\n            \" ngôn ngữ.\"\n        ),\n    ],\n    prompts=[\n        \"Viết lại nội dung này để thân thiện với SEO. Bao gồm các từ khóa có liên quan, tối ưu hóa tiêu đề và tiêu đề phụ, và đảm bảo văn bản trôi chảy tự nhiên cho các công cụ tìm kiếm và người đọc.\",\n        \"Viết lại bài viết này để làm cho nó đơn giản hơn và dễ hiểu hơn đối với đối tượng chung. Sử dụng ngôn ngữ rõ ràng và súc tích trong khi vẫn giữ nguyên ý nghĩa ban đầu và các chi tiết chính.\",\n        \"Tái hiện bài viết này với giọng điệu hấp dẫn và sáng tạo hơn. Thêm phép ẩn dụ, phép so sánh hoặc các yếu tố kể chuyện để làm cho nó hấp dẫn hơn đối với người đọc.\",\n        \"Viết lại bài viết này để làm cho nó thuyết phục và hấp dẫn hơn. Tập trung vào việc củng cố các lập luận, thu hút cảm xúc và sử dụng các kỹ thuật tu từ để thuyết phục người đọc.\",\n        \"Viết lại bài viết này để làm nổi bật đề xuất giá trị độc đáo của nó trong khi đảm bảo nó được xếp hạng tốt cho các từ khóa mục tiêu.\",\n        \"Viết lại nội dung này với giọng điệu có thẩm quyền, kết hợp các nguồn, dữ liệu và tài liệu tham khảo đáng tin cậy để tăng độ tin cậy và thứ hạng SEO.\",\n        \"Viết lại bài viết này để đối tượng chuyên nghiệp, tập trung vào các chi tiết kỹ thuật, thuật ngữ chuyên ngành và thông tin chi tiết có thể thực hiện được.\",\n    ],\n    position=FieldPosition(\n        title=[\"Tiêu đề\"],\n        description=[\"Mô tả\"],\n        document=[\"Bài viết chỉnh sửa\"],\n        main_points=[\"Điểm nổi bật\", \"Điểm chính\"],\n        categories=[\"Danh mục\", \"Chủ đề\"],\n        tags=[\"Từ khoá\"],\n    ),\n    title=[\n        \"Viết lại tiêu đề để phản ánh từ khóa và chủ đề chính.\",\n        \"Viết lại tiêu đề để làm cho nó ngắn gọn, dễ nhớ và được tối ưu hóa cho SEO.\",\n        \"Tạo một tiêu đề ngắn gọn, rõ ràng, thu hút sự chú ý và được tối ưu hóa cho SEO.\",\n        \"Phát triển một tiêu đề hấp dẫn, thân thiện với SEO và thể hiện chính xác nội dung.\",\n        \"Sửa đổi tiêu đề để đảm bảo từ khóa liên quan, hấp dẫn và dễ hiểu.\",\n        \"Soạn một tiêu đề truyền tải rõ ràng chủ đề và được tối ưu hóa cho các công cụ tìm kiếm.\",\n        \"Viết lại tiêu đề để tối đa hóa sự rõ ràng, hấp dẫn và liên quan đến nội dung.\",\n        \"Tạo một tiêu đề bổ sung cho tiêu đề nhưng thêm chi tiết hơn. Làm cho tiêu đề mang tính đối thoại để thu hút người đọc.\",\n        \"Tập trung vào một góc độ gây ngạc nhiên hoặc độc đáo trong tiêu đề. Bao gồm các con số hoặc số liệu thống kê trong tiêu đề để tạo sự cụ thể.\",\n        \"Kết hợp các từ khóa hoặc cụm từ thịnh hành vào tiêu đề. Đảm bảo tiêu đề có liên quan và gắn chặt với nội dung.\",\n        \"Viết lại tiêu đề sao cho ngắn gọn, rõ ràng và tối ưu hóa SEO.\",\n        \"Thêm từ ngữ mạnh mẽ vào tiêu đề để gợi sự tò mò hoặc cảm xúc.\",\n        \"Tập trung vào những lợi ích trong tiêu đề để thu hút sự chú ý.\",\n        \"Sử dụng động từ hành động để tạo tiêu đề hấp dẫn và năng động.\",\n        \"Viết lại tiêu đề để phản ánh từ khóa và chủ đề chính.\",\n    ],\n    description=[\n        \"Viết lại phần mô tả bằng một tuyên bố hoặc số liệu thống kê táo bạo để thu hút sự chú ý.\",\n        \"Viết phần mô tả bài viết trong một hoặc hai câu, đồng thời tập trung vào lợi ích của người đọc và khơi gợi sự tò mò.\",\n        \"Bắt đầu phần mô tả bằng một giai thoại hoặc câu chuyện hấp dẫn để tối ưu hóa SEO.\",\n        \"Viết lại phần mô tả để làm nổi bật một sự thật đáng ngạc nhiên hoặc hiểu biết độc đáo khiến người đọc tò mò.\",\n        \"Soạn thảo phần mô tả trong một hoặc hai câu, nhấn mạnh giá trị mà người đọc sẽ nhận được từ bài viết.\",\n        \"Bắt đầu phần mô tả bằng một câu hỏi gợi mở để khơi dậy sự tò mò và khuyến khích nhấp chuột.\",\n        \"Viết phần mô tả bắt đầu bằng một mẹo hoặc lời khuyên hữu ích để thu hút ngay lập tức đối tượng mục tiêu.\",\n        \"Tạo phần mô tả tập trung vào cách bài viết giải quyết một vấn đề hoặc thách thức phổ biến mà người đọc gặp phải.\",\n        \"Viết lại phần mô tả bằng ngôn ngữ khơi gợi cảm xúc, truyền cảm hứng cho người đọc khám phá sâu hơn.\",\n    ],\n    document=[\n        \"Viết lại bài viết này để đối tượng chuyên nghiệp, tập trung vào các chi tiết kỹ thuật, thuật ngữ chuyên ngành và thông tin chi tiết có thể thực hiện được.\",\n        \"Viết lại nội dung này với giọng điệu có thẩm quyền, kết hợp các nguồn, dữ liệu và tài liệu tham khảo đáng tin cậy để tăng độ tin cậy và thứ hạng SEO.\",\n        \"Viết lại bài viết này để làm nổi bật đề xuất giá trị độc đáo của nó trong khi đảm bảo nó được xếp hạng tốt cho các từ khóa mục tiêu.\",\n        \"Viết lại bài viết này để làm cho nó thuyết phục và hấp dẫn hơn. Tập trung vào việc củng cố các lập luận, thu hút cảm xúc và sử dụng các kỹ thuật tu từ để thuyết phục người đọc.\",\n        \"Tái hiện bài viết này với giọng điệu hấp dẫn và sáng tạo hơn. Thêm phép ẩn dụ, phép so sánh hoặc các yếu tố kể chuyện để làm cho nó hấp dẫn hơn đối với người đọc.\",\n        \"Viết lại bài viết này để làm cho nó đơn giản hơn và dễ hiểu hơn đối với đối tượng chung. Sử dụng ngôn ngữ rõ ràng và súc tích trong khi vẫn giữ nguyên ý nghĩa ban đầu và các chi tiết chính.\",\n        \"Viết lại nội dung này để thân thiện với SEO. Bao gồm các từ khóa có liên quan, tối ưu hóa tiêu đề và tiêu đề phụ, và đảm bảo văn bản trôi chảy tự nhiên cho các công cụ tìm kiếm và người đọc.\",\n    ],\n    main_points=[\n        \"Tóm tắt các ý chính thành các điểm chính ngắn gọn, có thể hành động để thêm ngữ cảnh nhằm khiến chúng hấp dẫn hơn.\",\n        \"Đơn giản hóa các điểm chính ban đầu để làm cho chúng rõ ràng hơn và thân thiện hơn với người đọc.\",\n        \"Đảm bảo tất cả các điểm chính đều có mạch lạc hợp lý từ điểm này sang điểm khác.\",\n        \"Tóm tắt những điểm chính từ văn bản này thành các điểm chính, đảm bảo tính rõ ràng và súc tích.\",\n        \"Tạo một tài liệu tóm tắt chắt lọc các chủ đề chính và các điểm chính hỗ trợ từ văn bản này.\",\n        \"Viết lại các điểm chính để súc tích hơn và dễ thực hiện hơn.\",\n        \"Nhóm các điểm chính liên quan để tổ chức tốt hơn.\",\n        \"Thêm ví dụ hoặc giải thích ngắn gọn cho mỗi điểm chính.\",\n        \"Đơn giản hóa các ý tưởng phức tạp thành các điểm chính dễ hiểu.\",\n        \"Viết lại các điểm chính dưới dạng câu hỏi để làm cho chúng hấp dẫn hơn.\",\n        \"Đảm bảo tất cả các điểm chính đều có sự liên kết hợp lý từ điểm này sang điểm khác.\",\n        \"Biến các khái niệm trừu tượng thành các hành động cụ thể trong các điểm chính.\",\n    ],\n    categories=[\n        \"Viết lại các danh mục để phù hợp với chủ đề phổ biến theo bài viết.\",\n        \"Tạo danh sách danh mục để phù hợp với các từ khóa được sử dụng trong bài viết.\",\n        \"Chọn các danh mục cải thiện SEO và khả năng khám phá theo nội dung bài viết.\",\n        \"Chỉ định các danh mục phản ánh chủ đề chính của bài viết.\",\n        \"Viết lại các danh mục để phù hợp với các tiêu chuẩn của ngành hoặc các chủ đề phổ biến.\",\n        \"Tập trung vào các danh mục rộng nhưng cụ thể để tổ chức tốt hơn.\",\n        \"Đảm bảo các danh mục phản ánh sở thích của đối tượng mục tiêu.\",\n        \"Viết lại các danh mục để phù hợp với các từ khóa được sử dụng trong bài viết.\",\n        \"Chọn các danh mục cải thiện SEO và khả năng khám phá.\",\n        \"Sử dụng các danh mục phù hợp với các bài viết tương tự về chủ đề này.\",\n        \"Tránh các danh mục quá rộng hoặc mơ hồ bằng cách cụ thể.\",\n        \"Viết lại các danh mục để làm nổi bật các lĩnh vực trọng tâm chính của bài viết.\",\n    ],\n    tags=[\n        \"Tạo danh sách 5 từ khóa thịnh hành giúp SEO tốt hơn.\",\n        \"Tạo danh sách 5 từ khóa có liên quan phù hợp với truy vấn tìm kiếm phổ biến.\",\n        \"Tập trung vào các từ khóa phổ biến trong bài viết để SEO tốt hơn dưới 5 từ khoá.\",\n        \"Viết lại 3 đến 5 từ khóa để bao gồm các từ khóa có liên quan.\",\n        \"Thêm các cụm từ khóa thịnh hành để tăng khả năng hiển thị trong khoảng 3 đến 5 từ khoá.\",\n        \"Sử dụng các từ khóa phản ánh các chủ đề phụ hoặc chủ đề của bài viết trong phạm vi dưới 5 từ khoá.\",\n        \"Đảm bảo các từ khóa phù hợp với các truy vấn tìm kiếm phổ biến dưới 5 từ khoá.\",\n        \"Viết lại từ 3 đến 5 từ khóa để làm cho chúng cụ thể và có mục tiêu hơn.\",\n        \"Tạo 5 từ khóa phù hợp với nội dung tương tự để có cơ hội quảng cáo chéo.\",\n    ],\n)\n\nresponse = vietnamese_gemma_template.apply_template(\n    title=\"Gemma open models\",\n    description=\"Gemma: Introducing new state-of-the-art open models.\",\n    main_points=[\"Main point 1\", \"Main point 2\"],\n    categories=[\"Artificial Intelligence\", \"Gemma\"],\n    tags=[\"AI\", \"LLM\", \"Google\"],\n    document=\"Gemma open models are built from the same research and technology as Gemini models. Gemma 2 comes in 2B, 9B and 27B and Gemma 1 comes in 2B and 7B sizes.\",\n    output=\"A new family of open language models demonstrating strong performance across academic benchmarks for language understanding, reasoning, and safety.\",\n    max_hidden_words=.05,  # Hide 5% words of per documents.\n    min_chars_length=2,  # Minimum character of a word, used to create unigrams, bigrams, and trigrams. Default is 2.\n    max_chars_length=0,  # Maximum character of a word, used to create unigrams, bigrams and trigrams. Default is 0.\n)  # remove kwargs if not used.\n\nprint(response)\n```","metadata":{}},{"cell_type":"markdown","source":"# Automatic Model Save Every Hour\n\nThis repository includes an implementation for automatically saving model checkpoints during training on Kaggle. Due to Kaggle's training time limit, the `TrainerCallback` class has been extended to handle periodic backups. \n\nThe maximum running time is set to 11 hours 30 minutes, and you can configure it using the `stopped_at` parameter (configuration on top of this notebook).\n\n## Automatic Backup Implementation\n\nThe following code demonstrates how the `HubCallback` class is implemented to save checkpoints every hour:","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom pathlib import Path\nfrom transformers import TrainerCallback, Trainer\nfrom transformers.trainer_callback import TrainerControl, TrainerState\nfrom transformers.training_args import TrainingArguments\n\nTRAINING_ARGS_NAME = \"training_args.bin\"\nTRAINER_STATE_NAME = \"trainer_state.json\"\nOPTIMIZER_NAME = \"optimizer.pt\"\nOPTIMIZER_NAME_BIN = \"optimizer.bin\"\nSCHEDULER_NAME = \"scheduler.pt\"\nSCALER_NAME = \"scaler.pt\"\nFSDP_MODEL_NAME = \"pytorch_model_fsdp\"\n\nREADME = \"\"\"\n# Training Arguments\nProject Id: {project_id}\nTraining Steps: {step}\n\n### Hyperparameters:\n{hyperparameters}\n\"\"\"\n\nclass HubCallback(TrainerCallback):\n    def __init__(self, trainer: Trainer, project_id, stopped_at, save_every_n_minutes=60):\n        super().__init__()\n        self.trainer = trainer\n        self.project_id = project_id\n        self.stopped_at = stopped_at\n        self.save_every_n_minutes = save_every_n_minutes\n        self.save_after = datetime.now() + timedelta(minutes=save_every_n_minutes)\n\n    def on_step_begin(self, train_args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n        if datetime.now().timestamp() > self.stopped_at.timestamp():\n            self.save_checkpoint(train_args)\n            control.should_training_stop = True\n            control.should_save = True\n        else:\n            if datetime.now().timestamp() > self.save_after.timestamp():\n                self.save_after = datetime.now() + timedelta(minutes=self.save_every_n_minutes)\n                self.save_checkpoint(train_args)\n\n    def save_checkpoint(self, train_args: TrainingArguments, output_dir: str = \".checkpoint/\"):\n        try:\n            print(\"*\" * 30, \" SAVE CHECKPOINT \", \"*\" * 30)\n            Path(output_dir).mkdir(parents=True, exist_ok=True)\n            self.trainer.save_model(output_dir, _internal_call=True)\n            self.trainer.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n            torch.save(self.trainer.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n            torch.save(self.trainer.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))\n            with open(os.path.join(output_dir, \"README.md\"), \"w\", encoding=\"utf-8\") as fp:\n                readme_doc = README.format(\n                    project_id=self.project_id,\n                    step=self.trainer.state.global_step,\n                    hyperparameters=json.dumps(train_args.to_dict(), ensure_ascii=False, indent=4)\n                )\n                fp.write(readme_doc)\n\n            print(\"*\" * 30, \" PROCESSED PUSH TO HUB \", \"*\" * 30)\n            self.push_to_hub(train_args, output_dir)\n            print(\"*\" * 30, \" COMPLETED PUSH TO HUB \", \"*\" * 30)\n\n        except Exception as e:\n            print(\"FAILED TO SAVE CHECKPOINT:\", e)\n\n    def push_to_hub(self, train_args: TrainingArguments, output_dir: str = \".checkpoint/\"):\n        self.trainer.tokenizer.save_pretrained(output_dir)\n        self.trainer.model.save_pretrained(output_dir)\n        self.trainer.tokenizer.push_to_hub(\n            self.project_id,\n            private=train_args.hub_private_repo,\n            token=train_args.hub_token\n        )\n        self.trainer.model.push_to_hub(\n            self.project_id,\n            commit_message=\"Training steps: {}\".format(self.trainer.state.global_step),\n            private=train_args.hub_private_repo,\n            token=train_args.hub_token\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.775435Z","iopub.execute_input":"2025-01-01T13:21:38.7757Z","iopub.status.idle":"2025-01-01T13:21:38.793897Z","shell.execute_reply.started":"2025-01-01T13:21:38.775677Z","shell.execute_reply":"2025-01-01T13:21:38.792758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure WanDB","metadata":{}},{"cell_type":"code","source":"import os\nimport wandb\n\nrun = wandb.init(\n    project=project_id,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.795057Z","iopub.execute_input":"2025-01-01T13:21:38.79544Z","iopub.status.idle":"2025-01-01T13:21:38.813466Z","shell.execute_reply.started":"2025-01-01T13:21:38.7954Z","shell.execute_reply":"2025-01-01T13:21:38.812415Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure Hyperparameters\n\nBelow is an configuration for `TrainingArguments`:","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrain_args = TrainingArguments(\n    # ---Output settings--\n    # Output directory where model predictions and checkpoints will be stored\n    output_dir = f\".results/{project_id}\",\n    logging_dir = f\".results/{project_id}/logs\",\n    overwrite_output_dir = True,\n    # No eval running\n    do_eval = False,\n    # Save strategy\n    save_strategy = \"steps\",\n    # Save steps\n    save_steps = 300,\n    # Save total limit\n    save_total_limit = 1,\n    # Batch size per GPU core for training\n    per_device_train_batch_size = 2,\n    # Number of update steps to accumulate the gradients for\n    gradient_accumulation_steps = 4,\n    # Train epochs\n    num_train_epochs = 1,\n    # Learning rate\n    learning_rate = 2e-4,\n    # Enable float16 precision\n    fp16 = not torch.cuda.is_bf16_supported(),\n    # Enable bfloat16 precision. False then fp16 is True\n    bf16 = torch.cuda.is_bf16_supported(),\n    # Logging: Log every X update step\n    logging_steps = 3,\n    # Optimizer to use\n    optim = \"adamw_8bit\",\n    # Weight decay\n    weight_decay = 0.1,\n    lr_scheduler_type = \"linear\",\n    # Ratio of steps for a linear warmup (from 0 to learning rate)\n    warmup_ratio = 0.05,\n    hub_private_repo = True,\n    remove_unused_columns = True,\n    seed = seed,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:38.814557Z","iopub.execute_input":"2025-01-01T13:21:38.814952Z","iopub.status.idle":"2025-01-01T13:21:51.394342Z","shell.execute_reply.started":"2025-01-01T13:21:38.814885Z","shell.execute_reply":"2025-01-01T13:21:51.392832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure SFT Trainer and Trainer","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,\n    args=train_args,\n)\n\n# Add HubCallback to save model every hour and stop after 11 hours 30 minutes\ntrainer.add_callback(HubCallback(trainer, project_id, stopped_at))\n\n# Start training\ntrainer = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.395036Z","iopub.status.idle":"2025-01-01T13:21:51.395387Z","shell.execute_reply":"2025-01-01T13:21:51.395243Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Push to HuggingFace","metadata":{}},{"cell_type":"code","source":"try:\n    with open(os.path.join(train_args.output_dir, \"README.md\"), \"w\", encoding=\"utf-8\") as fp:\n        readme_doc = README.format(\n        project_id=project_id,\n        step=trainer.state.global_step,\n        hyperparameters=json.dumps(train_args.to_dict(), ensure_ascii=False, indent=4)\n    )\n    fp.write(readme_doc)\nexcept:\n    pass\n    \ntry:\n    model.save_pretrained(project_id)\n    tokenizer.save_pretrained(project_id)\nexcept:\n    pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.396361Z","iopub.status.idle":"2025-01-01T13:21:51.396757Z","shell.execute_reply":"2025-01-01T13:21:51.396562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    model.push_to_hub(project_id)\n    tokenizer.push_to_hub(project_id)\nexcept:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.397607Z","iopub.status.idle":"2025-01-01T13:21:51.39796Z","shell.execute_reply":"2025-01-01T13:21:51.397792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"elapsed = stopped_at - datetime.now()\nprint(\"Elapsed:\", str(elapsed))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T13:21:51.398625Z","iopub.status.idle":"2025-01-01T13:21:51.399038Z","shell.execute_reply":"2025-01-01T13:21:51.398867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model Evaluation Metric\n=======================\n\nThis repository provides tools for evaluating models using **GoogleBLEU** and **ROUGE**, following the guidelines outlined in the [HuggingFace documentation on choosing a metric](https://huggingface.co/docs/evaluate/choosing_a_metric).\n\nOverview\n--------\n\nThe evaluation metrics implemented in this project include:\n\n1.  **GoogleBLEU**: A variant of BLEU designed for evaluating machine translation quality.\n    \n2.  **ROUGE**: Widely used for evaluating text summarization and machine translation models.\n    \n\nThese metrics ensure a comprehensive assessment of model performance across various natural language processing tasks.\n\nInstructions\n------------\n\n1.  Follow the instructions on the [HuggingFace Evaluation Metric Guide](https://huggingface.co/docs/evaluate/choosing_a_metric) to integrate these metrics into your workflow.\n    \n2.  Due to the runtime limitations of Kaggle trainers, the complete dataset results are hosted externally. You can view the full benchmark results here: [Benchmark Results](https://github.com/thewebscraping/gemma-template/blob/main/docs/benchmark.md).\n    \n\nResults\n-------\n\nA summary of evaluation results can be found in the [benchmark documentation](https://github.com/thewebscraping/gemma-template/blob/main/docs/benchmark.md).\n\nReferences\n----------\n\n*   HuggingFace Documentation: [Choosing a Metric](https://huggingface.co/docs/evaluate/choosing_a_metric)\n    \n*   Benchmark Data: [Benchmark Results](https://github.com/thewebscraping/gemma-template/blob/main/docs/benchmark.md)\n    \n\n### Example Code\n-------------\n","metadata":{}},{"cell_type":"code","source":"import torch\n\nfrom unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    project_id,\n    max_seq_length = max_seq_length,\n    dtype = None,\n    load_in_4bit = True,\n)\nFastLanguageModel.for_inference(model)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nstopped_at = stopped_at + timedelta(minutes=15)\n\ndef is_expired(expired_at):\n    if datetime.now().timestamp() > expired_at.timestamp():\n        return True\n    return False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prepare Test Dataset\n\n1. **Combined Prompt**:\n   - **10%**: Prompt include the `title` and `document`.\n   - **10%**: Prompt include the `title`, `document`, and `description`.\n   - **10%**: Prompt include the `title`, `document`, and `main points`.\n   - **10%**: Prompt include the `title`, `document`, `categories`, and `tags`.\n   - **60%**: Prompt fully structure format.\n   - **Remove instruction template for evaluate.**\n   - **Overwrite default prompt structure template.**","metadata":{}},{"cell_type":"code","source":"try:\n    test_dataset = load_from_json_file(\"/kaggle/input/gemma-template/test-gemma-template.json\")\nexcept FileNotFoundError:\n    test_dataset = load_from_huggingface_hub(\"twodev/gemma-template\", split='test')\n\ntotal_rows = len(test_dataset)\nfive_percent_idx = int(total_rows * 0.1)\n\ndataset_mapping = {\n    \"Combined response including title and document\": {\n        \"data\": test_dataset.select(range(0, five_percent_idx)),\n        \"excluded_fields\": [\"description\", \"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and description\": {\n        \"data\": test_dataset.select(range(five_percent_idx, five_percent_idx * 2)),\n        \"excluded_fields\": [\"main_points\", \"categories\", \"tags\"]\n    },\n    \"Combined response including title, document and main points\": {\n        \"data\": test_dataset.select(range(five_percent_idx * 2, five_percent_idx * 3)),\n        \"excluded_fields\": [\"description\", \"categories\", \"tags\"]\n        \n    },\n    \"Combined response including title, document and categories and tags\": {\n        \"data\": test_dataset.select(range(five_percent_idx * 3, five_percent_idx * 4)),\n        \"excluded_fields\": [\"description\", \"main_points\"]\n        \n    },\n    \"Prompt Structure Format\": {\n        \"data\": test_dataset.select(range(five_percent_idx * 4, len(test_dataset))),\n        \"excluded_fields\": []\n        \n    },\n}\n\nprint(dataset_mapping)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Configure Template\n- Remove instruction template and override default prompt structure template for evaluate.\n- There is also another method used: `gemma_template.generate_prompt(**kwargs)`","metadata":{}},{"cell_type":"code","source":"GEMMA_PROMPT_TEMPLATE = \"\"\"<start_of_turn>user\n{input}<end_of_turn>\n<start_of_turn>model\n\n\"\"\"\n\nPROMPT_TEMPLATE = \"\"\"{% if prompt %}\\n\\n{{ prompt }}\\n\\n{% endif %}{% if structure_fields %}# Response Structure Format:\nYou must follow the response structure:\n\n{% for field in structure_fields %}{{ field.label }}\\n{% endfor %}\n{% endif %}\"\"\"\n\nVIETNAMESE_PROMPT_TEMPLATE = \"\"\"{% if prompt %}\\n\\n{{ prompt }}\\n\\n{% endif %}{% if structure_fields %}# Định Dạng Cấu Trúc Phản Hồi:\nBạn phải tuân theo cấu trúc phản hồi:\n\n{% for field in structure_fields %}{{ field.label }}\\n{% endfor %}\n{% endif %}\"\"\"\n\ngemma_template.instruction_template = []  \ngemma_template.prompt_template = [PROMPT_TEMPLATE]\nvietnamese_gemma_template.instruction_template = []\nvietnamese_gemma_template.prompt_template = [VIETNAMESE_PROMPT_TEMPLATE]\n\neval_dataset = []\ninput_datasets = []\nfor task, item in dataset_mapping.items():\n    if is_expired(stopped_at):\n        break\n        \n    print(\"Prepare dataset for task:\", task)\n    split_dataset = item['data'].train_test_split(test_size=language_ratio_size)\n\n    # prepare dataset use instruction and structure English language.\n    english_dataset = gemma_template.load_dataset(\n        split_dataset[\"train\"], \n        excluded_fields=item['excluded_fields'],\n        output_format='alpaca',\n    )\n    english_dataset = english_dataset.map(lambda x: {\"task\": [\"English\" for _ in x[\"input\"]], }, batched=True)\n    input_datasets.append(english_dataset)\n                                                       \n    # prepare dataset use instruction and structure Vietnamese language.\n    vietnamese_dataset = vietnamese_gemma_template.load_dataset(\n        split_dataset[\"test\"], \n        excluded_fields=item['excluded_fields'],\n        output_format='alpaca',\n    )\n    vietnamese_dataset = vietnamese_dataset.map(lambda x: {\"task\": [\"Vietnamese\" for _ in x[\"input\"]], }, batched=True)\n    input_datasets.append(vietnamese_dataset)\n\nfrom datasets import concatenate_datasets\n\nif input_datasets:\n    eval_dataset = concatenate_datasets(input_datasets).shuffle(seed=42)\n    eval_dataset = eval_dataset.map(lambda x: {\"prompt\": [GEMMA_PROMPT_TEMPLATE.format(input=input_str) for input_str in x[\"input\"]], }, batched=True)\n    print(eval_dataset)\n    print(eval_dataset[0]['prompt'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run Evaluate","metadata":{}},{"cell_type":"code","source":"import json\nimport evaluate\n\ndef clean_response(response: str):\n    response = response.split(\"<start_of_turn>model\")[-1].split(\"<end_of_turn>\")\n    return response[0].strip()\n\n\ngoogle_bleu = evaluate.load(\"google_bleu\")\nrouge = evaluate.load('rouge')\neval_responses = []\n\nfor idx, item in enumerate(eval_dataset):\n\n    # Remove `is_expired` for fully eval, this code is avoid Kaggle limit.\n    if is_expired(stopped_at):\n        break\n\n    task = str(item['task']).upper()\n    input_str = item['prompt']\n    output_str = item['output'].strip()\n    predictions, references = [output_str], []\n    input_ids = tokenizer(input_str, return_tensors=\"pt\").to(device)\n    outputs = model.generate(**input_ids, max_new_tokens=1024)\n\n    model_references = []\n    rouge_score, google_bleu_score = {}, {}\n\n    try:\n        for output in outputs:\n            model_response = tokenizer.decode(output)\n            model_references.append(model_response)\n            references.append(clean_response(model_response))\n        \n        if not (predictions and references):\n            continue\n\n        try:\n            rouge_score = rouge.compute(predictions=predictions, references=references)\n            rouge_score = {k: float(v) for k, v in rouge_score.items()}\n        except:\n            pass\n\n        try:\n            google_bleu_score = google_bleu.compute(predictions=predictions, references=references)\n        except:\n            pass\n            \n    except:\n        pass\n    \n    try:\n        item.update({\"rouge\": rouge_score, \"google_bleu\": google_bleu_score, \"model_references\": model_references})\n        eval_responses.append(json.loads(json.dumps(item, default=str)))\n    except:\n        pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save Evaluate Data ","metadata":{}},{"cell_type":"code","source":"def is_valid_score(r: dict, field: str):\n    if isinstance(r, dict):\n        if r.get(field):\n            return True\n        \ndef write_json(obj, path: str = \"dump.json\", *, ensure_ascii=False, indent=4):\n    with open(path, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump(obj, json_file, ensure_ascii=ensure_ascii, indent=4, default = str)\n\n\ntry:\n    write_json(eval_responses, project_id + \"/example_eval.json\")\nexcept:\n    pass\n\n\ntotal_rows = len(eval_responses)\ntry:\n    rouge_mapping = {\n        \"rouge1\": sum([r['rouge']['rouge1'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n        \"rouge2\": sum([r['rouge']['rouge2'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n        \"rougeL\": sum([r['rouge']['rougeL'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n        \"rougeLSum\": sum([r['rouge']['rougeLsum'] for r in eval_responses if is_valid_score(r, \"rouge\")]) / total_rows,\n    }\n    print(\"AVG ROUGE SCORE:\", str(rouge_mapping))\nexcept:\n    pass\n\ntry:\n    google_bleu_mapping = {\"google_bleu\": sum([r['google_bleu']['google_bleu'] for r in eval_responses if is_valid_score(r, \"google_bleu\")]) / total_rows}\n    print(\"AVG GOOGLE BLEU:\", str(google_bleu_mapping))\n    print(\"*\" * 90)\n    print(\"\\n\")\nexcept:\n    pass\n\ntry:\n    for response in eval_responses[:5]:\n        print(\"=\" * 90)\n        print(\"*\" * 30, \" ORIGIN OUTPUT \", \"*\" * 30)\n        print(response['output'])\n        print(\"*\" * 30, \" MODEL OUTPUT \", \"*\" * 30)\n        print(clean_response(response['model_references'][0]))\n        print(\"*\" * 30, \" SCORE \", \"*\" * 30)\n        print(\"ROUGE SCORE:\", response['rouge'])\n        print(\"GOOGLE BLEU SCORE:\", response['google_bleu'])\n        print(\"=\" * 90)\nexcept:\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf .checkpoint wandb .results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Vietnamese VMLU Evaluation\nIf you are interested in using Vietnamese MMLU Evaluation you can use the following code:\n\n**Note:** please visit to [https://vmlu.ai/](https://vmlu.ai/) for download eval dataset data.","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import torch\n# import json\n# import glob\n# import logging\n# import os\n# import argparse\n# import time\n\n# from string import Template\n# from pathlib import Path\n# from tqdm import tqdm\n# from typing import Any\n\n# import warnings\n# warnings.simplefilter(\"ignore\")\n\n# def predict(args):\n#     device = args.device\n#     folder = args.folder\n\n#     path = args.project_id.split('/')[-1].strip()\n#     filename = f\"./logs/{args.project_id}.log\"\n    \n#     ## create directory\n#     directory_path = './logs'\n#     if not os.path.exists(directory_path):\n#         os.makedirs(directory_path)\n        \n#     # Configure logging\n#     logging.basicConfig(filename=filename, level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n#     logging.info(f'Model name: {args.project_id}')\n\n#     # Create empty lists to store data\n#     ids = []\n#     questions = []\n#     choices_A = []\n#     choices_B = []\n#     choices_C = []\n#     choices_D = []\n#     choices_E = []\n\n#     # Read JSONL files\n#     data_path = Path(folder)\n#     jsonl_files = list(data_path.glob(args.dataset))\n\n#     for file in jsonl_files:\n#         with open(file, \"r\", encoding=\"utf-8\") as f:\n#             lines = f.readlines()\n#             for line in lines:\n#                 data = json.loads(line)\n#                 ids.append(data[\"id\"])\n#                 questions.append(data[\"question\"])\n#                 choices = data[\"choices\"]\n#                 try:\n#                     choices_A.append(choices[0])\n#                 except:\n#                     choices_A.append('')\n#                 try:\n#                     choices_B.append(choices[1])\n#                 except:\n#                     choices_B.append('')\n#                 try:\n#                     choices_C.append(choices[2])\n#                 except:\n#                     choices_C.append('')\n#                 try:\n#                     choices_D.append(choices[3])\n#                 except:\n#                     choices_D.append('')\n#                 try:\n#                     choices_E.append(choices[4])\n#                 except:\n#                     choices_E.append('')\n\n#     # Create a DataFrame\n#     df = pd.DataFrame({\n#         \"id\": ids,\n#         \"prompt\": questions,\n#         \"A\": choices_A,\n#         \"B\": choices_B,\n#         \"C\": choices_C,\n#         \"D\": choices_D,\n#         \"E\": choices_E\n#     })\n#     logging.info(df.head())\n\n#     preamble = \\\n#         'Chỉ đưa ra chữ cái đứng trước câu trả lời đúng (A, B, C, D hoặc E) của câu hỏi trắc nghiệm sau: '\n\n#     template = Template(args.template)\n\n#     def format_input(df, idx):\n#         prompt = df.loc[idx, 'prompt']\n#         a = df.loc[idx, 'A']\n#         b = df.loc[idx, 'B']\n#         c = df.loc[idx, 'C']\n#         d = df.loc[idx, 'D']\n#         e = df.loc[idx, 'E']\n\n#         input_text = template.substitute(\n#             preamble=preamble, prompt=prompt, a=a, b=b, c=c, d=d, e=e)\n\n#         return input_text\n\n#     inputs = args.tokenizer(format_input(df, 0), return_tensors=\"pt\").to(device)\n#     outputs = args.model.generate(**inputs, max_new_tokens=1)\n#     answer = args.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n#     logging.info('Contruct a toy eg')\n#     logging.info(\"Generated answer: %s\", answer)\n\n#     answers = []\n\n#     start = time.time()\n#     for idx in tqdm(df.index):\n#         inputs = args.tokenizer(format_input(df, idx), return_tensors=\"pt\").to(device)\n#         outputs = args.model.generate(**inputs, max_new_tokens=args.max_new_tokens)\n#         answer_decoded = args.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n#         last_element = answer_decoded[-1]\n#         answer = last_element.split()[-1]\n#         if \"án:\" in answer:\n#             answer = \"-\"\n\n#         answers.append(answer)\n\n#     end = time.time()\n#     duration = end - start\n#     print('Time taken for running inference: ', duration)\n\n#     df['answer'] = answers\n#     logging.info(df.head())\n\n#     return df\n\n# from dataclasses import dataclass\n\n# @dataclass\n# class EvalArgs:\n#     model: Any\n#     tokenizer: Any\n#     project_id: str = \"gemma-2b\"\n#     folder: str = \"./vmlu_v1.5\"  # please visit to https://vmlu.ai/ for download eval dataset data.\n#     dataset: str = \"test.jsonl\"\n#     device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#     template: str = \"$preamble\\n\\n$prompt\\n\\n $a\\n $b\\n $c\\n $d\\n $e\\nĐáp án:\"\n#     max_new_tokens: int = 1\n\n# eval_args = EvalArgs(model=model, tokenizer=tokenizer, project_id=project_id)\n# df = predict(eval_args)\n# df[['id','answer']].to_csv(\"./gemma-2b-vmlu-benchmark.csv\", index = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Took: %.2f seconds.\" % (time.perf_counter() - start_at))\nprint(\"Task End:\", str(datetime.now() - stopped_at))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
